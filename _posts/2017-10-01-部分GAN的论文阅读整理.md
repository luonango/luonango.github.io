---
layout:     post
title:      部分GAN论文的阅读整理
subtitle:   一些GAN的论文阅读笔记
date:       2017-10-01
author:     BY
header-img: img/tag-bg-o.jpg
catalog: true
tags:
    - GAN
    - 论文阅读笔记

---

### Good Semi-supervised Learning That Requires a Bad GAN
    Zihang Dai∗, Zhilin Yang∗, Fan Yang, William W. Cohen, Ruslan Salakhutdinov
    School of Computer Science
    Carnegie Melon University



---

### f-GAN: 2016-Training Generative Neural Samplers using Variational Divergence Minimization

    Nowozin S, Cseke B, Tomioka R. Minimization[J]. 2016. NIPS

本文贡献：

- 推导f-散度的GAN训练目标，并提供了KL散度、Pearson散度的例子
- 简化Goodfellow等人提出的鞍点优化程序
- 实验测试哪个散度更适合作为自然图像的生成性神经采样器

使用变分散度最小化训练生成神经采样器

里面有关于f-散度的推导，以及其他散度分析。有很多公式。需要就去看吧。

---

### 解释及运用对抗样本：2015-Explaining and Harnessing Adversarial Examples

    Ian J. Goodfellow, Jonathon Shlens & Christian Szegedy. Google Inc., Mountain View, CA.  ICLR

#### Abstruct：

- 当数据被进行有意小绕动后，许多神经网络模型将以高置信度输出错误答案。早期解释集中在非线性、过度拟合方面。
- 本文认为主要是神经网络的线性特性。这解释在架构和训练集间的泛化的定量实验结果得到支持。
- 且本文还提供了生成对抗样本的方法，采用对抗样本，减少了Maxout网络在mnist的错误率。

#### Introduction:

- Szegedy等人(2014)发现：几种前沿的神经网络模型在对抗样本上仍然显得脆弱。对抗样本的原因未知，前人推测是DNN的极端非线性、或是纯粹监督学习中的模型且正则化相结合不足有关。
- 但本文表明这些推测都是不必要的。高维空间的线性属性足以造成面对对抗样本脆弱情况，
- 这观点促使我们设计一个快速生成对抗样本的方法，使模型能对抗样本训练。
- 我们表明对抗样本训练能比单独使用dropout方法提供额外的正则效果。
- 通过dropout、预训练、模型平均等方法不会降低模型在对抗样本训练时脆弱性，但是非线性模型（如RBF网络）可以。
- 设计易于训练的线性模型和能抵抗样本绕动的非线性模型存在根本的张力。长远来看可以通过设计能成功训练非线性模型的更强大的优化方法来抵抗样本绕动。

### Related Work：

- Szegedy等人（2014b）展示了神经网络和相关模型的各种有趣特性。与本文最相关的内容包括：
    + Box-constrained（箱约束） L-BFGS 可以找到对抗样本。
    + 一些数据集上（ImageNet等）对抗样本和真实例子很接近，人类难以区分。
    + 相同的对抗样本经常被各种分类器错分类，或者让各种分类器在训练数据子集上进行训练
    + 浅Softmax回归模型在对抗样本上也很脆弱
    + 在对抗样本上的训练能够让模型正则，但要在内部循环进行昂贵的约束优化，这在当时很不现实。
- 当前性能优异的分类器，也没能学习确定正确输出标签的真正基础概念。
- 它们建立了Potemkin village从而在自然图片中很好分类，但当出现一个数据在数据分布中不具有高概率出现的情况时，这些算法缺点就暴露出来了。当前流行方法是采用欧几里得距离接近感知距离的空间，如果网络感知距离很小的图像表示却与不同的类别相对应时候，这相似性显然有缺陷。
- 尽管线性分类也有这个问题，但这结果常被解释为深度网络的缺陷。
- 已经有人开始设计对抗样本的模型，但还没成功地同时让测试集达到领先准确度。


### The Linear Explanation of Adversarial Examples 对抗样本上的线性解释：

- 输入样本$x$，对抗样本$\tilde{x}=x+\eta$ 其中$\eta$ 是绕动因子，它足够小以至于不影响数据的表示($\mid\mid \eta \mid\mid _\infty < \epsilon$):

- $w^x \tilde{x}=w^T x+w^T \eta$
- 对抗绕动造成的激活值是$w^T \eta$，为了让这个扰动达到最大，我们让 $\eta=sign(w)$
- 如果$w$有$n$维，权重的元素平均长度量是$m$，按激活值将增加$mn$，虽然$\eta$不受维度变化而增长，但多个维度绕动造成的激活变化将非常大（足以改变输出）。
- 即如果一个简单的线性模型的输入有足够的维度，它就可能拥有对抗样本。
- 这简单的线性例子，也能解释为什么softmax回归在对抗样本前很脆弱。

### Linear Perturbation of Non-Linear Models ： 非线性模型的线性绕动

- 我们假设太线性的神经网络不能抵抗对抗样本。LSTMs、ReLUs、Maxout网络等等都有意设计成非常线性的方式表现（易于优化），复杂的非线性模型（Sigmoid网络）将大部分时间在非饱和上（依旧非常线性），这些线性方式虽然简洁易用，但是会损害神经网络。

- 设$\theta$是模型的参数，$x$是模型的输入，$y$是与$x$相关的目标（对于具有目标的机器学习任务）和$J(\theta,x,y)$是用于训练神经网络的成本。我们可以将成本函数线性化为$\theta$的当前值，得到最优的最大范数约束：
- $\eta=\epsilon sign (\nabla _x J(\theta,x,y))$
- 从而得到对抗样本$\tilde{x}=x+\eta$
- 以此对抗样本测试许多模型，发现此方法能产生有效的对抗样本。

### Adversarial Training of Linear Models Versus Weight Decay 线性模型与权值衰减的对抗训练


- 标签$y \in \lbrace -1,1 \rbrace$且$P(y=1)=\sigma(w^Tx+b)$,$w^Tsign(w)=\mid\mid w\mid\mid _1$
- $\zeta(y) = log (1 + exp(z))$ 是 softplus 函数，逻辑回归的对抗样本是最小化从：
- ${\Bbb E}_{x,y \sim p_{data}} \zeta(-y(w^T x+b))$
变为：${\Bbb E}_{x,y \sim p_{data}} \zeta(y(\epsilon \mid\mid w \mid\mid _1 -w^T x-b))$
- 这有点类似于$L^1$正则化。但是$L^1$的惩罚从训练期间模型的激活中减去，而不是加到训练成本上。这意味着如果模型学习到预测足够好以使$\zeta$饱和，那么惩罚最终会开始消失。但在欠拟合时候对抗样本训练不一定会加重欠拟合。故可认为在欠拟合情况下$L^1$的权重衰减会比对抗样本训练差很多，因为$L^1$在好的结果上没有停止。
- 在多类别的softmax回归上,$L^1$正则将更糟糕。它将softmax每个输出当成独立的绕动。事实上不可能找到一个$\eta$符合（对齐）所有类的权重向量。
- 权重衰减高估了绕动造成的影响（即使是深度神经网络），所以有必要使用比$L^1$权重衰减方法精度更小的方法，即采用我们的对抗样本训练。
- 实验测试发现$L^1$太重了难以调（0,0025依旧太大了），而用$\epsilon=0.25$效果就很好（$\mid\mid \eta \mid\mid _\infty < \epsilon$）
- 较小的重量衰减系数允许成功的训练，但不赋予正则化效益

### Adversarial Training of Deep Networks 深度网络的对抗训练

- 混合训练对抗样本和原样本，神经网络能以某程度被正则化。
- 数据增强和对抗样本不同。数据增强的样本一般不是真实自然图片云云。模型的决策功能将会缺陷云云。
- 基于快速梯度sign的对抗样本目标函数训练是一种有效的正则化方法：
- $\tilde{J}(\theta,x,y)=\alpha J(\theta,x,y)+(1-\alpha)J(\theta,x+\epsilon sign(\nabla _x J(\theta,xy))$
- 本文不考虑超参$\theta$的u优化，取值0.5即可。
- 实验发现对抗样本训练很有效。
- 不仅提高了精度，学习模型的权重还发生了显着变化，对抗训练模型的权重明显更加局部化和可解释。
 

### Why Do Adversarial Examples Generalize? 为什么对抗样本通用（泛化能力）？

- 有趣的是：对抗样本常被其他模型错分类（且他们不同架构、不同数据集上训练），且还被他们错分为统一类（互相认可）。
- 线性角度：对抗样本是出现在宽广的子空间，而不是某个精确的特别区域。这解释了为什么对抗样本丰富，且错分类的样本还被相互认可。
- 分类器能学到大致相同的分类权重（即使在不同子集上训练），这基础的分类权重的稳定性会影响对抗样本的稳定性。

为了解释为什么多个分类器将相同的类分配到敌对的例子，我们假设用当前的方法训练的神经网络都类似于在相同训练集上学习的线性分类器。这个参考分类器在训练集的不同子集上训练时能够学习大致相同的分类权重，这仅仅是因为机器学习算法能够推广。基础分类权重的稳定性反过来又会导致敌对性的例子的稳定性。

### Alternative Hypotheses 另一些猜想：

- 生成训练可以提供更多的训练过程，，从而让模型学习得更好：
    + 其他的模型要么不可区分推理过程，使得难以计算对抗样本，要么需要额外的非生成的鉴别器模型才能得到高精度分类。
    + MP-DBM中我们确保生成模型本身就对应着对抗样本，而不是非生成的分类器模型。但这种模型依旧容易受到对抗样本攻击。一些其他形式的生成模型可能提供抵抗力，但这个事实不足够。
- 为什么对抗样本存在的猜想：
    + 单模型容易存在偏向，多个模型的平均能够冲淡这问题，（能抵抗对抗样本的攻击）。
    + 而实验确实证明如此。但面对对抗样本，相比单模型，集成的方法虽然有提升但依旧有限。

### Summary and Discussion

作为总结，本文提出了以下意见：

- 对抗样本可解释为一个高维点产品的属性。它们 **是模型过于线性,,而不是非线性导致的结果**。
- 不同模型之间，对抗样本的泛化能力可以解释为对抗性扰动与模型的权重向量方向一致，而不同模型在进行相同任务训练时会学习到类似的权重。
- 扰动的方向，而不是空间中的特定点，而是实体。空间中不是充满了对抗样本（不像有理数一样充满着空间）。
- 因为是实体方向，故对抗绕动会概括不同的原始样本。
- **提出一系列能快速产生对抗样本的方法。**
- **已经证明对抗性训练可以导致正规化。甚至Dropout进一步正规化。**
- **对$L^1$权重衰减和添加噪音两种方法进行了控制实验，想达到类似对抗性训练的正则效果（或比较好一点的效果），但均难以调控，失败了。**
- **易于优化的模型容易受到扰动。**
- **线性模型缺乏抗对抗性扰动的能力;只有具有隐藏层的结构（通用逼近定理适用）才能被训练来抵抗对抗样本。**
- **RBF网络能抵抗对抗样本。**
- **训练模型输入分布的模型不能抵抗对抗样本。**
- **集成模型方法，不能抵抗对抗样本（对抗有限）。**

垃圾类别的样本（无关类的样本，即无意义的样本，如全噪音图）：
- 垃圾类样本无处不在，容易生成。
- 浅线模型不能抵抗垃圾类的样本。
- RBF网络抵制垃圾类样本。

要优化训练局部更加稳定的模型，才能让模型不容易被对抗样本欺骗。

---

### 综述： 2017-Generative Adversarial Networks: An Overview

    Creswell A, White T, Dumoulin V, et al.[J]. 2017.
笔记记在纸质版论文上。
概括得挺好，从中选一些论文作为后续论文阅读。

---

### 更好训练GAN：2016-Improved Techniques for Training GANs

     Tim Salimans，Ian Goodfellow，Wojciech Zaremba，Vicki Cheung
     [github-tensorflow](https://github.com/openai/improved_gan)

提出Feature matching,mnibatch feature,Virtural batch normalization三种提高训练GAN的方法。

- **Feature matching 特征匹配：**
    + 指定G的新目标，从而防止GAN训练不稳定、防止D过度训练。新目标是要求G生成与实际数据相匹配的特征。通过训练G，来匹配D中间层的特征期望值。
    + 将G的目标定义为：
        $$ 
        \begin{align}
        \mid\mid E_{x \sim p_{data}} f(x)-E_{z \sim {p_z(z)})}f(G(z))) \mid\mid_{2}^2 
        \end{align}$$
    + 实验证明，特征匹配确实在常规GAN变得不稳定的情况下有效

- **Minibatch discrimination（小样本鉴别）：**
    + GAN失效原因之一是：G总是生成相同的数据（Generator collapse)。D反馈的梯度都指向同一方向，让G容易收敛到一个点。
    + 是因为每次D都是独立处理样本，这导致没有机制告诉G要生成的样本不一样。G会向某一个方向改进，从而让最终G生成样本的分布无法收敛到正确样本的熵分布。
    + 解决方法是允许D同时观察多样本的组合，并执行minbatch discrimination（小样本鉴别）
    + 假设$f(x_i)\in R^{A}$ 表示D中间层的输出向量。将$f(x_{i})$乘以矩阵 $T \in R^{A×B×C}$，得到一个矩阵 $M_{i} \in R^{B×C}$,其中 $i \in\lbrace1,2,...,n\rbrace$ 。
    + 计算$M_{i}$中每行间的$L_{1}-distance$，并加以负对数得到$c_b(x_i,x_j)= exp(- \mid\mid M_{i,b} - M_{j,b} \mid\mid _{L_1})$
    + 再将$c_b(x_i,x_j)$的和作为下一层的输入$o(x_i)$。
    $$
    \begin{align}
    o(x_i)_b =& \sum_{j=1}^n c_b(x_i,x_j) \in {\cal R} \\
    o(x_i) =&{\big [} o(x_i)_1, o(x_i)_2,...,o(x_i)_B {\big]} \in {\cal R^B} \\
    o(X) \in & R^{n \times B} \\
    \end{align}$$
    + D中依旧有原先的判断机制，而这个机制起辅助作用。能防止collapse，也能让D容易分辨G是否生成相同样本。快速生成更吸引的图像。

- **Historical averaging（历史平均):**
    + 每个样本的损失项包括：$\mid\mid \theta - \frac{1}{t} \sum_{i=1}^t \theta[i] \mid\mid ^2$
    + 这样梯度将不容易进入稳定的轨道，就能向纳什均衡点靠近。
- **One-sided label smoothing（单边标签平滑）**
    + 用$\alpha$代替正分类目标，用$\beta$代替负目标，最优鉴别器变为:
    $$
    \begin{align}
        D(x)= \frac{\alpha p_{data}(x)+\beta p_{model}(x)}{p_{data}(x)+p_{model}(x)}
    \end{align}
    $$
    + 其中$p_{data}(x)\rightarrow 0$且$p_{model}(x)\rightarrow 1$时候，来自$p_{model}(x)$的loss特别小（D的辨别能力太强），从而让G没办法得到训练。
    + 因此只将正标签平滑到如0.9而非1，负标签设置为0。这样就算D很自信，也能正常反向更新G参数
- **Virtual batch normalization：**
    + BN 能提高网络性能，但是它强依赖于同一个batch所有样本的输入。为了解决这问题，提出了VBN。
    + 每个batch在训练开始前就要固定好：从训练集中取出另一个batch，计算它均值方差，用它来更新当前batch的所有样本。然后再将更新完后的batch传入网络。
    + 弊端：每次都要两份数据，故网络中只在G中使用这方法。

**Semi-supervised learning (半监督学习):**

- 可用任何标准分类器进行半监督学习。只需要将GAN生成的样本加入数据集中（标签为$y=K+1$），并将分类器最终输出为$K+1$维。然后有：
    $$\begin{align}
    &L=-{\Bbb E}_{x,y \sim p_{data}(x,y)}{\big [}\log p_{model}(y|x){\big]} - {\Bbb E}_{x\sim G}{\big[}\log p_{model}(y=K+1|x){\big]} \\
    &\quad= L_{supervised}+L_{unsupervised} ,where: \\
    &L_{supervised}=-{\Bbb E}_{x,y \sim p_{data}(x,y)}{\big [}\log p_{model}(y|x,y<K+1){\big]} \\
    &L_{unsupervised}=- \big\lbrace {\Bbb E}_{x\sim p_{data}(x)}\log {\big[}1- p_{model}(y=K+1|x){\big]} + {\Bbb E}_{x\sim G}\log {\big[}p_{model}(y=K+1|x){\big ]} \big\rbrace
    \end{align}$$
    + 将$D(x)=1-p_{model}(y=K+1|x)$代入表达式得：
    $$\begin{align}
    L_{unsupervised}= - \big\lbrace {\Bbb E}_{x \sim p_{data}(x)} \log D(x)+{\Bbb E}_{z \sim noise} \log (1- D(G(z)))\big\rbrace
    \end{align}$$
- 最小化$L_{supervised}$和$L_{unsupervised}$这两个损失函数可以得到最优解。
-  在实验中，使用Feature matching(特征匹配)GAN对G进行优化对于半监督学习非常有效，而使用带有minibatch discrimination(小样本鉴别)的GAN进行G训练不起作用。此处采用这些方法来展示作者实证结果，而方法对于G和D间的相互作用的全面理解将留到将来工作。 

**结论：**

- 提出了几种技术来稳定培训，使能够训练以前不可能的模型。
- 提出的评估指标（初始评分）提供了比较这些模型质量的基础。
- 运用提到的技术来解决半监督学习问题，在计算机视觉的许多不同的数据集上实现了最新的结果。

---

### 软决策树 理解DNN如何分类 2017-Distilling a Neural Network Into a Soft Decision Tree
    Nicholas Frosst, Geoffrey Hinton
    Google Brain Team

通过层级决策模型把 DNN 所习得的知识表达出来，具体决策解释容易很多。这最终缓解了泛化能力与可解释性之间的张力。

Introduction:

- 深度神经网络优秀的泛化能力依赖于其隐藏层中对分布式表征的使用，但是这些表征难以理解。
- 相比之下，很容易解释决策树是如何做出特定分类的。因为它依赖于一个相对短的决策序列，直接基于输入数据做出每个决策。
- 但是决策树并不像深度神经网络一样可以很好地泛化。与神经网络中的隐藏单元不同，决策树较低级别的典型节点仅被一小部分训练数据所使用，所以决策树的较低部分倾向于过拟合，除非相对于树的深度，训练集是指数量级的规模。
- 在测试过程中，我们使用决策树作为我们的模型。该模型的性能可能会略微低于神经网络，但速度快得多，并且该模型的决策是可解释的。

The Hierarchical Mixture of Bigots(专家的层次化混合):

- 使用小批量梯度下降训练软决策树。对于内部节点$i$，有一个学习到的过滤器$w_i$和偏置$b$，每个叶节点(left node)$l$有一个学习到的分布$Q_l$，对于每个内部节点，模型的输入$x$，$\sigma$是激活函数时，选择右分支的概率是：
$$\begin{align}
p_i(x)=\sigma(xw_i+b_i)
\end{align}$$

- 由于每个专家（决策树的点）都是Bigots(偏执固执),训练完后，对任何输入都生成相同的分布。模型学习到一个过滤器分层体系，会对每个样本分配特定的路径概率，并且每个Bigots都学习到一个简单、固定的关于所有可能输出类别K的概率,$Q_.^{\scr l}$ 表示第$l^th$层的叶的概率分布，$\phi_.^{\scr l}$ 是该叶的学习参数。
$$\begin{align}
 Q_{k}^{\scr l}=\frac{exp(\phi_{k}^{\scr l})}{\sum_{k'}exp(\phi_{k'}^{\scr l})}
\end{align}$$
- 为了避免决策树有非常的软决策，引入了inverse temperature $\beta$到激活函数，让节点$i$采取右分支的概率变为$p_i(x)= \sigma(\beta(xw_i+b_i))$ .
- 使用损失函数来训练软决策树，寻求最小化每个叶子之间的交叉熵，通过它的路径概率来加权，并且目标分布。对于输入$x$和目标分布$T$、x到达叶子$\scr {l}$$层时候的概率P^{\scr l}(x)$，对应的损失函数是：
$$\begin{align}
 L(x)=-\log{\big (} \sum _{ {\scr l}\in LeafNodes} P^{\scr l}(x) \sum_k T_k \log Q_k^{\scr l} {\big )}
\end{align}$$

Regularizer(正则)：

- 为了避免在训练时停留在差解，故引入了一个惩罚项，鼓励每个内部节点平等使用左右子树（否则一个或多个节点将几乎所有概率分给某子树时，逻辑梯度总是非常接近0）。这个惩罚是平均分布$(0.5,0.5)$和实际分布$(\alpha,1-\alpha)$间的交叉熵，$P^i(x)$是从根节点传递到节点$i$的路径概率，则有：
$$\begin{align}
 \alpha _i = \frac{\sum_x P^i(x)p_i(x)}{\sum_x P^i(x)}
\end{align}$$
- 则所有内部节点的惩罚是（其中$\lambda$是惩罚强度的超参数）：
$$\begin{align}
C=-\lambda \sum_{i \in InnerNodes} 0.5\log(\alpha_i)+0.5\log(1-\alpha_i)
\end{align}$$
- 随着树节点的深度$d$加大，节点上分类的数目将会很小（如倒数第二层可能只负责两个输入类别，不等分的比例容易很大），此时对节点进行不等分的惩罚会损害模型的准确性。故采用随着深度$d$增大，惩罚强度$\lambda$呈指数衰减。实验发现得到好的测试精度结果。
- 当一棵树下降时候，每个节点能处理的数据（对同一个批次来说）呈指数下降，这意味着用两个子树来计算实际概率变得不准确。故采用一个平均实际概率值的时间窗口（它和深度$d$呈指数函数关系），保持实际概率的指数衰减。实验发现得到好的测试精度结果。
- 软决策树开始过度拟合的参数总数通常少于多层神经网络开始过拟合的参数总数。这是因为决策树的较低节点只接收到一小部分训练数据。

MNIST上的表现：

- 对于深度为8的软决策树，当对真实目标进行训练时，我们能够达到至多94.45％的测试精度。（软决策树直接对MNIST分类）
- 2个卷积层+两层完全连接层（最后一层是Dense(10)）的神经网络的测试精度达到了99.21％
- 利用神经网络的准确性，通过训练真实标签和神经网络的预测相结合的软目标制作成的更好的软决策树，测试准确度达到了96.76％。
- 准确度在 直接去训练数据的 神经网络 和 软决策树 之间。

---





---
