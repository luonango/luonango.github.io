---
layout:     post
title:      杂乱的几篇论文阅读笔记
subtitle:   论文技术分享：feature maps
date:       2018-01-01
author:     BY
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - CNN
    - 网络结构
    - 模型压缩
    - 集成

---


# 随便记录一些论文



## 模型压缩加快

### 1. 模型压缩Deep Compression：2016《Deep Compression: Compression Deep Neural Networks With Pruning, Trained Quantization And Huffman Coding》
    Han S, Mao H, Dally W J. Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding[J]. Fiber, 2015, 56(4):3--7.
    2016 ICLR最佳论文

参考：

- [Deep Compression阅读理解及Caffe源码修改](http://blog.csdn.net/may0324/article/details/52935869)

Deep Compreession 实现有三步：

**Prunning（权值修剪）、Quantization（权值共享和量化）、Huffman Encoding（Huffman编码)**

- Prunning（权值修剪）:
    + 1.正常训练模型得到权值
    + 2.将低于阈值的权值设为0
    + 3.重新训练得到的新权值
    + 权值修剪后的稀疏网络用CSC(compressed sparse column)或者CSR(compressed sparse row)表示。
    + 用下标法记录权值参数:记录非零值和其在数组的下标，并用差分形式表示下标（记录和上一个数值的位置距离，如果4bits不够表示距离，则填1111，并把那个非零值记为0.
- Quantization（权值共享和量化）：
    + K-mean方法吧所有参数聚成$2^n$个类，聚类初始化有： 
> * 随机初始化： 随机产生K个观察值做为聚类中心。
> * 密度分布初始化： 现将累计概率密度CDF的y值分布线性划分，然后根据每个划分点的y值找到与CDF曲线的交点，再找到该交点对应的x轴坐标，将其作为初始聚类中心。 
> * 线性初始化： 将原始数据的最小值到最大值之间的线性划分作为初始聚类中心
> * 而 **线性初始化方式**则能更好地保留大权值中心，因此文中采用这一方式。
    + 同一聚类的参数相等，对网络调优。梯度下降时候，归于同一类的节点的梯度相加，施加到该类的聚类中心上（而不是每个节点参数上）
    + 使用n比特编码的聚类中心替代原有参数 
- Huffman Encoding（Huffman编码):
    + 编码为n位的非零数据取值；
    + 编码为4位的非零元素下标。 
    + 这两者的分布都不均匀，可以使用Huffman编码进一步压缩存储

### 2. 模型压缩SqueezeNet：2016《SqueezeNet- AlexNet-level accuracy with 50x fewer parameters and 0.5MB model size》
    Iandola F N, Han S, Moskewicz M W, et al. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size[J]. 2016.
    UC Berkeley和Stanford

参考：
- [神经网络瘦身：SqueezeNet](https://www.jianshu.com/p/8e269451795d)

- 用了Deep Compression的方法（上一篇）
- “多层小卷积核”策略成功的根源：
    + 内存读取耗时要远大于计算耗时。
- Dense→Sparse→Dense 的SDS训练法:
    + 本文还有一个神奇的发现：使用裁剪之后的模型为初始值，再次进行训练调优所有参数，正确率能够提升4.3%。 
    + 稀疏相当于一种正则化，有机会把解从局部极小中解放出来。这种方法称为DSD(dense->sparse->dense)

### 3. 交错组卷积（IGC，Interleaved Group Convolution）《Interleaved Group Convolutions for Deep Neural Networks》
    微软 ICCV 2017

参考：

- [王井东详解ICCV 2017入选论文：通用卷积神经网络交错组卷积](https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&mid=2649441412&idx=1&sn=76b8e24616a4cdc07fbf985798ef4942&chksm=82c0ad00b5b724163561a9174f8213d365ca87f5d73c7ec278ac358293b55e7dcb9e488b1eb8&mpshare=1&scene=1&srcid=0731zmN8ulFwvXY4HRh8vpKs#rd)

从通道角度出发，解决了神经网络基本卷积单元中的冗余问题。可以在无损性能的前提下，缩减模型、提升计算速度，有助于深度网络在移动端的部署.

每个交错组卷积模块包括两个组卷积过程。不同组的卷积不存在交互，不同组的输出通道并不相关。

为了让不同组的通道相关联，引入第二次组卷积：第二次的每组的输入通道均来自第一组输出的不同的组的通道（交错组成）。

![](https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Interleaved_Group_Convolutions_for_Deep_Neural_Networks_1.png)

Xception可以看成是特例：

- 如果第一次组的每组只有一个输出通道，那么就变成了特殊的组卷积，即 channel-wise convolution，第二次组卷积成为 1X1 的卷积，这与 Xception 相似；
- 如果第一次组卷积过程里仅有一组，那么这个过程就变成了普通卷积，第二次组卷积过程则相当于分配给每个通过一个不同的权重。
- 极端情况下:网络的性能会随着通道数及组数的变化而变化，最优性能配置点存在于两个极端情况之间。

---

### 4. 通道随机分组ShuffleNet：《ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices 》
    Face++ 2017
    ShuffleNet_An_Extremely_Efficient_Convolutional_Neural_Network_for_Mobile_Devices_

参考：

- [ShuffleNet算法详解](http://m.blog.csdn.net/u014380165/article/details/75137111)
- [变形卷积核、可分离卷积？卷积神经网络中十大拍案叫绝的操作](https://zhuanlan.zhihu.com/p/28749411)

降低深度网络计算量。

采用**Channel-Shuffle、Pointwise和Depthwise**来修改原来的ResNet单元.

- Channel Shuffle:
![](https://github.com/luonango/luonango.github.io/raw/master/img/pictures/ShuffleNet_An_Extremely_Efficient_Convolutional_Neural_Network_for_Mobile_Devices_1.png) 

- pointwise group convolution：
    + 带组的卷积核为1*1的卷积。

- depthwise separable convolution，将传统卷积过程分两步：
    + 如先用M个3*3卷积核一对一卷积成M个特征图。卷积时候互不相交。
    + 接着用N个1*1的卷积核对这M个特征图全部卷积（正常卷积），生成N个特征图。

> ShuffleNet的核心:
> 
>    用pointwise group convolution，channel shuffle和depthwise separable convolution,代替ResNet block的相应层构成了ShuffleNet uint，达到了减少计算量和提高准确率的目的。
>    
>    channel shuffle解决了多个group convolution叠加出现的边界效应，pointwise group convolution和depthwise separable convolution主要减少了计算量
>

增强了全局信息的流通，超少量的参数。

相比Xception和Mobilenet，ShuffleNet都获得了性能上的提升，至于速度带来的提升则不是很明显。
以及超越mobilenet、媲美AlexNet的准确率.


>微软亚洲研究院MSRA最近也有类似的工作，他们提出了一个IGC单元（Interleaved Group Convolution,如上），即通用卷积神经网络交错组卷积，形式上类似进行了两次组卷积。

>要注意的是，Group conv是一种channel分组的方式，Depthwise +Pointwise是卷积的方式，只是ShuffleNet里面把两者应用起来了。
>
>因此Group conv和Depthwise +Pointwise并不能划等号。
>

---

### 5. SENet 2017《Squeeze-and-Excitation Networks》
    2017 ImageNet 冠军
    CVPR，国内自动驾驶创业公司Momenta

##### 特点：
- 考虑特征通道间的关系，动态调整各通道的特征响应值
- 构造简单、容易被部署。增加很小的计算消耗，获得高的性能提升
- 可能用于辅助网络修剪/压缩的工作

##### 模块构造
|Squeeze-and-Excitation 模块|
|-|
|![](https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Squeeze-and-Excitation_Networks_1.jpg)|
|图中输入是X，有$c_1$特征通道。经过一系列卷积变换后，得$c_2$个大小为$w×h$的通道（map）|
|**Squeeze**： 顺着空间维度进行特征压缩，每个二维特征通道变成一个实数，如图得到$1×1×c_2$.表征着特征通道相应的全局分布.全局平均池化来生成各通道的统计量|
|**Excitation**: 考察各通道的依赖程度,W被学习表示特征通道间的相关性。最后sigmoid的输出就是各通道的权重，根据输入数据调节各通道特征的权重，有助于增强特征的可分辨性|
|**Reweight**: 通过乘法逐通道加权到先前特征，完成对原始特征的重标定。|

##### 示例：部署简单，如插件
|SE_Inception图示|SE_ResNet图示|
|-|-|
|![](https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Squeeze-and-Excitation_Networks_4.png)|![](https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Squeeze-and-Excitation_Networks_5.png)|

#####　实验情况：增加小的计算消耗，获得大的性能提升
||
|-|
|![](https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Squeeze-and-Excitation_Networks_3.png)|
|![](https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Squeeze-and-Excitation_Networks_6.jpg)|

---
## 模型变换

### 1. 可变形卷积网络2017《Deformable Convolutional Networks》
    微软亚洲研究院.可变卷积和可变ROI采样
    Dai J, Qi H, Xiong Y, et al. Deformable Convolutional Networks[J]. 2017.
    Deformable_Convolutional_Networks_

参考：

- [https://github.com/msracver/Deformable-ConvNets](https://github.com/msracver/Deformable-ConvNets)
- [【VALSE 前沿技术选介17-02期】可形变的神经网络](http://valser.org/thread-1261-1-1.html)
- [目标检测方法——R-FCN](https://www.cnblogs.com/lillylin/p/6277094.html)
- [Deformable ConvNets论文笔记](http://m.blog.csdn.net/yucicheung/article/details/78113843)

对卷积核每个采样点位置都增加偏移变量。用convolution去学习dilatation值，和STN异曲同工。

引入了两种模块：**deformable convolution** 、 **deformable RoI pooling**


> 对象尺度，姿态，视点和部分形变的几何变换一般有两种方法：
> 
> - 建立足够期望变化的训练数据集，通常增加数据样本，如仿射变换等，从数据集中学习鲁棒表示。但需要昂贵的训练和复杂的模型参数
> - 使用变换不变的特征和算法，如SIFT（尺度不变特征变换）、基于滑动窗口的检测范例。
> 
> 上述的几何变换是固定已知的。这样的先验知识的假设阻止了未知的几何变换，并且，人工设计的特征和算法对于过度复杂的变换是困难不可行的。

1. **Deformable Convolution:**
    + 将2D偏移量加入标准卷积中的网格区，使网格区自由形变（再进行卷积）。偏移量通过附加的卷积层从前面的特征图中学习。因此，变形以局部的，密集的和自适应的方式受到输入特征的限制。
    + |||
      |-|-|
      |(a)是标准卷积的采样网格。<br>(b)是变形后采样位置和变形卷积中的偏移量箭头<br>(c)和(d)是形变后的特殊情况。<br><br>表明可变形卷积推广了各种尺度(各向异性)纵横比和旋转的变换|![](https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Deformable_Convolutional_Networks_1.png)|

    + 
    |||
      |-|-|
      |在相同的输入特征图上应用卷积层，获得偏移。2N的通道表示N个通道的2维偏移。<br><br>训练时，同时学习偏移量的卷积核参数以及普通卷积核参|![](https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Deformable_Convolutional_Networks_2.png)|
      

2. **Deformable RoI(region-of-interest) pooling**
    + 为RoI（region-of-interest）池的常规bin分区中的每个bin位置添加一个偏移量。（池化时，池化区的每个位置都加入对应偏移量，再进行池化）。<br>类似地，从前面的特征映射和RoI学习偏移，使得具有不同形状的对象的自适应部分定位成为可能。
    + |||
      |-|-|
      |fc层产生归一化偏移量，再归一化偏移量（让偏移的学习不因RoI尺寸不同而变化）|![](https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Deformable_Convolutional_Networks_3.png)|
3. **Position-Sensitive (PS) RoI Pooling** （目标检测方法R-FCN提及到）
    + 位置敏感RoI池是完全卷积的,[目标检测R-FCN中率先提出position sensitive score map概念](https://www.cnblogs.com/lillylin/p/6277094.html)。
    + |deformable PS RoI pooling||
      |-|-|
      |下面分支：通过一个卷积层，所有的 input 特征图先被转化。每个物体类别都生成$k^2$个分数图（如果物体类别是C，那么总共有C+1个,C类物体外加1个背景）<br><br>上面分支：通过卷积层，生成完全的空间像素偏移域。<br><br>对每个RoI（同时还指定了某个类别），PS RoI pooling会被应用在这些域上以获取归一化的偏移量$\Delta \hat{p}_{ij}$，然后通过上述deformable RoI pooling中一样的方法变成真正的偏移量$\Delta p_{ij}$。|![](https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Deformable_Convolutional_Networks_4.png)|


梯度是通过双线性运算反向传播，详情阅读论文公式及附录。 

---


## 集成方面：

### 1. 数据上的集成：

#### 对图片转换，半监督《Data Distillation: Towards Omni-Supervised Learning》
    Ilija Radosavovic Piotr Dollar Ross Girshick Georgia Gkioxari Kaiming He 
    Facebook AI Research (FAIR)
    人体姿态估计   物体识别框

- 一些参考：
    [如何评价FAIR的最新工作Data Distillation？](https://www.zhihu.com/question/264009268/answer/275800268)

思想主要如图：
![](https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Data_Distillation_Towards_Omni-Supervised_Learning_1.png)

**数据变换：**

- 选择多变换推理的几何变换（使用**缩放和水平翻转**）

**在未标记的数据上生成标签（聚合多变换推断（Multi-transform inference）的结果）：**

- 将输入的数据进行多种变换，都应用于相同的模型，然后汇总结果。
- 某数据可能某单个变换比任何其他的变换得到的模型预测更好。
    + 观察发现，聚合预测产生新的知识，使用这些信息生成的标签，原则上能让模型进行学习。

- 生成的标签可能是 **“软标签”**（即是概率向量，而非分类标签），重新训练时候如果考虑到概率值，则要重新设计损失函数。且对于结构化输出空间的问题，如对象检测或人体姿态估计，**平均输出没有意义**。

    + 故本文**只聚合那些和真实标签有相同结构、类型（或分布）的“硬标签”**：
        * **额外设计专用逻辑结构**来处理，如非极大值抑制来组合多组方框之类的做法。
    + 期望预测的框和关键点足够可靠，可以生成良好的培训标签：
        * 以预测的检测分数作为预测质量的评估，**超过某个分数阈值的预测才能产生注释**。
        * **专用逻辑结构**：(作者自己设计)发现如果使得“每个未标记图像的预测标注实例的平均数量”大致等于“每个真实标记图像的标注实例的平均数量”，则分数阈值运行良好。虽然这种启发式假定未标记和标记的图像遵循**类似的分布**，但是作者发现它是稳健的并且即使在假设不成立的情况下也工作良好。

**看法：**

- **从数据而非模型来思考集成**。
- 不知道 **数据转换是静态转换，还是可学习的转换（学习转换矩阵Ｗ)**，文中似乎是前者，但后者或许更佳。
- 感觉是作者添加的**专用逻辑结构起了很大作用**。即**添加了发现的规律**，增加了额外有用信息（先验知识），从而督促了网络学习吧。
>　发现如果“每个未标记图像的预测标注实例的平均数量”大致等于“每个真实标记图像的标注实例的平均数量”，则分数阈值运行良好.
 