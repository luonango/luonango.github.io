<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>BY Blog</title>
    <description>Every failure is leading towards success.</description>
    <link>http://luonango.github.io/</link>
    <atom:link href="http://luonango.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 06 Oct 2018 11:03:00 +0800</pubDate>
    <lastBuildDate>Sat, 06 Oct 2018 11:03:00 +0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Fusedmax与Oscarmax</title>
        <description>&lt;h1 id=&quot;fusedmaxoscarmaxattention&quot;&gt;2017_Fusedmax与Oscarmax_稀疏及结构化的Attention正则化框架&lt;/h1&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2017_NIPS
康奈尔大学, Vlad Niculae
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;论文提出&lt;strong&gt;让Attention输出稀疏&lt;/strong&gt;且更关注&lt;strong&gt;输入数据的片段或组&lt;/strong&gt;的正则化机制，它还能直接加入神经网络进行前向与反向传播。&lt;/p&gt;

&lt;p&gt;论文地址: &lt;a href=&quot;https://papers.nips.cc/paper/6926-a-regularized-framework-for-sparse-and-structured-neural-attention.pdf&quot;&gt;A Regularized Framework for Sparse and Structured Neural Attention&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;1. 简单介绍:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Attention的关键是映射函数&lt;/strong&gt;，它对输入元素的相对重要性进行编码，将数值映射为概率。而常用的关注机制&lt;strong&gt;Softmax会产生密集的注意力&lt;/strong&gt;。因为softmax的输出都大于0，故其输入的所有元素对最终决策都有或多或少的影响。&lt;/p&gt;

&lt;p&gt;为了克服softmax这种缺点，&lt;a href=&quot;https://arxiv.org/pdf/1602.02068.pdf&quot;&gt;From Softmax to Sparsemax:A Sparse Model of Attention and Multi-Label Classification&lt;/a&gt; 提出了能够&lt;strong&gt;输出稀疏概率的Sparsemax&lt;/strong&gt;，这能为Attention提供更多的解释性。&lt;/p&gt;

&lt;p&gt;本文基于Sparsemax，提出的方法既可以得到稀疏的输出，又可以作为一个诱导稀疏的惩罚模型，可作用于当前的其他模型上。 本文提出的通用框架是&lt;strong&gt;建立在$\max$运算符上的，采用强凸（strong convex）函数进行调整后得到的算子是可微的，它的梯度定义了从输入数值到概率的映射，适合作为Attention机制&lt;/strong&gt;。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;基于fused lasso提出了&lt;strong&gt;Fusedmax鼓励网络对连续区域（或说文本段）的Attention值相同&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;限定连续区域的Attention值相同的假设太严苛，文中又基于oscar提出&lt;strong&gt;Oscarmax的关注机制，鼓励网络对不连续的单词组的Attention值相同&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上述只是文章简单概括。 疑问有很多，比如：建立在$\max$算子是什么意思？强凸函数怎么调整？梯度定义了映射函数？Fusedmax和Oscarmax又是怎么做到的？&lt;/p&gt;

&lt;p&gt;那下面就一步步解释文章思路。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;## 2. 论文的思路:&lt;/p&gt;

&lt;p&gt;先给出一些定义：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;集合${1,2,…,d}$定义为$[d]$，$d-1$维的单形体为$\Delta^d:={x\in R^d:\mid\mid x\mid\mid_1=1,x \geq0}$， 在欧几里得的投影为$P_{\Delta^d}(x):={\arg\min}_{y\in \Delta^d}\mid\mid y-x\mid\mid^2$&lt;/li&gt;
  &lt;li&gt;如果函数$f:R^d\rightarrow R\bigcup {\infty}$, 则其凸共轭(convex conjugate)为$f^{&lt;em&gt;}(x):=\sup_{y\in dom\;f}y^Tx-f(y)$.  给定范数$\mid\mid\cdot\mid\mid$,它的对偶定义为$\mid\mid x\mid\mid_&lt;/em&gt; :=\sup_{\mid\mid y\mid\mid \leq 1}y^T x$. 用$\partial f(y)$ 表示函数$f$在$y$处的次微分 &lt;br /&gt;
&amp;gt; 次微分subdifferential,凸函数$f(x)=\mid x\mid$在原点的次微分是区间$[−1, 1]$.&lt;/li&gt;
  &lt;li&gt;函数$f$的Jacobian(雅可比)$J_{g}(y)\in R^{d\times d}$,Hessian(海森矩阵)$H_{f}(y)\in R^{d\times d}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;现在我们看下$max$算子（虽然是$R^d \rightarrow \Delta^d$的映射函数，但不适合作为Attention机制):&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\max(x):=\max_{i\in [d]} x_i = \sup_{y\in \Delta^d} y^Tx&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;由于在单形体的线性上确界总是顶点，也即为标准基向量${e_i}^d_{i=1}$的其中之一。 这也容易看出这个上确界$y^&lt;em&gt;$ 就是$\max(x)$的一个次梯度$\partial \max(x) ={e_{i^&lt;/em&gt;} : i^* \in \arg\max_{i\in [d]} x_i}$.&lt;/p&gt;

&lt;p&gt;我们将这些次梯度看作一种映射：$\prod:R^d \rightarrow \Delta^d$,它将所有的概率质量都放在一个元素上(即$\max$操作只有一个元素获得非0输出):$\prod(x) = e_i,\; for \;any\; e_i \in \partial \max(x)$.&lt;/p&gt;

&lt;p&gt;因为这映射函数不连续（存在阶跃断点），这不适合通过梯度下降法进行优化,显然这种属性我们是不希望的.&lt;/p&gt;

&lt;p&gt;从这可以看出，$\max(x)$的次梯度$y^*$是$\prod:R^d \rightarrow \Delta^d$映射，&lt;strong&gt;如果$\max(x)$的变种函数是连续可二次微分，那$y^&lt;em&gt;$也就可以用作Attention，且也能够让梯度下降方法进行优化了（梯度为$y^&lt;/em&gt;$的导数）&lt;/strong&gt;。&lt;br /&gt;
&amp;gt;注意，此处$\prod(x)$也表示Attention的输出，如果$\prod(x)$可导，就可以嵌入普通神经网络进行梯度下降优化了。&lt;/p&gt;

&lt;p&gt;受到&lt;a href=&quot;http://luthuli.cs.uiuc.edu/~daf/courses/optimization/MRFpapers/nesterov05.pdf&quot;&gt;Y.Nesterov. Smooth minimization of non-smooth functions&lt;/a&gt;的启发,本文运用了Smooth技术. 对于$\max(x)$的共轭函数${\max}^&lt;em&gt;(y)$:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
{\max}^*(y)=\left\{
             \begin{array}{l}
             0, &amp; if\; y\in \Delta^d \\
             \infty, &amp; o.w. 
             \end{array}\right. %]]&gt;&lt;/script&gt; &lt;br /&gt;
&amp;gt;该共轭函数的证明在&lt;a href=&quot;http://papers.nips.cc/paper/5710-smooth-and-strong-map-inference-with-linear-convergence&quot;&gt;Smooth and strong: MAP inference with linear&lt;br /&gt;
convergence, Appendix B&lt;/a&gt; 中。其实通过求解共轭函数的方法就可以得解：$max^&lt;/em&gt;(y)=\sup(y^Tx-\sup z^Tx)$, 对$x$求偏导得$y^&lt;em&gt;=z^&lt;/em&gt;$再带入原式即可。&lt;/p&gt;

&lt;p&gt;那现在将正则化添加到共轭函数中：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
{\max}^*_{\Omega}(y)=\left\{
             \begin{array}{l}
             \gamma\Omega(y), &amp; if\; y\in \Delta^d \\
             \infty, &amp; o.w. 
             \end{array}\right. %]]&gt;&lt;/script&gt; &lt;br /&gt;
其中假设函数$\Omega:R^d\rightarrow R$是关于norm $\mid\mid\cdot\mid\mid$的&lt;strong&gt;$\beta$-strongly convex&lt;/strong&gt;($\beta强凸$)。$\gamma$ 控制着正则强度。&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;参考&lt;a href=&quot;https://cswhjiang.github.io/2015/04/08/strong-convexity-and-smoothness/&quot;&gt;Strong Convexity and Smoothness&lt;/a&gt;：&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;函数$f$是 $\alpha$-strong convex($\alpha &amp;gt; 0$)需要满足条件：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;f(y)-f(x)\geq \nabla f(x)^T(y-x) + \frac{\alpha}{2}\mid\mid y-x \mid\mid^2_P&lt;/script&gt;&lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;如果一个连续可微的函数$f$的梯度$\nabla f(x)$是$\beta$-Lipschitz的，即：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\mid\mid\nabla f(x)-\nabla f(y)\mid\mid \leq\beta\mid\mid x-y\mid\mid,&lt;/script&gt;&lt;br /&gt;
那么我们称 $f(x)$ 是$\beta$-smooth的,更一般表示为：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\mid\mid\nabla f(x)-\nabla f(y)\mid\mid_D \leq\beta\mid\mid x-y\mid\mid_P,&lt;/script&gt;&lt;br /&gt;
其中$\mid\mid\cdot\mid\mid_P$是范数norm，$\mid\mid\cdot\mid\mid_D$是对偶范数dual norm。&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;函数的$\alpha$-strongly convex和 $\beta$-smoothness有对偶关系，如果函数$f$是关于对偶范数 $\mid\mid\cdot\mid\mid_D$ 的$\beta$-smooth,那么$f^&lt;em&gt;$是关于范数$\mid\mid\cdot\mid\mid_P$的$\frac{1}{\beta}$-strongly convex。 其中$f^&lt;/em&gt;=\max_y(y^Tx-f(y))$是函数$f(x)$的共轭函数。&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;Wiki中也可以查阅详细定义与解释。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;为了定义出平滑的$\max$算子: $\max_{\Omega}$，再次使用共轭：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;{\max}_{\Omega}(x)=max_{\Omega}^{**}(x) = \sup_{y\in R^d} y^Tx - {\max}^*_{\Omega} (y)=\sup_{y\in\Delta^d}y^Tx-\gamma\Omega(y)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;由此，前面提到的映射${\prod}&lt;em&gt;{\Omega}: R^d\rightarrow \Delta^d$定义为：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;{\prod}_{\Omega}(x):=\arg\max_{y\in \Delta^d}y^Tx-\gamma\Omega(y)=\nabla max_{\Omega}(x)&lt;/script&gt;&lt;br /&gt;
&amp;gt; 1. 上述式子可由：$\max&lt;/em&gt;{\Omega}(x)=(y^&lt;em&gt;)^Tx-\max^&lt;/em&gt;&lt;em&gt;{\Omega}(y^*)\Longleftrightarrow y^* \in \partial\max&lt;/em&gt;{\Omega}(x)$ 证得。&lt;br /&gt;
&amp;gt; 2. $\partial\max_{\Omega}(x)={\nabla\max_{\Omega}(x)}$只有唯一解（$y^*$是单形体的顶点）。故$\prod_{\Omega}$ 是梯度的映射函数。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;强凸的重要性：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对$\Omega$的$\beta$-strongly convex的假设是很重要的，如果函数$f:R^d\rightarrow R$ 的共轭函数 $f^*$ 是关于对偶范数 $\mid\mid\cdot\mid\mid_D$ 的$\frac{1}{\beta}$-smooth, 那么 $f$ 就是关于范数 $\mid\mid\cdot\mid\mid_P$ 的 $\beta$-strongly convex。那么这足以确保 $\max_{\Omega}$是$\frac{1}{\gamma\beta}$-smooth, 或者说 $\max_{\Omega}$ 处处可微，且它的梯度 $\prod_{\Omega}$ 在对偶范数 $\mid\mid\cdot\mid\mid_D$上是 $\frac{1}{\gamma\beta}$-Lipschitz的.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;训练问题（$\prod_{\Omega}$即表示Attention的输出）：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;前向时，需要解决的问题是：如何计算 $\prod_{\Omega}$ ?&lt;/li&gt;
  &lt;li&gt;反向时，需要解决的问题是：如何计算 $\prod_{\Omega}$ 的雅可比矩阵？或说如何计算 $\max_{\Omega}$ 的Hessian矩阵？&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;## 3. 验证方法：用正则项 $\Omega$恢复Softmax与Sparsemax&lt;/p&gt;

&lt;p&gt;在推导新的Attention机制前，先展示如何用正则项$\Omega$恢复出softmax和sparsemax。&lt;/p&gt;

&lt;h3 id=&quot;softmax&quot;&gt;3.1 Softmax：&lt;/h3&gt;

&lt;p&gt;选择$\Omega(y)=\sum_{i=1}^d y_i \log y_i$ ,即负熵。则它的共轭函数为 $log\;sum\;exp$, 即$f^*(y)=\log \sum_{i=1}^d e^{y_i}$.&lt;br /&gt;
&amp;gt;证明此时$\Omega(y)$的共轭函数为 $log\;sum\;exp$ :&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;若函数 $f(x)=\log \sum_{i=1}^n e^{x_i}$ ,则由$f^*(y)=sup y^Tx-f(x)$ 对$x$求偏导得：$y_i=\frac{e^{x_i}}{\sum_{i=1}^n e^{x_i}}$ , 即$1^Ty=1,\sum y_i =1$.&lt;/p&gt;

  &lt;p&gt;故有$e^{x_i}=y_i\sum e^{x_i} \Rightarrow$&lt;/p&gt;

  &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{array}{l}
f^*(y) &amp; =\sum y_i x_i - log\sum e^{x_i} \\
&amp; =\sum y_i \log(y_i\sum e^{x_i})-\log\sum e^{x_i} \\
&amp; = \sum y_i\log y_i + \sum y_i \log\sum e^{x_i}-\log\sum e^{x_i} \\
&amp; = \sum y_i\log y_i
\end{array} %]]&gt;&lt;/script&gt;&lt;br /&gt;
由于$f(x)$是凸函数（可证），所以$f^{**}=log\;sum\;exp$.&lt;br /&gt;
也可以查阅&lt;a href=&quot;https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf&quot;&gt;《Convex optimization》&lt;/a&gt;的习题3.25, 里面有详细证明。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果 $f(x)=\gamma g(x)$,则对于 $\gamma &amp;gt;0$,有 $f^&lt;em&gt;(y)=\gamma g^&lt;/em&gt;(y/\gamma)$. 则${\max}&lt;em&gt;{\Omega}(x)=\gamma \log\sum&lt;/em&gt;{i=1}^d e^{x_i / \gamma}$.&lt;/p&gt;

&lt;p&gt;由于$\Omega(y)$负熵在 $\mid\mid\cdot\mid\mid_1$ 是 1-strongly convex，所以${\max}&lt;em&gt;{\Omega}$在 $\mid\mid\cdot\mid\mid&lt;/em&gt;{\infty}$ 上是$\frac{1}{\gamma}$-smooth.&lt;/p&gt;

&lt;p&gt;通过对 ${\max}_{\Omega}$ 的 $x$ 求偏导即得softmax：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;{\prod}_{\Omega}(x) = \frac{\partial {\max}_{\Omega}}{\partial x}=\frac{e^{x/\gamma}}{\sum_{i=1}^d e^{x_i/\gamma}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;其中 $\gamma$越小，输出的 $softmax$就越尖锐。&lt;a href=&quot;https://arxiv.org/abs/1503.02531?context=cs&quot;&gt;Distilling the Knowledge in a Neural Network&lt;/a&gt; 里面也涉及到$\gamma$的设计。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;### 3.2 Sparsemax：&lt;/p&gt;

&lt;p&gt;选择 $\Omega(y)=\frac{1}{2}\mid\mid y\mid\mid^2_2$, 它也被称为Moreau-Yosida正则化常用于近端算子理论。&lt;/p&gt;

&lt;p&gt;由于 $\frac{1}{2}\mid\mid y\mid\mid^2_2$ 在$\mid\mid\cdot\mid\mid_2$ 中是 1-strongly convex，所以在${\max}_{\Omega}$在$\mid\mid\cdot\mid\mid_2$上是$\frac{1}{\gamma}$-smooth.&lt;/p&gt;

&lt;p&gt;由此能得：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;{\prod}_{\Omega}(x)=P_{\Delta^d}(x/\gamma) = \arg\min_{y\in \Delta^d}\mid\mid y-x/\gamma\mid\mid^2&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;当$\gamma=1$时，上式就为Sparsemax（softmax的一个稀疏替代）。&lt;/p&gt;

&lt;p&gt;这推导得出：调控 $\gamma$ 可以控制稀疏性。根据sparsemax的论文&lt;a href=&quot;https://arxiv.org/pdf/1602.02068.pdf&quot;&gt;From softmax to sparsemax: A sparse model of attention and multi-label classification&lt;/a&gt;中的公式 $9$可以知道 ${\prod}&lt;em&gt;{\Omega}$ 的雅可比矩阵为:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;J_{ {\prod}_{\Omega}}(x)=\frac{1}{\gamma}J_{P_{\Delta^d}}(x/\gamma)=\frac{1}{\gamma}\big(diag(s)-ss^T/\mid\mid s\mid\mid_1\big)&lt;/script&gt;&lt;br /&gt;
&amp;gt; 其中 $s\in{0,1}^d$ 指示着 ${\prod}&lt;/em&gt;{\Omega}(x)$ 的非$0$元素。&lt;br /&gt;
&amp;gt; &lt;br /&gt;
&amp;gt; ${\prod}_{\Omega}(x)$ 是Lipschitz 连续，处处可导。&lt;br /&gt;
&amp;gt; &lt;br /&gt;
&amp;gt; 详情建议阅读sparsemax的论文。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;## 4. Fusedmax与Oscarmax 新Attention机制&lt;/p&gt;

&lt;p&gt;论文提出 **论点 $1$：如果 $\Omega$ 函数可微，那可以计算出 ${\prod}&lt;em&gt;{\Omega}$ 的雅可比矩阵 .**&lt;br /&gt;
&amp;gt; 论文附录 $A.1$ 已证明，只要提供了$\Omega$的Jacobian和Hessian，那就可以根据论文提供的公式计算出 $J&lt;/em&gt;{ {\prod}_{\Omega}}$ .&lt;/p&gt;

&lt;p&gt;那来一个简单例子吧。&lt;/p&gt;

&lt;h3 id=&quot;squared-p-norms-p-&quot;&gt;4.1. 示例：Squared p-norms（平方 p-范式）&lt;/h3&gt;

&lt;p&gt;Squared p-norms作为单纯形上可微函数的一个有用例子：$\Omega(y)=\frac{1}{2}\mid\mid y\mid\mid^2_p = \big(\sum_{i=1}^d y_i^p \big)^{2/p}$ , 其中 $y\in\Delta^d$ 且 $p\in(1,2]$.&lt;/p&gt;

&lt;p&gt;我们已知 squared p-norm 在$\mid\mid\cdot\mid\mid_p$是strongly convex, 这也指出，当$\frac{1}{p} + \frac{1}{q} = 1$时， ${\max}_{\Omega}$ 在 $\mid\mid\cdot\mid\mid_q$ 是$\frac{1}{\gamma(p-1)}$-smooth 。计算出sq-pnorm-max为：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;{\prod}_{\Omega}(x)= \arg\min_{y\in \Delta^d} \frac{\gamma}{2}\mid\mid y\mid\mid^2_p - y^Tx&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;论点$1$ 所需要的梯度和Hessian可以通过 $\nabla\Omega(y)=\frac{y^{p-1}}{\mid\mid y\mid\mid^{p-2}&lt;em&gt;p}$ 以及下式得到所需要的$J&lt;/em&gt;{ {\prod}_{\Omega}}$.&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;H_{\Omega}(y)=diag(d)+ uu^T,\quad where \quad d=\frac{(p-1)}{\mid\mid y\mid\mid^{p-2}_p}y^{p-2} \quad and \quad u=\sqrt{\frac{(2-p)}{\mid\mid y\mid\mid^{2p-2}_p}} y^{ p-1}&lt;/script&gt;&lt;br /&gt;
&amp;gt; sq-pnorm-max 的 $p = 2$ 时就恢复为 sparsemax 一样鼓励稀疏输出。 &lt;br /&gt;
&amp;gt; &lt;br /&gt;
&amp;gt; 但$1&amp;lt;p&amp;lt;2$ 时，$y^&lt;em&gt;=[0,1]$和$y^&lt;/em&gt;=[0,1]$ 间的转换将会更平滑。所以在实验中采用 $p=1.5$ 。详情可以查阅论文中对比图的分析与实验。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;### 4.2. Fusedmax与Oscarmax&lt;/p&gt;

&lt;p&gt;上文已经采用Squared p-norm 例子展示了论点$1$（或说这一套解决方案）的可行性之后，接下论文将提出适合作为Attention机制的可微且$\beta$-strongly convex 的 $\Omega$ 函数。&lt;br /&gt;
&amp;gt; 下面会讲到Fusedmax和Oscarmax，其中会涉及到TV(全变分)和OSCAR(用于回归的八边形收缩和聚类算法)。 &lt;br /&gt;
&amp;gt; &lt;br /&gt;
&amp;gt; TV和OSCAR在文章后面会有单独解释。&lt;/p&gt;

&lt;h4 id=&quot;fusedmax&quot;&gt;Fusedmax&lt;/h4&gt;
&lt;p&gt;当输入是连续且顺序是有一定意义的时（如自然语言），我们希望能&lt;strong&gt;鼓励让连续区域的文本段有着相同的Attention值&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;为此，基于&lt;a href=&quot;https://web.stanford.edu/group/SOL/papers/fused-lasso-JRSSB.pdf&quot;&gt;Sparsity and smoothness via the fused lasso&lt;/a&gt;的fused lasso（或称 1-d total variation(TV))，选择$\Omega(y)=\frac{1}{2}\mid\mid y\mid\mid^2_2 + \lambda\sum_{i=1}^{d-1}\mid y_{i+1} - y_i\mid$, 即为强凸项和1-d TV惩罚项的和。 故可以得到：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;{\prod}_{\Omega}(x)= \arg\min_{y\in {\Delta}^d}\frac{1}{2}\mid\mid y-x/\gamma\mid\mid^2 + \lambda\sum^{d-1}_{i=1}\mid y_{i+1}- y_i\mid.&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;oscarmax&quot;&gt;Oscarmax&lt;/h4&gt;

&lt;p&gt;由于TV让连续区域聚集（即鼓励连续区域的attention值相等），这样的前提假设太严格，于是作者参考&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2007.00843.x&quot;&gt;Simultaneous regression shrinkage, variable selection, and&lt;br /&gt;
supervised clustering of predictors with OSCAR&lt;/a&gt;中的OSCAR惩罚函数，以来鼓励对元素进行聚类，让同一集群的元素它们的Attention值相等。即&lt;strong&gt;鼓励对可能不连续的单词组给予同等重视&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;选择$\Omega(y)=\frac{1}{2}\mid\mid y\mid\mid^2_2 + \lambda\sum_{i&amp;lt;j} \max(\mid y_i\mid,\mid y_j\mid)$， 故可得到：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
{\prod}_{\Omega}(x)= \arg\min_{y\in {\Delta}^d}\frac{1}{2}\mid\mid y-x/\gamma\mid\mid^2 + \lambda\sum_{i&lt;j}\max(\mid y_i\mid,\mid y_j\mid). %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;fusedmaxoscarmax&quot;&gt;4.3. Fusedmax与Oscarmax的计算与其雅可比矩阵&lt;/h3&gt;

&lt;p&gt;根据论文中的论点$2$和论点$3$, 就可得出Fusedmax和Oscarmax分别对应的雅可比矩阵了。&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
[J_{P_{TV}}(x)]_{i,j} =\left\{
             \begin{array}{l}
             \frac{1}{\mid G^*_i\mid}  &amp; if\; j\in G^*_i \\
             0 &amp; o.w. 
             \end{array}\right. %]]&gt;&lt;/script&gt; &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
[J_{P_{OSC}}(x)]_{i,j} =\left\{
             \begin{array}{l}
             \frac{sign(z^*_i z^*_j)}{\mid G^*_i\mid}  &amp; if\; j\in G^*_i \; and \; z^*_i\neq 0\\
             0 &amp; o.w. 
             \end{array}\right. %]]&gt;&lt;/script&gt; &lt;br /&gt;
&amp;gt; $P_{TV}(x):=\arg\min_{y\in {R}^d}\frac{1}{2}\mid\mid y-x\mid\mid^2 + \lambda\sum^{d-1}&lt;em&gt;{i=1}\mid y&lt;/em&gt;{i+1}- y_i\mid.$&lt;br /&gt;
&amp;gt;$P_{OSC}(x):=\arg\min_{y\in {R}^d}\frac{1}{2}\mid\mid y-x\mid\mid^2 + \lambda\sum_{i&amp;lt;j}\max(\mid y_i\mid,\mid y_j\mid)$&lt;br /&gt;
&amp;gt; &lt;br /&gt;
&amp;gt; $G^&lt;em&gt;_i={j\in [d]:\mid  z^&lt;/em&gt;_i\mid = \mid z^&lt;em&gt;_j\mid }.$ 其中$\mid G^&lt;/em&gt;_i\mid$ 表示 $i$ 组的元素数量。&lt;br /&gt;
&amp;gt; &lt;br /&gt;
&amp;gt; 论文附录 $A.2$ 有详细的证明过程。&lt;br /&gt;
&amp;gt; &lt;br /&gt;
&amp;gt; 论文的附录中还有大量实验结果，可以加深理解。&lt;/p&gt;

&lt;p&gt;来看一个 法语-英语翻译 Attention实验效果：&lt;br /&gt;
&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/fusedmax_oscarmax.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;## 5. TV与OSCAR的简单介绍：&lt;/p&gt;

&lt;h3 id=&quot;tvtotal-variation-&quot;&gt;5.1. TV(Total variation, 全变分)&lt;/h3&gt;

&lt;p&gt;TV(Total variation, 全变分)，也称为全变差，在图像复原中常用到。TV是在一函数其数值变化的差的总和，具体可查阅&lt;a href=&quot;https://zh.wikipedia.org/zh-cn/%E6%80%BB%E5%8F%98%E5%B7%AE&quot;&gt;wiki-Total_variation&lt;/a&gt;或&lt;a href=&quot;https://zh.wikipedia.org/zh-cn/%E6%80%BB%E5%8F%98%E5%B7%AE&quot;&gt;wiki-总变差&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;实值函数ƒ定义在区间$[a, b] \in R$的总变差是一维参数曲线$x \rightarrow ƒ(x) , x \in [a,b]$的弧长。 连续可微函数的总变差，可由如下的积分给出:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;V^a_b(f)=\int^b_a \mid f&#39;(x)\mid \mathrm{d}x&lt;/script&gt;&lt;br /&gt;
任意实值或虚值函数ƒ定义在区间[a,b]上的总变差，由下面公式定义($p$为区间$[a,b]$中的所有分划):&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;V^a_b(f)=sup_P\sum^{n_p -1}_{i=0}\mid f(x_{i+1}) - f(x_{i})\mid&lt;/script&gt;&lt;/p&gt;

&lt;/blockquote&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;TV图解&lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;当绿点遍历整个函数时，&amp;lt;p&amp;gt;绿点在y-轴上的投影红点走过的&lt;strong&gt;路程&lt;/strong&gt;&amp;lt;p&amp;gt;就是该函数的总变分TV&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Total_variation.gif&quot; alt=&quot;Total_variation&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;因为细节或假细节（如噪音）区域较多的信号则TV值较大，那假如我们得到观察信号$x_i$, 希望对$x_i$进行去噪，就可以通过引入最小化$x_i$的全变分，得到去噪且保持了图像边缘的图像。即对原复原函数引入TV正则项，如一维去噪：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\min_y \frac{1}{2}\sum_i(x_i-y_i)^2 + \lambda \sum_{i=1}^n\mid y_{i+1} - y_i \mid&lt;/script&gt;&lt;br /&gt;
更多解释可参考&lt;a href=&quot;https://www.zhihu.com/question/47162419&quot;&gt;如何理解全变分（Total Variation，TV）模型？&lt;/a&gt;和&lt;a href=&quot;https://blog.csdn.net/hanlin_tan/article/details/52448803&quot;&gt;浅谈图象的全变分和去噪&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;论文中说Fused Lasso也称为1d-TV ，公式为：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}_{fused} = {\arg\min}_{\beta}\frac{1}{2}{\mid\mid y-\beta\mid\mid}_2^2 + \lambda \cdot \mid\mid D\beta\mid\mid_1&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;在1维中 : $\hat{\beta}&lt;em&gt;{fused} = {\arg\min}&lt;/em&gt;{\beta}\frac{1}{2}{\mid\mid y-\beta\mid\mid}&lt;em&gt;2^2 + \lambda \cdot \sum&lt;/em&gt;{i=1}^{n-1}\mid\beta_i - \beta_j\mid_1 $ ， 则此时$D$ 为(若n=5)：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\left(
 \begin{matrix}
   1 &amp; -1 &amp; 0 &amp; 0 &amp; 0\\
   0 &amp; 1 &amp; -1 &amp; 0 &amp; 0\\
   0 &amp; 0 &amp; 1 &amp; -1 &amp; 0\\
   0 &amp; 0 &amp; 0 &amp; 1 &amp; -1
  \end{matrix}
  \right) %]]&gt;&lt;/script&gt;&lt;br /&gt;
将上述方法应用在1维数据得到结果如下(图来自&lt;a href=&quot;http://euler.stat.yale.edu/~tba3/stat612/lectures/lec22/lecture22.pdf&quot;&gt;The Generalized Lasso，Lecture 22&lt;/a&gt;）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Fuse_lasso_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看出，数据最终呈现连续区域的聚集，即空间聚集（spatial clutering)，也就得到了稀疏形式的数据表示。 得到更平滑的数据表示，也能防止过拟合的数据表示。&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;但是TV让连续区域聚集（即鼓励连续区域的值相等），这样的前提假设太严格，于是作者参考OSCAR惩罚函数，以来鼓励对Attention的输出值进行聚集，让同集群的Attention值相等。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;p&gt;### 5.2. OSCAR (用于回归的八边形收缩和聚类算法)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2007.00843.x&quot;&gt;Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with OSCAR&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;OSCAR(用于回归的八边形收缩和聚类算法，octagonal shrinkage and clustering algorithm for regression)可以被解释为同时实现聚类和回归. 是做稀疏和分组变量选择，类似于Elastic Net.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://people.ee.duke.edu/~lcarin/Minhua3.20.09.pdf&quot;&gt;OSCAR-PPT解释&lt;/a&gt;：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp;Penalized Regression一般形式： &amp; \hat{\beta} &amp;  = \min_{\beta}\mid\mid y- x\beta\mid\mid^2_2 \quad s.t. \quad f(\beta)\le t \\
&amp;Ridge Regression: &amp; f(\beta) &amp;=\mid\mid\beta\mid\mid^2_2=\sum^{p}_{j=1}\beta^2_j\\
&amp;LASSO：  &amp; f(\beta) &amp; =\mid\mid\beta\mid\mid_1=\sum^{p}_{j=1}\mid\beta\mid \\
&amp;Group LASSO:  &amp; f(\beta) &amp; =\sum^{G}_{g=1}\mid\mid\beta_g\mid\mid_2\\
&amp;Elastic Net(弹性网络）:  &amp; f(\beta) &amp; =\alpha\mid\mid\beta\mid\mid^2_2+(1-\alpha)\mid\mid\beta\mid\mid_1\\
&amp;OSCAR:  &amp; f(\beta) &amp; =\mid\mid\beta\mid\mid_1 + c\sum_{j &lt; k} \max\{\mid\beta\mid_j,\mid\beta\mid_k\}, L_1与pair-wise L_{\infty}组合。
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;OSCAR是做稀疏和分组变量选择，类似于Elastic Net.&lt;/li&gt;
  &lt;li&gt;与Group-LASSO相比，它不需要群组结构的先验知识。&lt;/li&gt;
  &lt;li&gt;它比Elastic Net有更强的假设，OSCAR假定：相关变量（correlated variables）的回归系数的绝对值相同。&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Elastic Net&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;OSCAR&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/oscar_elasticnet.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/oscar_oscar.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$L_1$和$L_2$范式组合&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$L_1$和$L_{\infty}$范式组合&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;看上图也就能理解“八边形收缩”这名称了. 该OSCAR基于惩罚最小二乘法，将一些系数收缩至恰好为零。同时这惩罚函数能产生值相同的系数，鼓励相关的预测因子(即指$x_i$)它们对最终结果有着相同的影响，从而形成单个系数表示预测因子群集。&lt;/p&gt;

&lt;p&gt;至于更详细的理解，就阅读下原论文吧&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2007.00843.x&quot;&gt;Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with OSCAR&lt;/a&gt;。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;## 6. 后语&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;论文中提出新的Attention机制：fusedmax和oscarmax，它们是能被反向传播训练的神经网络所使用。基于论文给出的公式计算即可完成该Attention机制的前向和反向的计算。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;文中的实验都是文本语言方面（textual entailment，machine translation, summarization）， 但如果想用于视觉方面，应该需要适当修改 fusedmax。因为fusedmax所指的连续的区域应当是二维局域图，而非一维连续点（卷积层中）。而oscarmax因为可以对不连续的元素进行Attention值上的聚集，故可以直接应用在图像方面。&lt;/p&gt;

&lt;p&gt;Attention的输出值进行稀疏，这个原因和理由容易理解（即稀疏的优点）。但为什么要对元素进行聚集且赋予相同的Attention值呢？我觉得主要的原因还是防止过拟合的数据表示，即防止Attention机制过拟合。当然，如果翻翻聚类方面的论文或许有更好的解释。&lt;/p&gt;

&lt;p&gt;文中很多公式在其他论文中已证出，想完整地看懂这篇文章，还需要好好把引用的论文理解理解。&lt;br /&gt;
&amp;gt;随便点开都是一屏幕公式推导&lt;/p&gt;

&lt;p&gt;由于我数学方面的功底不好，论文涉及的一些背后知识都是现查现学.&lt;/p&gt;

&lt;p&gt;欢迎讨论指错，轻喷就好 = =。&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Sun, 01 Jul 2018 00:00:00 +0800</pubDate>
        <link>http://luonango.github.io/2018/07/01/Fusedmax%E4%B8%8EOscarmax%E7%A8%80%E7%96%8F%E5%8F%8A%E7%BB%93%E6%9E%84%E5%8C%96%E7%9A%84Attention%E6%AD%A3%E5%88%99%E5%8C%96%E6%A1%86%E6%9E%B6/</link>
        <guid isPermaLink="true">http://luonango.github.io/2018/07/01/Fusedmax%E4%B8%8EOscarmax%E7%A8%80%E7%96%8F%E5%8F%8A%E7%BB%93%E6%9E%84%E5%8C%96%E7%9A%84Attention%E6%AD%A3%E5%88%99%E5%8C%96%E6%A1%86%E6%9E%B6/</guid>
        
        <category>iOS</category>
        
        <category>ReactiveCocoa</category>
        
        <category>函数式编程</category>
        
        <category>开源框架</category>
        
        
      </item>
    
      <item>
        <title>CapsulesNet的解析</title>
        <description>&lt;h1 id=&quot;capsulesnet&quot;&gt;CapsulesNet的解析&lt;/h1&gt;

&lt;h2 id=&quot;section&quot;&gt;前言：&lt;/h2&gt;

&lt;p&gt;本文先简单介绍传统CNN的局限性及Hinton提出的Capsule性质，再详细解析Hinton团队近期发布的基于动态路由及EM路由的CapsuleNet论文。&lt;/p&gt;

&lt;h2 id=&quot;hintoncnn&quot;&gt;Hinton对CNN的思考&lt;/h2&gt;

&lt;p&gt;Hinton认为卷积神经网络是不太正确的，它既不对应生物神经系统，也不对应认知神经科学，甚至连CNN本身的目标都是有误的。&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;在生物神经系统上不成立&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;反向传播难以成立。神经系统需要能够精准地求导数，对矩阵转置，利用链式法则，这种解剖学上从来也没有发现这样的系统存在的证据。&lt;/li&gt;
  &lt;li&gt;神经系统含有分层，但是层数不高，而CNN一般极深。生物系统传导在ms量级（GPU在us量级），比GPU慢但效果显著。&lt;/li&gt;
  &lt;li&gt;灵长类大脑皮层中大量存在皮层微柱，其内部含有上百个神经元，并且还存在内部分层。与我们使用的CNN不同，它的一层还含有复杂的内部结构。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;cnn&quot;&gt;在认知神经科学上CNN也靠不住脚&lt;/h4&gt;
&lt;p&gt;人会不自觉地根据物体形状建立“坐标框架”(coordinate frame)。并且坐标框架的不同会极大地改变人的认知。人的识别过程受到了空间概念的支配，判断物体是否一样时，我们需要通过旋转把坐标框架变得一致，才能从直觉上知道它们是否一致，但是CNN没有类似的“坐标框架”。如人类判断下图两字母是否一致：&lt;br /&gt;
&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Capsules_net_Mental_rotation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;cnn-1&quot;&gt;CNN的目标不正确&lt;/h4&gt;

&lt;p&gt;先解释下不变性（Invariance)和同变性（Equivariance）。不变性是指物体本身不发生任何变化；同变性指物体可以发生变化，但变化后仍是这种物体。如将MNIST图片中的数字进行平移（该数字形状无变化），就实现了该数字的不变性。如果数字进行了立体的旋转翻转等，但人类仍能识别出这个数字，这就是该数字的同变性。&lt;/p&gt;

&lt;p&gt;显然，我们希望的是CNN能够实现对物体的同变性（Equivariance），虽然CNN在卷积操作时实现了同变性（卷积操作是对视野内的物体进行矩阵转换，实现了视角变换），但主要由于Pooling，从而让CNN引入了不变性。这从而让CNN实现对物体的不变性而不是同变性。&lt;/p&gt;

&lt;p&gt;比如CNN对旋转没有不变性（即旋转后的图片和原图CNN认为是不一样的），我们平时是采用数据增强方式让达到类似意义上的旋转不变性（CNN记住了某图片的各种角度，但要是有个新的旋转角度，CNN仍会出问题）。当CNN对旋转没有不变性时，也就意味着舍弃了“坐标框架”。&lt;/p&gt;

&lt;p&gt;虽然以往CNN的识别准确率高且稳定，但我们最终目标不是为了准确率，而是为了得到对内容的良好表示，从而达到“理解”内容。&lt;/p&gt;

&lt;h2 id=&quot;hintoncapsules&quot;&gt;Hinton提出的Capsules&lt;/h2&gt;

&lt;p&gt;基于上述种种思考，Hinton认为物体和观察者之间的关系（比如物体的姿态），应该由一整套激活的神经元表示，而不是由单个神经元或者一组粗编码（coarse-coded）表示（即一层中有复杂的内部结构）。这样的表示，才能有效表达关于“坐标框架”的先验知识。且构成的网络必须得实现物体同变性。&lt;/p&gt;

&lt;p&gt;这一套神经元指的就是Capsule。Capsule是一个高维向量，用一组神经元而不是一个来代表一个实体。并且它还隐含着所代表的实体出现的概率。&lt;/p&gt;

&lt;p&gt;Hinton认为存在的两种同变性(Equivariance)及capsule的解决方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;位置编码（place-coded）：视觉中的内容的位置发生了较大变化，则会由不同的 Capsule 表示其内容。&lt;/li&gt;
  &lt;li&gt;速率编码（rate-coded）：视觉中的内容为位置发生了较小的变化，则会由相同的 Capsule 表示其内容，但是内容有所改变。&lt;/li&gt;
  &lt;li&gt;两者的联系是，高层的 capsule 有更广的域 (domain)，所以低层的 place-coded 信息到高层会变成 rate-coded。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;dynamic-routing-between-capsules&quot;&gt;第一篇《Dynamic Routing Between Capsules》的解析&lt;/h2&gt;

&lt;p&gt;好的，那让我们看看Hinton 团队2017年10月公布的论文：&lt;a href=&quot;https://arxiv.org/abs/1710.09829&quot;&gt;Dynamic Routing Between Capsules&lt;/a&gt;（Sara Sabour为第一作者）&lt;/p&gt;

&lt;p&gt;官方源代码已发布：&lt;a href=&quot;https://github.com/Sarasra/models/tree/master/research/capsules&quot;&gt;Tensorflow代码&lt;/a&gt; 。不得不提下，由于官方代码晚于论文，论文发布后很多研究者尝试复现其代码，获得大家好评的有&lt;a href=&quot;https://github.com/naturomics/CapsNet-Tensorflow&quot;&gt;HuadongLiao的CapsNet-Tensorflow&lt;/a&gt;、&lt;a href=&quot;https://github.com/XifengGuo/CapsNet-Keras&quot;&gt;Xifeng Guo的CapsNet-Keras&lt;/a&gt;等等。&lt;/p&gt;

&lt;p&gt;该论文中的Capsule是一组神经元（即向量），表示特定类型实体的实例化参数。而其向量的长度表示该实体存在的概率、向量的方向表示实例化的参数。同一层的 capsule 将通过变换矩阵对高层的 capsule 的实例化参数进行预测。当多个预测一致时（文中使用动态路由使预测一致），高层的 capsule 将变得活跃。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;动态路由方法&lt;/h3&gt;

&lt;p&gt;有很多方法实现capsule的总体思路，该论文只展示了动态路由方法（之后不久Hinton用EM-Routing的方法实现capsule的整体思路，相应解析在后面）。&lt;/p&gt;

&lt;p&gt;由于想让Capsule的输出向量的长度，来表示该capsule代表的实体在当前的输入中出现的概率，故需要将输出向量的长度（模长）限制在$[0,1]$。文中采用Squashing的非线性函数作为激活函数来限制模长，令当前层是$j$层，$v_j$为$capsule_j$的输出向量，$s_j$是$capsule_j$的所有输入向量。&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;v_j=\frac{\mid\mid s_j\mid \mid ^2}{1+\mid\mid s_j\mid\mid ^2}\frac{s_j}{\mid\mid s_j\mid \mid }&lt;/script&gt;&lt;br /&gt;
那$s_j$怎么来呢？别急，我们定义一些概念先：&lt;/p&gt;

&lt;p&gt;令$u_i$是前一层(第$i$层）所有$capsule_i$的输出向量，且$\hat{u}&lt;em&gt;{j|i}$是$u_i$经过权重矩阵$W&lt;/em&gt;{ij}$变换后的预测向量。&lt;br /&gt;
那除了第一层，其他层的$s_j$都是前一层所有$capsule_i$的预测向量$\hat{u}&lt;em&gt;{j|i}$的加权和（加权系数为$c&lt;/em&gt;{ij}$）。&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;s_j = \sum_i c_{ij}\hat{u}_{j|i} \quad,\qquad \hat{u}_{j|i} = W_{ij}u_i&lt;/script&gt;&lt;br /&gt;
其中的耦合系数$c_{ij}$就是在迭代动态路由过程中确定的，且每个$capsule_i$与高层所有$capsule_j$的耦合系数$c_{ij}$之和为1。它是通过‘路由softmax’得出：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;c_{ij}=\frac{exp(b_{ij})}{\sum_k exp(b_{ik})}&lt;/script&gt;&lt;br /&gt;
$b_{ij}$可理解为当前$j$层的输出向量$v_j$与前一层所有$capsule_i$的预测向量$\hat{u}_{j|i}$的相似程度，它在动态路由中的迭代公式为：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;b_{ij}=b_{ij}+\hat{u}_{j|i}v_j&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;那么，当得到$l$层的$capsule_i$所有输出向量$u_i$，求$l+1$层的输出向量的流程为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;a) 通过权重矩阵$W_{ij}$，将$u_i$变换得到预测向量$\hat{u}_{j&lt;/td&gt;
          &lt;td&gt;i}$。权重矩阵$W$是通过反向传播更新的。&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;b) 进入动态路由进行迭代（通常迭代3次就可以得到较好结果）:
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Capsules_net_6.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;c) 得到第$l+1$ 层$capsule_j$的输出向量$v_j$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以看看&lt;a href=&quot;https://www.jiqizhixin.com/articles/2017-11-05&quot;&gt;机器之心绘制的层级结构图&lt;/a&gt;来加深理解：&lt;br /&gt;
&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/CapsNet_1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;capsule&quot;&gt;Capsule网络结构&lt;/h3&gt;
&lt;p&gt;解决了动态路由的算法流程后，我们再来看下论文设计的简单的网络结构：&lt;br /&gt;
&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Capsules_net_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;第一层ReLU Conv1层的获取&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;普通的卷积层方法，使用256个9×9的卷积核、步幅为1、ReLU作为激活函数，得到256×20×20的张量输出。与前面的输入图片层之间的参数数量为:256×1×9×9+256=20992.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;第二层PrimaryCaps层的获取&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;从普通卷积层构建成Capsule结构的PrimaryCaps层，用了256×32×8个9×9的卷积核，步幅为2，32×8个bias，得到32×8×6×6的张量输出（即32×6×6个元素数为8的capsule）。&lt;/li&gt;
      &lt;li&gt;与前面层之间的参数总量为：256×32×8×9×9+32×8=5308672.（不考虑routing里面的参数）&lt;/li&gt;
      &lt;li&gt;值得注意的是，官方源码中接着对这32×6×6个capsule进行动态路由过程（里面包括了Squashing操作），得到新的输出。而论文中没提及到此处需要加上动态路由过程，导致一些研究人员复现的代码是直接将输出经过Squashing函数进行更新输出（部分复现者已在复现代码中添加了动态路由过程）。&lt;/li&gt;
      &lt;li&gt;PrimaryCaps层的具体构建可查看源码中的layers.conv_slim_capsule函数。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;第三层DigitCaps层的获取&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;从PrimaryCaps层有32&lt;em&gt;6&lt;/em&gt;6=1152个capsule，DigitCaps有10个，所以权重矩阵$W_{ij}$一共有1152×10个，且$W_{ij}=[8×16]$，即$i \in [0,8), j\in[0,16)$。另外还有10*16个偏置值。&lt;/li&gt;
      &lt;li&gt;进行动态路由更新，最终得到10*16的张量输出。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;参数的更新&lt;/strong&gt;：
    &lt;ul&gt;
      &lt;li&gt;权重矩阵 $W_{ij}、bias$通过反向传播进行更新。&lt;/li&gt;
      &lt;li&gt;动态路由中引入的参数如$c_{ij}、b_{ij}$均在动态路由迭代过程中进行更新。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-3&quot;&gt;损失函数&lt;/h3&gt;
&lt;p&gt;解决了论文设计的网络结构后，我们来看下论文采用损失函数（Max-Margin Loss形式）：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;L_c=T_c \max\big(0,m^+ - \mid\mid v_c\mid\mid\big)^2 + \lambda\big(1-T_c\big) \max\big(0,\mid\mid v_c\mid\mid - m^-\big)^2&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;其中$c$表示类别，$c$存在时，指示函数$T_c=1$,否则$T_c=0$。$m^+、m^-$分别为上边界和下边界，$\mid\mid v_c\mid\mid$为$v_c$的L2范数。论文设置$\lambda=0.5$，降低第二项的loss的系数，防止活泼的（模长较大）capsule倾向于缩小模长，从而防止网络训练差（系数大则求导的数值绝对值大，则第二项loss反馈的更新会更有力）。上下边界一般不为1或0，是为了防止分类器过度自信。&lt;/li&gt;
  &lt;li&gt;总loss值是每个类别的$L_c$的和:  $L=\sum_{c} L_c$&lt;/li&gt;
  &lt;li&gt;该损失函数与softmax区别在于：
    &lt;ul&gt;
      &lt;li&gt;softmax倾向于提高单一类别的预测概率而极力压低其他类别的预测概率，且各类别的预测概率和为1。适用于单类别场景中的预测分类。&lt;/li&gt;
      &lt;li&gt;而此损失函数，要么提高某类别的预测概率（若出现了该类 ），要么压低某类别的预测概率（若未出现该类），不同类别间的预测概率互不干扰，每个类别的预测概率均在$[0,1]$中取值。适用于多类别并存场景中的预测分类。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-4&quot;&gt;重构与表征&lt;/h3&gt;

&lt;p&gt;重构的思路很简单，利用DigitCaps中的capsule向量，重新构建出对应类别的图像。文章中使用额外的重构损失来促进 DigitCaps 层对输入数字图片进行编码：&lt;br /&gt;
![](&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Capsules_net_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;由于DigitCaps中每个capsule向量都代表着一个实体(数字)，文章采用掩盖全部非正确数字的capsule，留下代表正确数字实体的capsule来重构图片。&lt;/p&gt;

&lt;p&gt;此处的重构损失函数是采用计算在最后的 FC Sigmoid 层采用的输出像素点与原始图像像素点间的欧几里德距离。且在训练中，为了防止重构损失主导了整体损失（从而体现不出Max-Margin Loss作用），文章采用 0.0005 的比例缩小了重构损失。&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;实验结果&lt;/h3&gt;

&lt;h4 id=&quot;mnist&quot;&gt;采用MNIST数据集进行分类预测：&lt;/h4&gt;
&lt;p&gt;在此之前，一些研究者使用或不使用集成+数据增强，测试集错误率分别为0.21%和0.57%。而本文在单模型、无数据增强情况下最高达到0.25%的测试集错误率。具体如下图：&lt;br /&gt;
&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Dynamic_Routing_10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;multimnist&quot;&gt;采用MultiMNIST数据集进行重构实验：&lt;/h4&gt;

&lt;p&gt;采用混合数字图片的数据集进行重构，如下图，对一张两数字重叠的图片进行重构，重构后的数字用不同颜色显示在同一张图上。$L:(l_1,l_2)$表示两数字的真实标签，$R:(r_1,r_2)$表示预测出并重构的两数字，带$*$标识的那两列表示重构的数字既不是标签数字也不是预测出的数字（即挑取其他capsule进行重构）。&lt;br /&gt;
&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Dynamic_Routing_11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;## 第二篇《Matrix Capsules with EM routing》的解析&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://openreview.net/pdf?id=HJWLfGWRb&quot;&gt;Matrix Capsules with EM routing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;紧接着上一篇，Hinton以第一作者发布的了这篇论文，现已被ICLR接收。那这一篇与前一篇动态路由Capsule有什么不同呢？&lt;/p&gt;

&lt;p&gt;论文中提出三点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(1). 前一篇采用capsule输出向量(pose vector)的模长作为实体出现的概率，为了保证模长小于1，采用了无原则性质的非线性操作，从而让那些活泼的capsule在路由迭代中被打压。&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;(2).计算两个capsule的一致性（相似度）时，前一篇用两向量间夹角的余弦值来衡量（即指相似程度的迭代：$b_{ij}=b_{ij}+\hat{u}_{j&lt;/td&gt;
          &lt;td&gt;i} v_j$）。与混合高斯的对数方差不同，余弦不能很好区分”好的一致性”和”非常好的一致性”。&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;(3).前一篇采用的pose是长度为$n$的向量，变换矩阵$W_{ij}$具有$n_i&lt;em&gt;n_j$个参数（如$W_{ij}=[8&lt;/em&gt;16], n_i=8,n_j=16$）。 而本文采用的带$n$个元素的矩阵作为pose，这样变换矩阵$W_{ij}$具有$n$个参数（如$n=4*4)$。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从而Hinton设计了新的Capsule的结构：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;结构&lt;/th&gt;
      &lt;th&gt;图示&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;a). 4*4的pose矩阵,表示该capsule代表的实体,对应第(3)点.&lt;br /&gt;&lt;br /&gt;b). 1个激活值,表示该capsule代表实体出现的概率,对应第(1)点.&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Capsules_matrix_7.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;且上一篇采用的是动态路由的方法完成了Capsule网络，而这儿将则采用EM路由算法。这也对应着上述第（2）、（3）点。&lt;/p&gt;

&lt;h3 id=&quot;em-routing&quot;&gt;理解EM Routing前的准备&lt;/h3&gt;
&lt;p&gt;我们先定义下标符号：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$i$: 指$L$层中的某个capsule。&lt;/li&gt;
  &lt;li&gt;$c$: 指$L+1$层中的某个capsule。&lt;/li&gt;
  &lt;li&gt;$h$: 指Pose矩阵中的某维，共16维。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了更好理解EM-Routing算法的过程，我们先理理前期思路：&lt;/p&gt;

&lt;p&gt;与前一篇方法类似，$L$层的$Cpasule_i$的输出($pose$)需要经过矩阵变换$W_{ic}=[4&lt;em&gt;4]$，得到它对$L+1$层的$Capsule_c$的$pose$的投票$V_{ic}$(维度和$pose$一样都是$[4&lt;/em&gt;4]$)之后，才能进入Routing更新。而当$h$为$4*4$中的某一维时，让$V_{ich} = pose_{ih} * W_{ich}$，这样就可以得到$V_{ic}$了，这也对应着上述(3)。&lt;/p&gt;

&lt;p&gt;换种解释说：$L$层的$Capsule_i$要投票给$L+1$层的$Capsule_c$，但是不同的$Capsule_c$可能需要不同变化的$Capsule_i$。所以对于每个$Capsule_c$，$Capsule_i$都有一个转换矩阵$W_{ic}$，$Capsule_i$转换后的$V_{ic}$就称投票值，而$V_{ich}$是指在$V_{ic}$在$h$维（一共4*4=16维）上的值。且变换矩阵$W_{ic}$是反向传播更新的。&lt;/p&gt;

&lt;p&gt;文中对每个Capsule的pose建立混合高斯模型，让pose的每一维都为一个单高斯分布。即$Capsule_c$的pose中的$h$维为一个单高斯模型，$P_{ich}$是指$pose_{ch}$的值为$V_{ich}$的概率。&lt;br /&gt;
令$\mu_{ch}$和$\sigma_{ch}^2$分别为$Capsule_c$在$h$维上的均值和方差，则：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P_{ich}=\frac{1}{\sqrt{2 \pi \sigma_{ch}^2}}{exp\big({-\frac{(V_{ich}-\mu_{ch})^2}{2\sigma_{ch}^2}}}\big)&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;ln(P_{ich})=  -\frac{(V_{ich}-\mu_{ch})^2}{2\sigma_{ch}^2} - ln(\sigma_{ch})-\frac{ln(2\pi)}{2}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;再令$r_i$为$Capsule_i$的激活值，就可以写出该$Capsule_c$在$h$维度上的损失$cost_{ch}$:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;cost_{ch}=\sum_i -r_i ln(P_{ich})=\frac{\sum_i{r_i\sigma_{ch}^2}}{2\sigma_{ch}^2} + \big(ln(\sigma_{ch})+ \frac{ln(2\pi)}{2}\big)\sum_i r_i=\big(ln(\sigma_{ch}+k)\big)\sum_i r_i&lt;/script&gt;&lt;br /&gt;
值得注意的是，这里的$\sum_i$ 并不是指$L$层的所有$capsule_i$，而是指可以投票给$Capsule_c$的$Capsule_i$，这点之后会解释。 另外式子中的$k$是一个常数，它可以通过反向传播进行更新，这个在后面也会提到。&lt;/p&gt;

&lt;p&gt;$Capsule_c$的激活值可用下面公式得出：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;a_c=logistic\big(\lambda\big(b_c-\sum_h cost_{ch}\big)\big)&lt;/script&gt;&lt;br /&gt;
其中$-b_c$代表$Capsule_c$在$h$维上的代价均值，它可以通过反向传播进行更新。而$\lambda$是温度倒数，是超参，我们可以在训练过程中逐步改变它的值。文章中的logistic函数采用sigmoid函数。&lt;/p&gt;

&lt;p&gt;好的，我们现在整理下在EM Routing要用的参数：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;参数&lt;/th&gt;
      &lt;th&gt;描述&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$Capsule_i$、$Capsule_c$&lt;/td&gt;
      &lt;td&gt;分别指为$L$层、$L+1$层的某个$Capsule$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{ic}$&lt;/td&gt;
      &lt;td&gt;$Capsule_i$投票给$capsule_c$前的变换矩阵，通过反向传播进行更新&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$V_{ic}$&lt;/td&gt;
      &lt;td&gt;$Capsule_i$经过矩阵变换后准备给$capsule_c$的投票值&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$P_{ic}$&lt;/td&gt;
      &lt;td&gt;$Capsule_i$的投票值在$Capsule_c$中的混合高斯概率&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$cost_{ch}$&lt;/td&gt;
      &lt;td&gt;$Capsule_c$在$h$维度上的损失&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\mu_{ch}$、$\sigma_{ch}^2$&lt;/td&gt;
      &lt;td&gt;Capsulec在h维上的均值和方差&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$k$&lt;/td&gt;
      &lt;td&gt;$cost_{ch}$式子中的一个常数，通过反向传播进行更新&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$a_c$&lt;/td&gt;
      &lt;td&gt;$Capsule_c$的激活值&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$-b_c$&lt;/td&gt;
      &lt;td&gt;$Capsule_c$在$h$维上的代价均值，通过反向传播进行更新&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\lambda$&lt;/td&gt;
      &lt;td&gt;温度倒数，是超参，在迭代过程中逐步增大它的值&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;###　EM Routing的流程：&lt;/p&gt;

&lt;h4 id=&quot;section-6&quot;&gt;总流程：&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/EM_ROUTING_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;梳理符号：
    &lt;ul&gt;
      &lt;li&gt;$R_{ic}$表示$Capsule_i$给$Capsule_c$投票的加权系数&lt;/li&gt;
      &lt;li&gt;$M_c$、$S_c$ 表示$Capsule_c$的Pose的期望和方差&lt;/li&gt;
      &lt;li&gt;$size(L+1)$ 表示$Capsule_i$要投票给$L+1$层的Capsule的总数，即与$Capsule_i$有关系的$Capsule_c$的总数&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;传入$L$层的所有$Capsule_i$的激活值$a$和矩阵转换后的投票值$V$，输出$L+1$层所有$Capsule_c$的激活值$a^{‘}$和Pose最终的期望值。&lt;/li&gt;
  &lt;li&gt;对投票加权系数初始化后就进行EM算法迭代（一般迭代3次）：
    &lt;ul&gt;
      &lt;li&gt;对每个$Capsule_c$，M-step得到它的Pose的期望和方差&lt;/li&gt;
      &lt;li&gt;对每个$Capsule_i$，E-step中得到它更新后的对所有$Capsule_c$投票的加权系数。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;再次提醒，这里$Capsule_i$不是投票给所有而是部分$Capsule_c$。此处的”所有“是为了表述方便。具体理由之后会解释。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;m-step&quot;&gt;分析M-Step步骤:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/EM_ROUTING_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;梳理符号：
    &lt;ul&gt;
      &lt;li&gt;$cost_h$即为$cost_{ch}$，是$Capsule_c$在$h$维度上的损失.&lt;/li&gt;
      &lt;li&gt;$\beta_v$即为$cost_{ch}$式子中的一个常数k（前面提及过），通过反向传播进行更新.&lt;/li&gt;
      &lt;li&gt;$\beta_a$即为$Capsule_c$在$h$维上的代价均值$-b_c$的负数(前面提及过)&lt;/li&gt;
      &lt;li&gt;$\lambda$即为温度倒数。是超参，在迭代中将逐步增大它的值（前面提及过），通过反向传播进行更新&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;对于某$Capsule_c$，传入所有$Capsule_i$对它的投票加权系数$R_{:c}$、所有$Capsule_i$的激活值$a$、所有$Capsule_i$矩阵转换后对它的投票值$V_{:c:}$，输出该$Capsule_c$的激活值以及pose期望方差。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;e-step&quot;&gt;分析E-Step步骤:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/EM_ROUTING_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;梳理符号：
    &lt;ul&gt;
      &lt;li&gt;$p_c$即为$P_{ic}$，是$Capsule_i$的投票值在$Capsule_c$中的混合高斯概率，前面提及过。&lt;/li&gt;
      &lt;li&gt;$r$即为$r_i$， 是$Capsule_i$给所有$Capsule_c$投票的加权系数&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;对于某$Capsule_i$， 传入它对所有$Capsule_c$的投票$V_i$、所有$Capsule_c$的激活值以及pose的均值和方差，得到它更新后的对所有$Capsule_c$投票的加权系数$r_i$。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;经过一轮的公式符号轰炸，我们就明白了EM-Routing的整体流程。&lt;/p&gt;

&lt;h3 id=&quot;matrix-capsules-&quot;&gt;Matrix Capsules 网络模型&lt;/h3&gt;

&lt;p&gt;接下来我们要看下Hinton设计的Matrix Capsule的网络模型：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/EM_ROUTING_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;由于涉及到Capsule的卷积操作，此处先定义一些概念，以ConvCaps1层为例子，在ConvCaps1层中：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;含有C个channel，每个channel含有6*6=36个capsule。&lt;/li&gt;
  &lt;li&gt;不同的channel含有不同的类型capsule，同一channel的capsule的类型相同但位置不同。&lt;/li&gt;
  &lt;li&gt;任一个capsule均有6*6-1=35个相同类型的capsule，均有C-1个位置相同的capsule。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;relu-conv1&quot;&gt;第一层ReLU Conv1的获取：&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;普通的卷积层，使用A个5×5的卷积核、步幅为2、ReLU作为激活函数，得到A×14×14的张量输出。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;primarycaps&quot;&gt;第二层PrimaryCaps的获取:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;从普通卷积层构建成Capsule结构的PrimaryCaps层，用了A×B×(16+1)个1×1的卷积核，步幅为1，得到B×14×14个pose和B&lt;em&gt;14&lt;/em&gt;14个激活值，即有B&lt;em&gt;14&lt;/em&gt;14个capsule。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;convcaps1&quot;&gt;第三层ConvCaps1的获取：&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;此处将PrimaryCaps层（即$L$层）中的$Capsule_i$的pose进行矩阵转换后，得到对应的投票矩阵$V$。将$V$和全部$Capsule_i$的激活值传入RM-Routing中，即可得到C&lt;em&gt;6&lt;/em&gt;6个$Capsule_c$的pose及激活值。&lt;/li&gt;
  &lt;li&gt;此处是Capsule卷积操作，卷积核大小为K，步幅为2。
    &lt;ul&gt;
      &lt;li&gt;对于$L+1$层的某个$Capsule_c$，需要得到投票给它的那些$Capsule_i$的投票矩阵$V_{:c}$
        &lt;ul&gt;
          &lt;li&gt;在$L$层只有K×K&lt;em&gt;B个$Capsule_i$投票给该$Capsule_c$，对这K×K&lt;/em&gt;B个$Capsule_i$的pose分别进行矩阵转换，即可得到投票矩阵$V_{ic}$&lt;/li&gt;
          &lt;li&gt;这里有K&lt;em&gt;K&lt;/em&gt;B&lt;em&gt;C个转换矩阵$W_{ic}$，每个$W_{ic}$是4&lt;/em&gt;4的矩阵（或说16维的向量）。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;这两层间转换矩阵$W_{ic}$共有：(K&lt;em&gt;K&lt;/em&gt;B)&lt;em&gt;C&lt;/em&gt;6&lt;em&gt;6&lt;/em&gt;个 （而不是14&lt;em&gt;14&lt;/em&gt;B&lt;em&gt;C&lt;/em&gt;6*6).&lt;/li&gt;
      &lt;li&gt;与普通二维卷积一样，不过卷积的乘法操作改为EM-Routing。即被卷积的是$L$层的所有$Capsule_i$的投票矩阵$V_i$和激活值，卷积结果是C&lt;em&gt;6&lt;/em&gt;6个$Capsule_c$的pose及激活值。
        &lt;ul&gt;
          &lt;li&gt;每个$L+1$层的$Caspule_c$都采用capsule卷积（EM-Routing）对应$L$层的K&lt;em&gt;K&lt;/em&gt;B个$Capsule_i$，从而得到该$Caspule_c$的pose和激活值。&lt;/li&gt;
          &lt;li&gt;对于$L$层的中心位置的B个$Capsule_i$，它们每个$Capsule_i$，都只投票给卷积时卷积核滑过它们的对应的$Capsule_c$（共K&lt;em&gt;K&lt;/em&gt;C个）。而$L$层的边缘位置的每个$Capsule_i$投票给$L+1$层的$Capsule_c$个数将小于K&lt;em&gt;K&lt;/em&gt;C个。如$L$层最左上位置的B个$Capsule_i$，它们只能投给$L+1$层最左上角的C个$Capsule_c$（只有$L+1$层的这个位置执行卷积时候卷积核才滑过$L$层最左上角）&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;convcaps2&quot;&gt;第四层ConvCaps2的获取：&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;capsule卷积层，与ConvCaps1一样的操作。&lt;/li&gt;
  &lt;li&gt;采用的卷积核K=3，步幅=1 。得到D&lt;em&gt;4&lt;/em&gt;4个capsule的pose与激活值。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;class-capsules&quot;&gt;第五层Class Capsules的获取：&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;从capsule卷积层到最后一层的操作，和前面的做法不同。
    &lt;ul&gt;
      &lt;li&gt;相同类型（即同一channels）的capsule共享转换矩阵，所以两层间共有D&lt;em&gt;E个转换矩阵$W_j$，每个$W_j$是4&lt;/em&gt;4的矩阵（或说16维的向量）。&lt;/li&gt;
      &lt;li&gt;拉伸后的扁长层（Class Capsules层）无法表达capsule的位置信息，而前面ConvCaps2层的每个$capsule_i$都有对应的位置信息。为了防止位置信息的丢失，作者将每个$Capsule_i$的位置信息（即坐标）分别加到它们的投票矩阵$V_ij$的一二维上。随着训练学习，共享转换矩阵$W_j$能将$V_ij$的一二维与$Capsule_i$的位置信息联系起来，从而让Class Capsules层的$Capsule_j$的pose的一二维携带位置信息。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;得到E个capsule， 每个capsule的pose表示对应calss实体，而激活值表示该实体存在的概率。&lt;/li&gt;
  &lt;li&gt;这样就可以单独拿出capsule的激活值做概率预测，拿capsule的pose做类别实体重构了。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;整体的Matrix Capsules网络模型就梳理完成了。现在还剩下损失函数了。&lt;/p&gt;

&lt;h3 id=&quot;spread-loss&quot;&gt;损失函数：传播损失(Spread Loss):&lt;/h3&gt;

&lt;p&gt;为了使训练过程对模型的初始化以及超参的设定没那么敏感，文中采用传播损失函数来最大化被激活的目标类与被激活的非目标类之间的概率差距。a_t表示target的激活值，a_i表示Class_Capsules中除t外第i个的激活值：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;L_i=\big(\max \big(0,m-(a_t-a_i)\big)\big)^2 , \quad  L=\sum_{i\neq t} L_i&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;m将从0.2的小幅度开始，在训练期间将其线性增加到0.9，避免无用胶囊的存在。那为什么要这样做呢？ &lt;br /&gt;
小编的理解是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当模型训练得挺好时候（中后期），每个激活值a_i的比较小而a_t比较大。此时m需要设置为接近1的数值。设置m为0.9而不是1，这是为了防止分类器过度自信，是要让分类器更专注整体的分类误差。&lt;/li&gt;
  &lt;li&gt;当模型初步训练时候，很多capsules起的作用不大，最终的激活值a_i和a_t相差不大，若此时m采用较大值如0.9,就会掩盖了(a_t-a_i)在参数更新的作用，而让m主导了参数更新。
    &lt;ul&gt;
      &lt;li&gt;比如两份参数W1和W2对同样的样本得到的$a_t-a_i$值有：$W1_{a_t-a_i} &amp;lt; W2_{a_t-a_i}$ ，那显然W2参数优于W1参数，即W1参数应该得到较大幅度的更新。但由于处于模型初步阶段，$W_{a_t-a_i}$值很小，若此时m较大，则m值主导了整体loss。换句话说，m太大会导致W1和W2参数更新的幅度相近，因为$a_t-a_i$被忽略了。&lt;/li&gt;
      &lt;li&gt;不过参数的更新幅度取决于对应的导数，由于此处的spread loss含有平方，所以m值的设置会关系到参数的导数，从而影响到参数更新的幅度 （有些loss由于公式设计问题会导致从loss看不出参数更新的幅度，如若此处将Spread loss的平方去掉，参数的更新就和m无关了）。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-7&quot;&gt;实验结果&lt;/h3&gt;

&lt;p&gt;作者采用smallNORB数据集（具有5类玩具的灰度立体图片：飞机、汽车、卡车、人类和动物）上进行了实验。选择smallNORB作为capsule网络的基准，因为它经过精心设计，是一种纯粹的形状识别任务，不受上下文和颜色的混淆，但比MNIST更接近自然图像。下图为在不同视角上的smallNORB物体图像：&lt;br /&gt;
&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/EM_ROUTING_6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;该smallNORB数据集，CNN中的基准线是：测试集错误率5.2%，参数总量4.2M。而作者使用小型的capsule网络（A=64, B = 8, C = D = 16，参数总量68K），达到2.2%的测试集错误率，这也击败了当前最好的记录。&lt;/p&gt;

&lt;p&gt;作者还实验了其他数据集。采用上图的Matrix Capsules网络及参数，在MNIST上就达到了0.44％的测试集错误率。如果让A=256，那在Cifar10上的测试集错误率将达到11.9％.&lt;/p&gt;

&lt;p&gt;文章后面还讨论了在对抗样本上，capsule模型和传统卷积模型的性能。实验发现，在白箱对抗攻击时，capsule模型比传统的卷积模型更能抵御攻击。而在黑箱对抗攻击时，两种模型差别不大。感兴趣的话可以看看论文中对这部分实验的设置及分析。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-8&quot;&gt;参考链接：&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.09829&quot;&gt;Dynamic Routing Between Capsules&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=HJWLfGWRb&quot;&gt;Matrix Capsules with EM routing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/29435406&quot;&gt;浅析 Hinton 最近提出的 Capsule 计划&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kndrck.co/posts/capsule_networks_explained/&quot;&gt;Capsule Networks Explained&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jiqizhixin.com/articles/2017-11-05&quot;&gt;先读懂CapsNet架构然后用TensorFlow实现：全面解析Hinton的提出的Capsule&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jiqizhixin.com/articles/capsule-implement-sara-sabour-Feb02&quot;&gt;Capsule官方代码开源之后，机器之心做了份核心代码解读&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/30970675&quot;&gt;CapsulesNet 的解析及整理&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jhui.github.io/2017/11/14/Matrix-Capsules-with-EM-routing-Capsule-Network/&quot;&gt;Understanding Matrix capsules with EM Routing (Based on Hinton’s Capsule Networks)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Sarasra/models/tree/master/research/capsules&quot;&gt;Dynamic Routing官方源代码-Tensorflow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/naturomics/CapsNet-Tensorflow&quot;&gt;Dynamic Routing:HuadongLiao的CapsNet-Tensorflow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/XifengGuo/CapsNet-Keras&quot;&gt;Dynamic Routing:Xifeng Guo的CapsNet-Keras&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Thu, 01 Feb 2018 00:00:00 +0800</pubDate>
        <link>http://luonango.github.io/2018/02/01/CapsulesNet/</link>
        <guid isPermaLink="true">http://luonango.github.io/2018/02/01/CapsulesNet/</guid>
        
        <category>iOS</category>
        
        <category>ReactiveCocoa</category>
        
        <category>函数式编程</category>
        
        <category>开源框架</category>
        
        
      </item>
    
  </channel>
</rss>
