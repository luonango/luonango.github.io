<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>BY Blog</title>
    <description>Every failure is leading towards success.</description>
    <link>http://luonango.github.io/</link>
    <atom:link href="http://luonango.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 06 Oct 2018 14:03:04 +0800</pubDate>
    <lastBuildDate>Sat, 06 Oct 2018 14:03:04 +0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>RA-CNN 多尺度循环Attention</title>
        <description>&lt;h1 id=&quot;ra-cnnattention&quot;&gt;RA-CNN_多尺度循环Attention_区域检测与特征提取协同合作_端到端训练弱监督细粒度图像识别&lt;/h1&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2017_CVPR,  Jianlong Fu &amp;amp; Heliang Zheng &amp;amp; Tao Mei
微软亚洲研究院
多尺度循环Attention网络，让区域检测与特征提取相互协作提升。
提出APN注意力建议子网络（attention proposal sub-network)
循环精调出Attention区域位置（可以并入反向传播，端到端训练）
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;论文:《&lt;a href=&quot;http://202.38.196.91/cache/9/03/openaccess.thecvf.com/c9a21b1be647ad791694df3a13879276/Fu_Look_Closer_to_CVPR_2017_paper.pdf&quot;&gt;Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-grained Image Recognition&lt;/a&gt;》&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;section&quot;&gt;前言&lt;/h1&gt;

&lt;p&gt;细粒度物体识别的难点在于 &lt;strong&gt;判别区域定位(discriminative region localization) 和 基于区域的特征表达(fine-grained feature learning)&lt;/strong&gt; 。&lt;/p&gt;

&lt;p&gt;一般part-based的做法分为两步走：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1 通过无监督或强监督(指利用box/part标注信息)的方法，分析卷积层的响应，再定位出object区域。&lt;/li&gt;
  &lt;li&gt;2 提取所有判别区域的特征进行encode，再拿去做识别。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但是这样有些弊端：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1 人类标注的box/part 信息 或无监督得到的part区域，未必就是最最最适合机器用于识别的信息（非最优解）。&lt;/li&gt;
  &lt;li&gt;2 对于相似类别的一些区域，细微的视觉差异仍然难以学习并区分。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;并且作者发现区域检测和细粒度特征学习是相互关联的，因此它们可以相互加强。&lt;/p&gt;

&lt;p&gt;为了解决上述难点及弊端，作者提出无需box/part 标注信息的RA-CNN（recurrent attention CNN)，递归地学习判别区域Attention和基于区域的特征表示，并让它们协同强化。&lt;/p&gt;

&lt;p&gt;#　论文做法：&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;网络设计：&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/RA-CNN_architecture_pig1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;多尺度网络：
    &lt;ul&gt;
      &lt;li&gt;共享相同的网络结构，但每个尺度的网络有各自的参数，从而适应不同分辨率（不同尺度）的图。&lt;/li&gt;
      &lt;li&gt;如图有三个尺度的网络（scale1-3), b1-3表示卷积网络。&lt;/li&gt;
      &lt;li&gt;文中采用VGG19或VGG16作为网络结构。均记载在Imagenet上预训练好的参数。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;分类子网络：
    &lt;ul&gt;
      &lt;li&gt;全连接层 + softmax层，c1-c3即为三个分类子网络。&lt;/li&gt;
      &lt;li&gt;和平时的最后分类结构一样。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;APN（Attention proposal sub-network):
    &lt;ul&gt;
      &lt;li&gt;是FC结构，传入网络的输出，输出三个数值 $(t_x,t_y,t_l)$ . 其中$t_x,t_y$表示 区域框的中心点，$t_l$ 表示框边长的一半（为正方形框）。&lt;/li&gt;
      &lt;li&gt;与其他的方法不同，此处是通过学习得到框的位置及大小。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Crop and Zoom in 操作：
    &lt;ul&gt;
      &lt;li&gt;结合APN的输出与前一尺度的图，得到区域框的图后再进行放大。&lt;/li&gt;
      &lt;li&gt;目的为了提取更细粒度的特征&lt;/li&gt;
      &lt;li&gt;放大到与scal1的尺度相同，这样网络的输入尺寸也就对应上了。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Loss 问题留在后面再讲。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;乍这么一看，似乎没什么问题，甚至会有“我们也能想到这方法阿”这样的冲动。实际上隐藏了个大boss：“得到APN的输出后，如何在裁剪放大图片的同时，还能保证反向传播顺利完成？”&lt;/p&gt;

&lt;p&gt;(思考 ing….　&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;(好的，我放弃了，看作者怎么变魔术吧…&lt;/p&gt;

&lt;p&gt;论文用Attention mask来近似裁剪操作。 一提到mask 就应该想到mask乘上原图能得到区域图。对的，论文就是定义一个连续的mask函数$M(\cdot)$ ，让反向传播顺利完成。 这个函数$M(\cdot)$ 是二维boxcar函数的变体。&lt;/p&gt;

&lt;p&gt;那么裁剪后的图片结果就是($X$为前一尺度的图,$\odot$表示逐元素乘)：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;x^{att}=X \odot M(t_x,t_y,t_l)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;令图的左上角(top-right,tl)点为$t_l$ ,右下角(bottom-right,br)。那么APN输出框的左上角$(t_{x(tl)},t_{y(tl)})$和右下角$(t_{x(br)},t_{y(br)})$ 为：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;t_{x(tl)}=t_x - t_l,\quad t_{y(tl)}=t_y - t_l \\
t_{x(br)}=t_x + t_l,\quad t_{y(br)}=t_y + t_l&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;则连续的mask函数$M(\cdot)$ 为：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;M(\cdot) = \big[h\big(x-t_{x(tl)}\big) - h\big(x-t_{x(br)}\big) \big]\cdot \big[h\big(y-t_{y(tl)}\big) - h\big(y-t_{y(br)}\big) \big]&lt;/script&gt;&lt;br /&gt;
其中里面的$h(\dot)$是logistic 函数($k=1$时即为sigmoid函数):&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;h(x)=\frac{1}{1+ \exp^{-kx}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;当$k$足够大时，这个逻辑函数就可认为是step function(阶梯函数)。换句话说，如果点$(x,y)$ 在框内，则 M 对应的值近似为1，否则近似为0。 这样的性质跟二维boxcar函数一样，可以很好地近似裁剪操作。文中让$k=10$.&lt;/p&gt;

&lt;p&gt;接下来是自适应缩放到大尺寸的做法了。论文采用双线性插值来计算目标图$X^{amp}$的点（利用$X^{att}$里对应的四个邻近点）。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;loss&quot;&gt;Loss函数&lt;/h3&gt;

&lt;p&gt;作者为了交替产生准确的区域Attention，切学习出更精细的特征，定义了两种损失函数：$L_{cls}$（intra-scale classification loss，尺度内分类损失）和$L_{rank}$(inter-scale pairwise ranking loss​,尺度间排序损失):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(X)=\sum^3_{s=1}\big\{L_{cls}\big(Y^{(s)},Y^*\big)\big\} + \sum^2_{s=1}\big\{L_{rank}\big(p^{(s)}_t, p^{(s+1)}_t\big)\big\}&lt;/script&gt;

&lt;p&gt;其中$p^{(s)}&lt;em&gt;t$表示s尺度网络的softmax预测向量中对应正确标签$t$的概率值（上面的总架构图有标出）。且$L&lt;/em&gt;{rank}$定义为： &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;L_{rank}\big(p^{(s)}_t, p^{(s+1)}_t\big) = \max \big\{0,p^{(s)}_t-p^{(s+1)}_t + margin\big\}&lt;/script&gt;&lt;br /&gt;
&amp;gt;强迫$p^{(s+1)}_t &amp;gt; p^{(s)}_t + margin$, 让细粒度尺寸的网络以粗尺度网络的预测作为参考，强制细尺度的网络逐步定位出最具判别力的区域，并产生更高的预测值。论文中让margin=0.05。&lt;/p&gt;

&lt;p&gt;最后还有个多尺度特征融合，即让多个尺度网络的最终输出（softmax前一层）进行concat，再并入FC+softmax 得到最终预测值。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-2&quot;&gt;训练步骤(交替训练)：&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;1 用在Imagenet上预训练好的VGG网络参数来初始化分类子网络中的卷积层与全连接层的参数&lt;/li&gt;
  &lt;li&gt;2 初始化APN参数，用分类子网络最后一层卷积层中具有高响应值的区域来初始化$(t_x,t_y,t_l)$&lt;/li&gt;
  &lt;li&gt;3 固定APN的参数，训练分类子网络直到$L_{cls}$收敛&lt;/li&gt;
  &lt;li&gt;4 固定分类子网络的参数，训练APN网络直到$L{rak}$收敛&lt;/li&gt;
  &lt;li&gt;5 交替循环步骤3、4，直到两个网络损失都收敛。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;此外，论文还仔细分析了关于Attention Learning问题，即为什么Attention mask函数能正确更新 $(t_x,t_y,t_l)$的问题。并且论文中还通过导数图(derivative)分析了不同情况下 $(t_x,t_y,t_l)$它们各自的更新方向。&lt;br /&gt;
&amp;gt;导数图:  计算导数范数的负平方(negative square of the norm of the derivatives), 获得与人类感知一致的优化方向. 参考自&lt;a href=&quot;https://arxiv.org/abs/1511.06789&quot;&gt;2015_ECCV_The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-3&quot;&gt;小小总结&lt;/h1&gt;

&lt;p&gt;论文提出的RA-CNN，无需box/part 标注信息就能挺好地学习判别区域Attention和基于区域的特征表示，最后得出来的Attention 区域如APN1输出的裁剪框和人类标注的物体框特别接近，同时最终分类结果甚至优于其他基于强监督信息的方法。&lt;/p&gt;

&lt;p&gt;让我惊艳的是论文采用Attention mask来近似裁剪操作，从而让反向传播顺利进行。并且$L_{rank}$(inter-scale pairwise ranking loss​,尺度间排序损失)的设计也很巧妙合理。这些解决问题思路方法都值得好好思考思考。&lt;/p&gt;

&lt;p&gt;同时论文提到的导数图（导数范数的负平方与人类感知的优化方向一致），这个我还是第一次听说，有时间得好好理解学习下。&lt;/p&gt;
</description>
        <pubDate>Fri, 28 Sep 2018 00:00:00 +0800</pubDate>
        <link>http://luonango.github.io/2018/09/28/RA-CNN_%E5%A4%9A%E5%B0%BA%E5%BA%A6%E5%BE%AA%E7%8E%AFAttention_%E5%8C%BA%E5%9F%9F%E6%A3%80%E6%B5%8B%E4%B8%8E%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%8D%8F%E5%90%8C%E5%90%88%E4%BD%9C_%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83%E5%BC%B1%E7%9B%91%E7%9D%A3%E7%BB%86%E7%B2%92%E5%BA%A6%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/</link>
        <guid isPermaLink="true">http://luonango.github.io/2018/09/28/RA-CNN_%E5%A4%9A%E5%B0%BA%E5%BA%A6%E5%BE%AA%E7%8E%AFAttention_%E5%8C%BA%E5%9F%9F%E6%A3%80%E6%B5%8B%E4%B8%8E%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%8D%8F%E5%90%8C%E5%90%88%E4%BD%9C_%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83%E5%BC%B1%E7%9B%91%E7%9D%A3%E7%BB%86%E7%B2%92%E5%BA%A6%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/</guid>
        
        <category>细粒度图像识别</category>
        
        <category>Attention</category>
        
        <category>区域检测</category>
        
        <category>Attention mask</category>
        
        
      </item>
    
      <item>
        <title>用SE结构的Multi-Attention Multi-Class约束</title>
        <description>&lt;h1 id=&quot;semulti-attentionnpairloss&quot;&gt;用SE结构的Multi-Attention在同类或不同类上的进行约束_采用NpairLoss弱监督细粒度图像识别&lt;/h1&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2018_ECCV,  Ming Sun &amp;amp; Baidu
用多个SE结构获得部位的Attention，再用N-pair Loss 对这些Attention进行约束。
使得不同SE结构生成不同的部位Attention，完成弱监督细粒度图像识别。
还提供了 Dogs-in-the-Wild 数据集。
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;论文:《&lt;a href=&quot;https://arxiv.org/pdf/1806.05372v1.pdf&quot;&gt;Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition&lt;/a&gt;》&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;section&quot;&gt;引言&lt;/h1&gt;

&lt;p&gt;现有的基于Attention的细粒度图像识别方法大多都没有考虑 Object part的相关性。而且以往大多方法都用multi-stage 或者 multi-scale机制，导致效率不高切难以end-to-end训练。&lt;/p&gt;

&lt;p&gt;此论文提出能调节不同输入图像的不同部位(object-part)的关系:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;基于SE设计One-squeeze multi-excitation(OSME)方法去学习每张图的每个关注区域特征。&lt;/li&gt;
  &lt;li&gt;再用Multi-attention multi-class constraint(MAMC)方法让同类别图像具有类似的Attention，而不同类别的图像具有不一样的Attention。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此方法效率高且容易end-to-end训练。&lt;/p&gt;

&lt;p&gt;此外，论文还提供Dogs-in-the-Wild的综合狗种数据集。它按类别覆盖范围，数据量和注释质量超过了类似的现有数据集。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;思路&lt;/h1&gt;

&lt;p&gt;强监督信息的细粒度图像分类的方法依赖于object part 标注，但标注这些信息开销很大，故弱监督信息的方法得到重视。但目前基于弱监督信息的细粒度图像分类的方法或多或少有着下面几个缺点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;额外步骤. 如增加额外的regions才能进行part localization 和 feature extraction，这导致计算量增大。&lt;/li&gt;
  &lt;li&gt;训练步骤复杂. 因为构建的结构复杂，需要多次交替或级联训练。&lt;/li&gt;
  &lt;li&gt;单独检测object part，而忽略它们的相关性 (这是最重要的)， 学习到的注意力模块可能只集中在相同的区域。假如好好利用“具有区分性的特征”，可能可以让注意力模块学习出多个不同的部位。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;文中提出细粒度图像分类的三个准则：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;检测的部位应该很好地散布在物体身上，并提取出多种不相关的features&lt;/li&gt;
  &lt;li&gt;提取出的每个part feature都应该能够区分出一些不同的类别&lt;/li&gt;
  &lt;li&gt;part extractors（部位检测器）应该能轻便使用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后论文就提出了满足这些准则的弱监督信息分类方案：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Attention用什么？ 用SENet的方案，设计one-squeeze multi-excitation module (OSME)来定位不同的部分。和现有的方法不同，现有的方法大多是裁剪出部位，再用额外的网络结构来前馈该特征。SEnet的输出可以看做soft-mask的作用。&lt;/li&gt;
  &lt;li&gt;如何让Attention学习出多个不同的部位？ 受到度量学习损失的启发（如TribleLoss），提出multi-attention multi-class constraint (MAMC) ，鼓励”same-calss和same-attention”的特征 和 “same-calss和different-attention”的特征 距离更接近。&lt;/li&gt;
  &lt;li&gt;现有的大多方法需要多个前馈过程或者多个交替训练步骤，而论文的方法是端到端的训练。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-2&quot;&gt;相关工作&lt;/h1&gt;

&lt;p&gt;在细粒度图像识别的任务中，由于类间差异是微妙的，需要特征的区分学习和对象部位定位。 一种直接的方法是手动在object parts上进行标注，并进行监督学习（强监督信息）。但是获得object part的标注开销巨大，基于弱监督信息的识别机制也就相应被提出。 如STN（空间变换网络）让人脸进行对齐（end2end方式）、如双线性模型( bilinear models)让特征提取和object part定位的两个模型协调合作。 但是论文方法不需要裁剪就可以直接提取object part的特征，切可以高效地拓展到多个object parts。&lt;/p&gt;

&lt;p&gt;深度量度学习是学习测量样本对之间的相似性。经典度量学习可认为是学习pairs of points间的 Mahalanobis distance。：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Siamese网络制定的深度度量学习是：最小化正对之间的距离，同时保持负对分开。&lt;/li&gt;
  &lt;li&gt;Triplet Loss 通过优化三个样本中的正对和一个负对的相对距离。虽然在细粒度产品搜索时很有效，但triplet loss 只考虑一个negative sample, 导致训练速度慢，并且易导致局部最小值。&lt;/li&gt;
  &lt;li&gt;N-pair Loss 考虑训练中的多个负样本，使用N元组，缓解了Triple Loss问题。&lt;/li&gt;
  &lt;li&gt;Angular Loss 增强了N-pair Loss。考虑角度关系作为相似性度量增加了三阶几何限制，捕获了triplet triangles的附加局部结构，收敛更好。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;section-3&quot;&gt;提出的方案&lt;/h1&gt;

&lt;p&gt;如下图，每个SE结构用于提取不同object-part的soft-mask（即注意的区域），即OSME模块是对每张图片提取P个特征向量（P个object-part的特征）。&lt;/p&gt;

&lt;p&gt;传入多张同类或不同类别的图片，最后得到的Attention传入MAMC中进行学习，让同类的Attention尽量相近，不同类的Attention尽量不一样。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/MAMC_overview_pig1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如何将提取的Attention特征引导到正确的类标签？ 直接做法是softmax损失对这soft-mask（连续注意力）进行评估。但这方法无法调节Attention features 之间的相关性。 另一类方法是迭代地从粗略生成精细的Attention区域，但由于当前预测依赖于前一次预测，在训练过程中，初始误差会被迭代放大，需要强化学习或者仔细初始化才能较好解决这问题。&lt;/p&gt;

&lt;p&gt;而论文提出 multi-attention multi-class constraint (MAMC) 来探索更丰富的object-parts的相关性。&lt;/p&gt;

&lt;p&gt;假设现有训练图像${(x,y),…}$ 共 $K$ 张, 和N-pair Loss论文的做法一样，采样出 $N$ 对样本:$B={(x_i,x^+&lt;em&gt;i,y_i),…}$,其中$x_i$和$x^+_i$都属于$y_i$类，集合 $B$ 中共有 $N$ 类图片。 对于每对样本 $(x_i,x^+_i)$, 用OSME提取出的P个特征向量为: ${f^p_i,f^{p+}_i}^P&lt;/em&gt;{p=1}$&lt;/p&gt;

&lt;p&gt;对于$y_i$类在第 $p$ 个Attention结构得到的特征向量$f^p_i$ 来说，其他的features可以分为四类：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;same-attention same-class features: $S_{sasc}(f^p_i)={f^{p+}_i}$&lt;/li&gt;
  &lt;li&gt;same-attention different-class features: $S_{sadc}(f^p_i)={f^p_j, f^{p+}&lt;em&gt;j}&lt;/em&gt;{j \neq i}$&lt;/li&gt;
  &lt;li&gt;different-attention same-class features: $S_{dasc}(f^p_i)={f^q_i, f^{q+}&lt;em&gt;i}&lt;/em&gt;{p \neq q}$&lt;/li&gt;
  &lt;li&gt;different-attention different-class features: $S_{dadc}(f^p_i)={f^q_j, f^{q+}&lt;em&gt;j}&lt;/em&gt;{j\neq i , p \neq q}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;那么下面给出那些是需要和$f^p_i$相近的，那些是需要远离$f^p_i$的集合定义，即对于$f^p_i$来说，正负集合的定义：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Same-attention same-class:
    &lt;ul&gt;
      &lt;li&gt;$P_{sasc}=S_{sasc},\quad N_{sasc}=S_{sadc} \bigcup S_{dasc}\bigcup S_{dadc}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Same-attention different-class:
    &lt;ul&gt;
      &lt;li&gt;$P_{sadc}=S_{sadc},\quad N_{sadc}=S_{dadc}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Different-attention same-class:
    &lt;ul&gt;
      &lt;li&gt;$P_{dasc}=S_{dasc},\quad N_{dasc}=S_{dadc}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以对于任何正例集合$P\in {P_{sasc},P_{sadc},P_{dasc}}$ 和负例集合 $N\in {N_{sasc},N_{sadc},N_{dasc}}$ , 我们希望当前的部位特征 $f^p_i$ 和正例集合距离越近，而和负例集合距离越远. 令 $m &amp;gt; 0$ 为distance margin，则：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\mid\mid f^p_i - f^+\mid\mid^2 + m \leq \mid\mid f^p_i - f^-\mid\mid,\quad \forall f^+ \in P,\quad f^- \in N&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;那就可以设计 hinge loss为:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\begin{equation*}
\big[\mid\mid f^p_i - f^+\mid\mid^2 -  \mid\mid f^p_i - f^-\mid\mid + m \big]_+
\end{equation*}&lt;/script&gt;&lt;br /&gt;
上述式子虽然广泛采用standard triplet sampling 来优化，但实际中会出现收敛慢、性能不稳定的问题。论文采用2016年出来的N-pair Loss 来优化上式：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\begin{equation*}
L^{np} = \frac{1}{N} \sum_{f^p_i \in B}\big\lbrace \sum_{f^+\in P} \log \big(1+\sum_{f^-\in N} exp(f^{pT}_i f^- - f^{pT}_if^+) \big)  \big\rbrace
\end{equation*}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;那么最终的loss定义为:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\begin{equation*}
L^{mamc}=L^{softmax} + \lambda \big(L^{np}_{sasc} + L^{np}_{sadc}+ L^{np}_{dasc} \big)
\end{equation*}&lt;/script&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;个人小总结&lt;/h1&gt;

&lt;p&gt;该方法在 CUB-200-2011、Stanford Dogs、Stanford Cars 以及论文贡献的Dogs-in-the-Wild数据集。 文中介绍了这些数据集，并且列举了许多其他方法在这些数据集上的结果以及细节（如是否采用部件级标注信息、是否能一次性训练完），这些表格数据信息实在是感人。&lt;/p&gt;

&lt;p&gt;文中从SENet得到启发，让SE的输出为部位的Attention，期望多个SE结构来分别学习不同部位的Attention。 为了达成这个想法，作者先对Attention出来的特征进行划分正负样例集合，利用了度量学习的N-pair Loss，引导不同的SE结构学习不同的部位。&lt;/p&gt;

&lt;p&gt;文中方法是基于弱监督信息的图像识别，文章大体流程清晰明朗。虽未找到论文代码，但复现起来难度应该不会特别大（需要注意训练过程中的N-pair Loss，以及正负例的归属）。&lt;/p&gt;

&lt;p&gt;关于N-pair Loss的解释及公式的理解，请移步至下面链接：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf&quot;&gt;N-pair Loss论文：2016_nips_Improved Deep Metric Learning with Multi-class N-pair Loss Objective&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/45014864&quot;&gt;理解公式的参考链接：从最优化的角度看待Softmax损失函数&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/LogSumExp&quot;&gt;理解公式的参看链接：LogSumExp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 28 Sep 2018 00:00:00 +0800</pubDate>
        <link>http://luonango.github.io/2018/09/28/Multi-Attention-Multi-Class-%E7%BA%A6%E6%9D%9F%E7%9A%84%E5%BC%B1%E7%9B%91%E7%9D%A3%E7%BB%86%E7%B2%92%E5%BA%A6%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/</link>
        <guid isPermaLink="true">http://luonango.github.io/2018/09/28/Multi-Attention-Multi-Class-%E7%BA%A6%E6%9D%9F%E7%9A%84%E5%BC%B1%E7%9B%91%E7%9D%A3%E7%BB%86%E7%B2%92%E5%BA%A6%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/</guid>
        
        <category>细粒度图像识别</category>
        
        <category>Attention</category>
        
        <category>SE结构</category>
        
        <category>弱监督信息</category>
        
        
      </item>
    
      <item>
        <title>ShufflenetV2_高效网络的设计指南</title>
        <description>&lt;h1 id=&quot;shufflenetv2&quot;&gt;ShufflenetV2_高效网络的设计指南&lt;/h1&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2018年 清华大学&amp;amp;Face++ Ningning Ma 
提出指导高效CNN网络设计的4种方案，并改进了shufflenetv1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;section&quot;&gt;背景介绍&lt;/h2&gt;

&lt;p&gt;在高效网络上（特别是移动端），网络必须在有限的计算能力中达到最优的精度。 目前很多论文研究着 &lt;strong&gt;轻量级架构设计和速度-精度的权衡&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;比如Xception、MobileNet、MobileNetv2、ShuffleNet、CondenseNet等等。在这些有效的网络结构中，&lt;strong&gt;Group Convolution(组卷积) 和 Depth-wise convolution（深度卷积）&lt;/strong&gt; 起到关键性作用。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;组卷积 Group Convolution：
      &lt;ul&gt;
        &lt;li&gt;将feature map拆分成几组，分别进行卷积，最后再将卷积结果的feature map 合并。&lt;/li&gt;
        &lt;li&gt;最早在AlexNet中出现。因为当时GPU内存有限，逼迫将feature map拆分到两个GPU上分别卷积再融合。&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;深度卷积 Depth-wise convolution：
      &lt;ul&gt;
        &lt;li&gt;对每一个通道进行各自的卷积，输出相同的通道数，再用进行跨通道的标准$1*1$ 卷积来融合信息&lt;/li&gt;
        &lt;li&gt;Xception 就是典型的代表&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;极大减少了参数，性能还有点提高（未知有无理论证明能完全替代普通卷积）。Alex认为组卷积实现类似正则的作用。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;速度(Speed)是直接指标，但不同设备不好比较， 故以往常用 FLOPs（乘或加的次数）来度量复杂度。但FLOP是一种间接指标，它不能直接作为评判的标准（如Mobilev2和Nasnet相比，他们的Flops相似，但是前者快很多）。&lt;br /&gt;
&amp;gt; 个人体会： &lt;br /&gt;
&amp;gt; &lt;br /&gt;
&amp;gt; - 在WRN和Resnet上，WRN的Flops和参数量远大于Resnet情况下，WRN比Resnet快很多。且ResNext比WRN慢很多。&lt;br /&gt;
&amp;gt; - shufflenetv2论文中在两种硬件环境中测试四种不同速度和FLOPs的网络结构。观察知道FLOPs不能替代Speed这评判指标。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;间接指标(Flops)和直接指标（速度）之间存在差异有两种：
    &lt;ul&gt;
      &lt;li&gt;对速度影响较大的因素，但没有影响到FLOPs。
        &lt;ul&gt;
          &lt;li&gt;如&lt;strong&gt;内存访问成本(MAC, memory access cost)&lt;/strong&gt;,它在如组卷积中占大量运行时间，导致MAC是瓶颈。&lt;/li&gt;
          &lt;li&gt;又如&lt;strong&gt;并行度(degree of parallelism)&lt;/strong&gt;，FLOPs相同情况下，高并行的模型可能会快很多。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;运行平台不同。
        &lt;ul&gt;
          &lt;li&gt;相同的FLOPs的运算在不同的平台上运行也会导致速度差异。&lt;/li&gt;
          &lt;li&gt;如以前会采用张量分解来加速矩阵乘法，但张量分解在GPU上运行会慢很多。作者调查发现最近的CUDNN库中有专门为$3*3$ 卷积进行了优化，也就是说张量分解后的运行速度有可能慢与优化库中的张量乘法。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;于是作者认为高效网络架构的设计应该考虑两个基本原则：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;用直接指标（如速度）评估，而不是间接指标（如Flops)。&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;在同一环境平台上进行评估。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;论文也是按照这两个原则，对多种网络（包括shufflenetv2)进行评估。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;高效网络设计的实用准则&lt;/h2&gt;

&lt;p&gt;作者发现 组卷积和深度卷积是目前较优网络的关键，且FLOPs数仅和卷积有关。虽然卷积操作耗费大量时间，但是其他内部过程如 I/O、data shuffle、element-wise操作(Add、ReLU等)都会耗费一定的时间。&lt;/p&gt;

&lt;p&gt;作者提出设计高效网络的4条准则：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;G1 相同通道宽度可以最小化内存访问成本MAC。&lt;/li&gt;
  &lt;li&gt;G2 过多的组卷积会增加内存访问成本MAC&lt;/li&gt;
  &lt;li&gt;G3 网络内部碎片操作会降低并行度&lt;/li&gt;
  &lt;li&gt;G4 Element-wise操作不容忽视&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;g1-mac&quot;&gt;G1 相同通道宽度可以最小化内存访问成本MAC&lt;/h4&gt;

&lt;p&gt;考虑无bias的$1*1$卷积。如果输入的通道数为$c_1$, 卷积后输出通道数为$c_2$. 让 $h,w$ 分别表示feature map的高和宽。假设cache足够大。&lt;/p&gt;

&lt;p&gt;那么普通卷积的Flops数为 $B=hw c_1 c_2$&lt;/p&gt;

&lt;p&gt;则$MAC=hw(c_1+c_2)+ c_1 c_2$.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;tips: &lt;br /&gt;
$hw(c_1+c_2)$ 为这两层各自的输出值(activate)，$c_1 c_2$ 为两层间卷积核的参数量。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;则得出：$MAC \geq 2\sqrt{hwB}+\frac{B}{hw}$&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;tips: &lt;br /&gt;
$(c_1 + c_2)^2 \geq 4c_1 c_2 \Rightarrow c_1+c_2\geq 2\sqrt{c_1c_2} \Rightarrow hw(c_1+c_2) + c_1c_2 \geq 2hw\sqrt{c_1c_2} + \frac{B}{hw} \Rightarrow MAC \geq 2\sqrt{c_1c_2B}+\frac{B}{hw}$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面不等式指出，同FLOPs情况下，如果相邻两层它们的通道数相等时，MAC将达到最小值。文中采用简单两层卷积网络，实验验证了这结果。&lt;/p&gt;

&lt;p&gt;但是实际情况下G1的前提条件“cache足够大”不一定满足。设备缓存不够大时很多计算库会采用复杂的缓存机制，这会导致MAC值和理论值有偏差。&lt;/p&gt;

&lt;h4 id=&quot;g2-mac&quot;&gt;G2 过多的组卷积会增加内存访问成本MAC&lt;/h4&gt;

&lt;p&gt;组卷积能够大量减少FLOPs，也成功被应用于多种优秀网络。&lt;/p&gt;

&lt;p&gt;延续G1中的符号定义及公式，此处的FLOPs为$B=hwc_1c_2 / g$,则&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
MAC =&amp; hw(c_1+c_2) + \frac{c_1c_2}{g}\\
    =&amp; hwc_1 + \frac{Bg}{c_1} + \frac{B}{hw}
\end{align} %]]&gt;&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;tips: $\frac{c_1c_2}{g}=\frac{c_1}{g} \cdot \frac{c_1}{g}$, $B=hw\frac{c_1}{g}\frac{c_2}{g} \cdot g$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从式子看出，若组卷积的组数 $g$ 增加，则 MAC 也会增大。故太多的组卷积会增加内存访问成本。&lt;/p&gt;

&lt;p&gt;文中采用控制变量法实验验证了这准则。&lt;/p&gt;

&lt;h4 id=&quot;g3-&quot;&gt;G3 网络内部碎片操作会降低并行度&lt;/h4&gt;

&lt;p&gt;碎片(Fragmentation)是指多分支上，每条分支上的小卷积或pooling等（如外面的一次大的卷积操作，被拆分到每个分支上分别进行小的卷积操作）。&lt;/p&gt;

&lt;p&gt;如在NasNet-A就包含了大量的碎片操作，每条分支都有约13个操作，而ResnetV1上就只有2或3个操作。虽然这些Fragmented sturcture能够增加准确率，但是在高并行情况下降低了效率，增加了许多额外开销（内核启动、同步等等）。&lt;/p&gt;

&lt;p&gt;文中采用控制变量实验验证了这准则。&lt;/p&gt;

&lt;h4 id=&quot;g4-element-wise&quot;&gt;G4 Element-wise操作不容忽视&lt;/h4&gt;

&lt;p&gt;Element-wise操作指的是 ReLU、AddTensor、AddBias等等。虽然这些操作只增加了一点点FLOPs，但是会带来很高的MAC，尤其是Depthwise Convolution深度卷积。&lt;/p&gt;

&lt;p&gt;文中采用‘bottleneck’单元，进行了一系列对比实验，验证了这准则。&lt;/p&gt;

&lt;h4 id=&quot;g1-g4&quot;&gt;G1-G4的总结：&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;平衡相邻层的卷积核数&lt;/li&gt;
  &lt;li&gt;留意组卷积带来的开销&lt;/li&gt;
  &lt;li&gt;减少碎片化的程度&lt;/li&gt;
  &lt;li&gt;减少Element-wise操作&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;讨论其他的网络：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ShuffleNet-V1 严重依赖组卷积，违背了G2。&lt;/li&gt;
  &lt;li&gt;Bottleneck结构违背了G1&lt;/li&gt;
  &lt;li&gt;MobileNet-V2 采用了反转bottleneck（违背了G1），且用到了Depth-wise、以及在feature map上采用relu这些trick，违背了G4.&lt;/li&gt;
  &lt;li&gt;自动搜索生成的网络结构（如NasNet）有很高的碎片，违背了G3.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;shufflenet-v2-&quot;&gt;ShuffleNet V2 架构&lt;/h2&gt;

&lt;p&gt;基于上面种种分析，作者着手改进ShuffleNet V1。&lt;/p&gt;

&lt;p&gt;首先ShuffleNetV1是在给定的计算预算（FLOP）下，为了增加准确率，选择采用了 逐点组卷积 和 类似瓶颈的结构，且还引入了 channel shuffle操作. 如下图的(a)、(b)所示。&lt;/p&gt;

&lt;p&gt;很明显，逐点组卷积 和 类似瓶颈的结构 增加了MAC（违背G1和G2），采用了太多组卷积违背了G3， ‘Add’ 操作违背了G4，因此关键问题在于如何保持大量同宽通道的同时，让网络非密集卷积也没太多组卷积。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;shufflenet_V1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;shufflenet_V2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=”./pictures/shufflenet_v1.png”,width=”500”&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;lt;img src=”./pictures/shufflenet_v2.png”,width=”500”&amp;gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;p&gt;DWConv：深度卷积 (depthwise convolution), GConv：组卷积 (group convolution)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ShuffleNetV2 引入通道分割（channel split）操作, 将输入的feature maps分为两部分$c-c’$ 和 $c’$. 根据G3， 一个分支为shortcut流，另一个分支含三个卷积（且三个分支的通道数一样，满足G1）。 满足G2： 两个$1*1$卷积不采用组卷积操作，因为前面的通道分割已经做了分组，降低了组数。&lt;/p&gt;

&lt;p&gt;之后分支合并采用拼接（concat），让前后的channel数一样(满足G1). 然后再进行Channel Shuffle（完成和ShuffleNetV1一样的功能）。&lt;/p&gt;

&lt;p&gt;shuffle后，以往的ShuffleNetV1是再接一个ReLu（Element-wise）操作，此处将其移到卷积分支中。 另外三个连续的Element-wise操作：通道分割、concat、shuffle合并成一个Element-wise操作，这满足了G4. 具体看图(c).&lt;/p&gt;

&lt;p&gt;ShuffleNetV2 的空间下采样模块经过些修改，去掉了通道分割，输出的通道数翻倍。详情看图(d).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;这是合理的做法，因为下采样后，feature map的尺寸降低了，通道数翻倍是自然的。保证了前后两个模块的运算量相近（VGG提出的这思路，后来大家默认这么做了）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-2&quot;&gt;讨论&lt;/h2&gt;

&lt;p&gt;ShuffleNet-V2 不仅高效，还高准确率。导致的原因有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;每个构建块都很高效，从而能利用更多的feature maps 和 更大的网络容量&lt;/li&gt;
  &lt;li&gt;特征重用(feature reuse)
    &lt;ul&gt;
      &lt;li&gt;因为通道分割，从而有一半(论文设置为$c’=c/2$)的特征直接传到下一个模块中。&lt;/li&gt;
      &lt;li&gt;这和DenseNet 以及CondenseNet 思想相近。&lt;/li&gt;
      &lt;li&gt;DenseNet分析feature reuse模式发现，越相近的模块，它们的传来的shortcut特征的重要性越大。 CondenseNet也支持类似的观点。&lt;/li&gt;
      &lt;li&gt;ShuffleNet-V2也是符合这样的观点, 特征重用信息随着两个模块间的距离呈指数衰减。即$(i+j)$层的feature maps中，含有$i$ 层feature maps数为$r^j c$，其中$c$为$i$ 层的feature maps数，$r$ 为通道分割的参数，论文设为$r=0.5$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此篇论文实属业界良心，从源头分析每一步会造成的开销，从理论分析，到控制变量法做足实验，总结出 4 条准则，指导大家设计更高效的网络。&lt;/p&gt;

&lt;p&gt;G1准则（相同通道宽度）从VGG、Resnet后大家几乎都默认遵循了， 而G2准则（组卷积问题）很多网络在违背着。主要是组卷积能够提高准确率，但很有意思的是，用了组卷积速度会慢很多，但直到shuffnetv2才开始分析原因（存疑，笔者没找到更早的关于组卷积会变慢的分析）。而组卷积为什么会变好？  “Alex认为group conv的方式能够增加filter之间的对角相关性，而且能够减少训练参数，不容易过拟合，这类似于正则的效果。” (参考自&lt;a href=&quot;https://blog.csdn.net/Chaolei3/article/details/79374563&quot;&gt;对深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;至于G3准则，文中主要还是对 自主搜索网络结构 方面的工作（如NasNet 进行批判）。这或许可以在自主搜索网络结构方面加上相应的损失函数，从而让搜索出的网络结构除了高准确率外，还具备高效的能力。&lt;/p&gt;

&lt;p&gt;违背G4准则是必不可免的。毕竟基于优秀网络上的改进，很多是增加一些Element-wise操作，从而获得准确率的提升。 但ShuffleNetV2告诉了我们，Element-wise操作可以简化（如relu的移位）。&lt;/p&gt;

&lt;p&gt;如果让网络变得精简的话，这可能会让网络获得意想不到的提升，比如WRN就是特别精简的网络，打败了众多妖魔鬼怪。&lt;/p&gt;

&lt;p&gt;论文还提到了特征重用(Feature reuse)的观点，这也是Densenet的关键。 感觉很多网络，用上了特征重用的方法（Densenet类型、Resnet类型、残差金字塔类型等等），效果就会变得不错。之后需要多了解这方面的论文和方法。&lt;/p&gt;

</description>
        <pubDate>Sat, 25 Aug 2018 00:00:00 +0800</pubDate>
        <link>http://luonango.github.io/2018/08/25/ShufflenetV2_%E9%AB%98%E6%95%88%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%8C%87%E5%8D%97/</link>
        <guid isPermaLink="true">http://luonango.github.io/2018/08/25/ShufflenetV2_%E9%AB%98%E6%95%88%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%8C%87%E5%8D%97/</guid>
        
        <category>shufflenetv2</category>
        
        <category>网络设计</category>
        
        <category>组卷积</category>
        
        
      </item>
    
      <item>
        <title>促使残差零响应，并裁剪Resnet的block</title>
        <description>&lt;h1 id=&quot;resnetblock&quot;&gt;促使残差零响应，并裁剪Resnet的block&lt;/h1&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2018年,  Xin Yu. Utah&amp;amp;NVIDIA
让ResV2的残差倾向0响应，从而学习严格的IdentityMapping
剪去冗余零响应残差，让预测网络参数量少且精度不变
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;论文:《&lt;a href=&quot;https://arxiv.org/abs/1804.01661v3&quot;&gt;Learning Strict Identity Mappings in Deep Residual Networks&lt;/a&gt;》&lt;/p&gt;

&lt;p&gt;基于pre-act Resnet，文章中提出的 $\epsilon-ResNet$能自动丢弃产生的响应小于阈值的残差块， 让网络性能略有下降或没有损失。&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;section&quot;&gt;引言&lt;/h1&gt;

&lt;p&gt;Resnet论文中提到 &lt;em&gt;“我们推测，优化带残差的网络比以前的网络起来更容易。在极端情况下，如果身份映射是最优的，那么将残差被推到零，这比通过一堆非线性层去拟合身份映射更容易”&lt;/em&gt;。&lt;/p&gt;

&lt;p&gt;然而实际情况中，深网络并不能完美地让一些残差块的输出接近0，该论文提出促进零残差的方法，即让网络学习严格意义上的身份映射关系。&lt;/p&gt;

&lt;p&gt;为了测试深度对残差网络的影响，文中设计从100-300层的残差网络，测试它们在cifar10上的精度。  实验发现随着深度，精度会呈微弱上升的趋势，但波动很大。因此想： 对于训练N层的残差网络，能否找到一个层数远低于N的子网络，让其性能相当？&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;section-1&quot;&gt;本文方法&lt;/h1&gt;

&lt;p&gt;对于Resnet中，映射函数$H(x)=x+F(x)$.  于是本文作者修改为$H(x)=x+S\big(F(x)\big)$. 其中若$F(x)$ 小于阈值$\boldsymbol{\epsilon}$, 则$S\big(F(x)\big)=\boldsymbol{0}$.如果$F(x)$的响应值都不小，那么$S\big(F(x)\big)=F(x)$ . $S(\cdot)$为sparsity-promoting function.&lt;/p&gt;

&lt;p&gt;如果某残差块使用提出的变量产生0响应时，交叉熵项与L2组成的损失函数将把那么它的filter权值推到0。因此在预测时，就可以将这些残差块给删去。&lt;/p&gt;

&lt;p&gt;看起来整体思想简单合理，我们拆分得三个子问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1 如何让$F(x_i)$小于阈值$\boldsymbol{\epsilon_i}$时输出0响应值？&lt;/li&gt;
  &lt;li&gt;2 如何让 对应的残差块参数更新推向0？&lt;/li&gt;
  &lt;li&gt;3 预测时如何删去残差块？（这个其实不算问题了，手动删都可以实现了）&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fxiboldsymbolepsiloni0&quot;&gt;问题1：如何让$F(x_i)$小于阈值$\boldsymbol{\epsilon_i}$时输出0响应值&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/4_ReLU_to_zero_or_one.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
文中采用4个ReLU结构，解决了这个问题（看图）。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(a,b)表示: $a\cdot F(x_l) + b$ . 参考自论文《Highway Networks》提出的方法。&lt;/li&gt;
  &lt;li&gt;对于$F(x_l)&amp;gt;\epsilon$ 或者 $F(x_l)&amp;lt; - \epsilon$ , $S(F(x_l))= F(x_l)$：&lt;/li&gt;
  &lt;li&gt;对于$\mid F(x_l)\mid &amp;lt; \epsilon$ 则 $S(F(x_l))= 0$.&lt;/li&gt;
  &lt;li&gt;$\epsilon$是阈值。$L$是个极大的正数。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;建议模拟下，这四个ReLU的设计还是挺有意思的.&lt;/p&gt;

  &lt;p&gt;但笔者觉得还是有点问题，如果$F(x_l)&amp;gt;\epsilon, and \; F(x_l) \rightarrow \epsilon$时，在第三个ReLU中，如何保证$L\cdot\big(F(x_l)- \epsilon\big) + 1 &amp;lt;0 $ ? 这样到最后到第四个ReLU的输出，不能保证输出为1。&lt;/p&gt;

  &lt;p&gt;当然这也是当$F(x_l)= \epsilon + \sigma$ 且$\sigma &amp;lt; \frac{1}{L}$ 时才会出现问题。&lt;br /&gt;
不过这已经满足这个结构的需求。&lt;/p&gt;

  &lt;p&gt;其实如果采用Mxnet.gluon框架，用代码:&lt;/p&gt;

  &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;S = (F &amp;gt; epsilon) * F
&lt;/code&gt;&lt;/pre&gt;
  &lt;/div&gt;

  &lt;p&gt;就可以实现这个结构功能。 但得到的$S$是ndarray格式，这不是符号流的代码。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;section-2&quot;&gt;问题2：如何让 对应的残差块参数更新推向0 ？&lt;/h3&gt;

&lt;p&gt;当$F(x_l)$特别小时，稀疏促进函数$S$ 的输出为0. 所以这些参数从交叉熵损失函数获得的梯度为0。&lt;/p&gt;

&lt;p&gt;但损失函数中还有正则项，正则项会为这些参数提供梯度，从而让这些参数越来越小（推向0）。&lt;/p&gt;

&lt;p&gt;残差块中某层的权重直方图如下所示，当残差块被认定可裁剪时，它的参数被迅速收敛到0：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/weight_collapse.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这样所有问题就解决了。&lt;/p&gt;

&lt;p&gt;论文还添加了side-supervision，即在损失函数上，除了原先的交叉熵、L2正则项外，还增加了side-loss：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;假设还存在N个残差块，那么用N/2 个残差块输出来计算side-loss：
    &lt;ul&gt;
      &lt;li&gt;对侧面的输出后添加全连接层(最后一层输出维度是类别数)、Softmax、交叉熵。&lt;/li&gt;
      &lt;li&gt;计算损失后BP回去更新前N/2层的参数&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;在训练期间，缩短前半层的反向传播路径。&lt;/li&gt;
  &lt;li&gt;在总的损失函数中，side-loss的系数设置为0.1。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;感觉和GoogleNet的做法差不多，但是GoogleNet是为了更好优化网络(梯度消失/爆炸问题)，而此处是想让后N/2个残差块倾向恒等映射（这样就可以删减掉了）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;section-3&quot;&gt;总结&lt;/h1&gt;

&lt;p&gt;$\epsilon-Resnet$ 是标准残差网络的一种变体，可以端到端地自动裁剪无效的冗余残差层。且$\epsilon-Resnet$ 让模型尺寸显著减少，同时在Cifar10/100、SVHN上保持良好精度。&lt;/p&gt;

&lt;p&gt;这种简单有效的模型压缩方法还是挺吸引人的，相比其他的采用强化学习、L1正则等等方法，本文方法实现起来简单许多。&lt;/p&gt;

&lt;p&gt;文中提到与其特别相似的论文:&lt;a href=&quot;https://arxiv.org/abs/1707.01213&quot;&gt;Data-driven sparse structure selection for deep neural networks&lt;/a&gt;. 类似的还有BlockDrop等.&lt;/p&gt;

&lt;p&gt;且还有往其他层次思考：ChannelPruning，如2018_ICLR的《&lt;a href=&quot;https://arxiv.org/abs/1802.00124&quot;&gt;Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers&lt;/a&gt;》&lt;/p&gt;
</description>
        <pubDate>Wed, 01 Aug 2018 00:00:00 +0800</pubDate>
        <link>http://luonango.github.io/2018/08/01/%E4%BF%83%E4%BD%BF%E6%AE%8B%E5%B7%AE%E9%9B%B6%E5%93%8D%E5%BA%94%E5%B9%B6%E8%A3%81%E5%89%AAResnet%E7%9A%84block/</link>
        <guid isPermaLink="true">http://luonango.github.io/2018/08/01/%E4%BF%83%E4%BD%BF%E6%AE%8B%E5%B7%AE%E9%9B%B6%E5%93%8D%E5%BA%94%E5%B9%B6%E8%A3%81%E5%89%AAResnet%E7%9A%84block/</guid>
        
        <category>模型压缩</category>
        
        <category>正则化</category>
        
        <category>稀疏</category>
        
        <category>修剪block</category>
        
        
      </item>
    
      <item>
        <title>模型压缩：利用BN放缩因子来修剪Channel</title>
        <description>&lt;h1 id=&quot;bnchannel&quot;&gt;模型压缩：利用BN放缩因子来修剪Channel&lt;/h1&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2017_ICCV
利用BN的放缩因子来修剪channel
用L1将BN的放缩因子推向0.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这次介绍一篇模型压缩的论文： &lt;strong&gt;将预训练好的网络删减掉一些Channel（再fine-tuning），让模型参数减少的同时，还能让准确率维持不变（或精度损失很少）。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;论文为:《&lt;a href=&quot;https://arxiv.org/abs/1708.06519&quot;&gt;Learning Efficient Convolutional Networks through Network Slimming&lt;/a&gt;》&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;那问题来了：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1）那它是以什么准则来删减Channel？&lt;/li&gt;
  &lt;li&gt;2）总体训练步骤是什么？&lt;/li&gt;
  &lt;li&gt;3）效果如何？优缺点是？&lt;/li&gt;
  &lt;li&gt;4）类似相关工作有哪些？&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;论文方法从BN中得到启发。我们回顾下BN：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hat{z} = \frac{z_{in} - \mu_B}{\sqrt{\sigma^2_B + \epsilon}} ;\quad z_{out} = \gamma \hat{z} + \beta&lt;/script&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;其中$\mu_B$表示mini-batch B中某feature map的均值。&lt;/p&gt;

  &lt;p&gt;scale $\gamma$ 和 shift $\beta$ 都是通过反向传播训练更新。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这么看来，可以直接用 $\gamma$ 来评估channel的重要程度。$\gamma$ 的数越小，说明该channel的信息越不重要，也就可以删减掉该Channel。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;此处笔者有点疑问，为什么忽略 $\beta$ 作为评估因子？&lt;/p&gt;

  &lt;p&gt;笔者猜想的答案为：&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;feature map的重要性是看其方差而非均值。方差越大则该feature map内的特征就越明显。而均值对feature map内部的特征表达无太大关系。&lt;/li&gt;
    &lt;li&gt;由于 $z_{out} \sim N(\beta,\gamma^2)$，$\gamma$ 即方差越小，则该feature map也就可判定为重要性越小。&lt;/li&gt;
    &lt;li&gt;下一层($l+1$层)的第 $j$ 张$map_{l+1,j}$的值，是其卷积核们对 $l$ 层的所有 $map_{l,:}$ 进行卷积求和。所以如果某 $map_{l,i}$ 的方差特别小（意味着 $map_{l,i}$ 里面的所有值都相近），那么这个 $map_{l,i}$ 对 $map_{l+1,j}$ 上所有单元的值的贡献都是一样的。&lt;/li&gt;
    &lt;li&gt;这么看来，就算去掉了 $map_{l,i}$， $map_{l+1,j}$ 内的特征变化还是不大（即只是分布平移，而不是发生拉伸等变化，神经元之间的差异性变化不大）。&lt;/li&gt;
  &lt;/ul&gt;

&lt;/blockquote&gt;

&lt;p&gt;虽然可以通过删减 $\gamma$值接近零的channel，但是一般情况下，$\gamma$值靠近0的channel还是属于少数。于是作者采用smooth-L1 惩罚 $\gamma$ ，来让$\gamma$值倾向于0。&lt;/p&gt;

&lt;p&gt;那么网络的损失函数就可设计为:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;L=\sum_{(x,y)} l\big(f(x,W),y\big) + \lambda \sum_{\gamma \in \Gamma} g(\gamma)&lt;/script&gt;&lt;br /&gt;
其中$(x,y)$是训练的输入和目标， $W$是可训练的权重，$g(\cdot)$ 是引导稀疏的惩罚函数，$\lambda$作为这两项的调整。 文中选择$g(\cdot)=\mid s\mid$,当然也可以采用Smooth_L1方法在零点为光滑曲线.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Tips：&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;
      &lt;p&gt;论文中提到Smooth-L1时,引用的是：&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-540-74958-5_28&quot;&gt;2007_Fast optimization methods for l1 regularization: A comparative study and two new approaches&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;而&lt;a href=&quot;https://arxiv.org/abs/1504.08083&quot;&gt;2015_Fast R-CNN&lt;/a&gt; 提出了 Smooth-L1 公式为:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
smooth_{L1} (x)=\left\{
           \begin{array}{l}
           0.5 x^2, &amp; if\; \mid x\mid &lt; 1 \\
           \mid x\mid - 0.5 , &amp; o.w. 
           \end{array}\right. %]]&gt;&lt;/script&gt;&lt;/li&gt;
    &lt;li&gt;作者源码采用的不是Fast R-CNN提出的SmoothL1. 可以看下&lt;a href=&quot;https://github.com/liuzhuang13/slimming&quot;&gt;论文提供的源码&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;接下来我们看看训练过程（非常简明的步骤）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/NetworkSlimming_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第一步：初始化网络；&lt;/li&gt;
  &lt;li&gt;第二步：加入Channel稀疏惩罚项，训练网络；&lt;/li&gt;
  &lt;li&gt;第三步：通过固定阈值来删减channel，如删减70%的channel；&lt;/li&gt;
  &lt;li&gt;第四步：Fine-tune。由于删减channel后精度会下降，故再训练去微调网络；&lt;/li&gt;
  &lt;li&gt;第五步：可以再跳到第二步，实现多次精简网络；&lt;/li&gt;
  &lt;li&gt;第六步：得到精简后的网络。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;方法简单明了，实现起来也是很方便容易。&lt;/p&gt;

&lt;p&gt;论文采用VGG、DenseNet、Resnet模型，在数据集CIFAR10、CIFAR100、SVHN以及ImageNet上进行实验。&lt;/p&gt;

&lt;p&gt;结果表明此方法在参数量在保持相似精度情况下，参数瘦身最高可达到10倍，且计算量上也能大大减少。&lt;/p&gt;

&lt;p&gt;更多的实验及优缺点分析请详细阅读&lt;a href=&quot;https://arxiv.org/abs/1708.06519&quot;&gt;原论文&lt;/a&gt;。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;说完了文章的思路及方法，我们再温习（预习）下论文的相关工作吧。&lt;/p&gt;

&lt;p&gt;模型压缩的工作有（想知道下述方法的论文名，请查阅本论文的参考文献）：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Low-rank Decomposition 低秩分解：
    &lt;ul&gt;
      &lt;li&gt;使用SVD等技术来近似于权重矩阵（它具有低秩矩阵）。&lt;/li&gt;
      &lt;li&gt;在全连接层上工作很好，但CNN的计算主要在卷积层。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Weight Quantization 量化权值：
    &lt;ul&gt;
      &lt;li&gt;如HashNet量化网络权值（采用共享权重和哈希索引大大节省存储空间）&lt;/li&gt;
      &lt;li&gt;但不能节省运行时间（因为权重还需要恢复从而进行网络推理inference）&lt;/li&gt;
      &lt;li&gt;二值化是个很好的方法（或用三值化{-1,0,1}），但它会有精度损失。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Weight Pruning/Sparsifying 权重修剪或稀疏：
    &lt;ul&gt;
      &lt;li&gt;有论文将训练好的网络里的小权值修剪掉（即设为0），这样也可以用稀疏格式储存权值。&lt;/li&gt;
      &lt;li&gt;但是需要专用的稀疏矩阵运算库或特殊硬件来加速，且运行内存也没有减少。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Structured Pruning/Sparsifying 结构修剪或稀疏化：
    &lt;ul&gt;
      &lt;li&gt;有提出在训练好的网络中，修剪那些较小权值连接的Channel，再微调网络恢复精度方法的论文&lt;/li&gt;
      &lt;li&gt;有提出在训练前随机停用channel从而引入稀疏，但这会带来精度损失。&lt;/li&gt;
      &lt;li&gt;有提出neuron-level的稀疏方法从而修剪神经元获得紧凑玩了个，也有提出结构化稀疏学习（SSL）的方法，去稀疏CNN不同层级的结构（filters、channels、layers）。但是这些方法在训练期间均采用群组稀疏正则(Group sparsity Regualarization)方法来获得结构正则，而本文采用简单的L1稀疏，优化目标要简单很多。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Neural Architecture Learning 神经结构学习：
    &lt;ul&gt;
      &lt;li&gt;有关于自动学习网络结构的方法，如谷歌的几篇通过强化学习来搜寻最佳网络结构，或者其他的给定巨大网络结构，从中学习出最佳子图网络。&lt;/li&gt;
      &lt;li&gt;但是资源消耗太大，时间太长。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

</description>
        <pubDate>Wed, 01 Aug 2018 00:00:00 +0800</pubDate>
        <link>http://luonango.github.io/2018/08/01/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9_%E5%88%A9%E7%94%A8BN%E6%94%BE%E7%BC%A9%E5%9B%A0%E5%AD%90%E6%9D%A5%E4%BF%AE%E5%89%AAChannel/</link>
        <guid isPermaLink="true">http://luonango.github.io/2018/08/01/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9_%E5%88%A9%E7%94%A8BN%E6%94%BE%E7%BC%A9%E5%9B%A0%E5%AD%90%E6%9D%A5%E4%BF%AE%E5%89%AAChannel/</guid>
        
        <category>模型压缩</category>
        
        <category>正则化</category>
        
        <category>稀疏</category>
        
        <category>修剪channels</category>
        
        
      </item>
    
      <item>
        <title>Fusedmax与Oscarmax</title>
        <description>&lt;h1 id=&quot;fusedmaxoscarmaxattention&quot;&gt;2017_Fusedmax与Oscarmax_稀疏及结构化的Attention正则化框架&lt;/h1&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2017_NIPS
康奈尔大学, Vlad Niculae
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;论文提出&lt;strong&gt;让Attention输出稀疏&lt;/strong&gt;且更关注&lt;strong&gt;输入数据的片段或组&lt;/strong&gt;的正则化机制，它还能直接加入神经网络进行前向与反向传播。&lt;/p&gt;

&lt;p&gt;论文地址: &lt;a href=&quot;https://papers.nips.cc/paper/6926-a-regularized-framework-for-sparse-and-structured-neural-attention.pdf&quot;&gt;A Regularized Framework for Sparse and Structured Neural Attention&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;1. 简单介绍:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Attention的关键是映射函数&lt;/strong&gt;，它对输入元素的相对重要性进行编码，将数值映射为概率。而常用的关注机制&lt;strong&gt;Softmax会产生密集的注意力&lt;/strong&gt;。因为softmax的输出都大于0，故其输入的所有元素对最终决策都有或多或少的影响。&lt;/p&gt;

&lt;p&gt;为了克服softmax这种缺点，&lt;a href=&quot;https://arxiv.org/pdf/1602.02068.pdf&quot;&gt;From Softmax to Sparsemax:A Sparse Model of Attention and Multi-Label Classification&lt;/a&gt; 提出了能够&lt;strong&gt;输出稀疏概率的Sparsemax&lt;/strong&gt;，这能为Attention提供更多的解释性。&lt;/p&gt;

&lt;p&gt;本文基于Sparsemax，提出的方法既可以得到稀疏的输出，又可以作为一个诱导稀疏的惩罚模型，可作用于当前的其他模型上。 本文提出的通用框架是&lt;strong&gt;建立在$\max$运算符上的，采用强凸（strong convex）函数进行调整后得到的算子是可微的，它的梯度定义了从输入数值到概率的映射，适合作为Attention机制&lt;/strong&gt;。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;基于fused lasso提出了&lt;strong&gt;Fusedmax鼓励网络对连续区域（或说文本段）的Attention值相同&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;限定连续区域的Attention值相同的假设太严苛，文中又基于oscar提出&lt;strong&gt;Oscarmax的关注机制，鼓励网络对不连续的单词组的Attention值相同&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上述只是文章简单概括。 疑问有很多，比如：建立在$\max$算子是什么意思？强凸函数怎么调整？梯度定义了映射函数？Fusedmax和Oscarmax又是怎么做到的？&lt;/p&gt;

&lt;p&gt;那下面就一步步解释文章思路。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;## 2. 论文的思路:&lt;/p&gt;

&lt;p&gt;先给出一些定义：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;集合${1,2,…,d}$定义为$[d]$，$d-1$维的单形体为$\Delta^d:={x\in R^d:\mid\mid x\mid\mid_1=1,x \geq0}$， 在欧几里得的投影为$P_{\Delta^d}(x):={\arg\min}_{y\in \Delta^d}\mid\mid y-x\mid\mid^2$&lt;/li&gt;
  &lt;li&gt;如果函数$f:R^d\rightarrow R\bigcup {\infty}$, 则其凸共轭(convex conjugate)为$f^{&lt;em&gt;}(x):=\sup_{y\in dom\;f}y^Tx-f(y)$.  给定范数$\mid\mid\cdot\mid\mid$,它的对偶定义为$\mid\mid x\mid\mid_&lt;/em&gt; :=\sup_{\mid\mid y\mid\mid \leq 1}y^T x$. 用$\partial f(y)$ 表示函数$f$在$y$处的次微分 &lt;br /&gt;
&amp;gt; 次微分subdifferential,凸函数$f(x)=\mid x\mid$在原点的次微分是区间$[−1, 1]$.&lt;/li&gt;
  &lt;li&gt;函数$f$的Jacobian(雅可比)$J_{g}(y)\in R^{d\times d}$,Hessian(海森矩阵)$H_{f}(y)\in R^{d\times d}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;现在我们看下$max$算子（虽然是$R^d \rightarrow \Delta^d$的映射函数，但不适合作为Attention机制):&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\max(x):=\max_{i\in [d]} x_i = \sup_{y\in \Delta^d} y^Tx&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;由于在单形体的线性上确界总是顶点，也即为标准基向量${e_i}^d_{i=1}$的其中之一。 这也容易看出这个上确界$y^&lt;em&gt;$ 就是$\max(x)$的一个次梯度$\partial \max(x) ={e_{i^&lt;/em&gt;} : i^* \in \arg\max_{i\in [d]} x_i}$.&lt;/p&gt;

&lt;p&gt;我们将这些次梯度看作一种映射：$\prod:R^d \rightarrow \Delta^d$,它将所有的概率质量都放在一个元素上(即$\max$操作只有一个元素获得非0输出):$\prod(x) = e_i,\; for \;any\; e_i \in \partial \max(x)$.&lt;/p&gt;

&lt;p&gt;因为这映射函数不连续（存在阶跃断点），这不适合通过梯度下降法进行优化,显然这种属性我们是不希望的.&lt;/p&gt;

&lt;p&gt;从这可以看出，$\max(x)$的次梯度$y^*$是$\prod:R^d \rightarrow \Delta^d$映射，&lt;strong&gt;如果$\max(x)$的变种函数是连续可二次微分，那$y^&lt;em&gt;$也就可以用作Attention，且也能够让梯度下降方法进行优化了（梯度为$y^&lt;/em&gt;$的导数）&lt;/strong&gt;。&lt;br /&gt;
&amp;gt;注意，此处$\prod(x)$也表示Attention的输出，如果$\prod(x)$可导，就可以嵌入普通神经网络进行梯度下降优化了。&lt;/p&gt;

&lt;p&gt;受到&lt;a href=&quot;http://luthuli.cs.uiuc.edu/~daf/courses/optimization/MRFpapers/nesterov05.pdf&quot;&gt;Y.Nesterov. Smooth minimization of non-smooth functions&lt;/a&gt;的启发,本文运用了Smooth技术. 对于$\max(x)$的共轭函数${\max}^&lt;em&gt;(y)$:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
{\max}^*(y)=\left\{
             \begin{array}{l}
             0, &amp; if\; y\in \Delta^d \\
             \infty, &amp; o.w. 
             \end{array}\right. %]]&gt;&lt;/script&gt; &lt;br /&gt;
&amp;gt;该共轭函数的证明在&lt;a href=&quot;http://papers.nips.cc/paper/5710-smooth-and-strong-map-inference-with-linear-convergence&quot;&gt;Smooth and strong: MAP inference with linear&lt;br /&gt;
convergence, Appendix B&lt;/a&gt; 中。其实通过求解共轭函数的方法就可以得解：$max^&lt;/em&gt;(y)=\sup(y^Tx-\sup z^Tx)$, 对$x$求偏导得$y^&lt;em&gt;=z^&lt;/em&gt;$再带入原式即可。&lt;/p&gt;

&lt;p&gt;那现在将正则化添加到共轭函数中：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
{\max}^*_{\Omega}(y)=\left\{
             \begin{array}{l}
             \gamma\Omega(y), &amp; if\; y\in \Delta^d \\
             \infty, &amp; o.w. 
             \end{array}\right. %]]&gt;&lt;/script&gt; &lt;br /&gt;
其中假设函数$\Omega:R^d\rightarrow R$是关于norm $\mid\mid\cdot\mid\mid$的&lt;strong&gt;$\beta$-strongly convex&lt;/strong&gt;($\beta强凸$)。$\gamma$ 控制着正则强度。&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;参考&lt;a href=&quot;https://cswhjiang.github.io/2015/04/08/strong-convexity-and-smoothness/&quot;&gt;Strong Convexity and Smoothness&lt;/a&gt;：&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;函数$f$是 $\alpha$-strong convex($\alpha &amp;gt; 0$)需要满足条件：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;f(y)-f(x)\geq \nabla f(x)^T(y-x) + \frac{\alpha}{2}\mid\mid y-x \mid\mid^2_P&lt;/script&gt;&lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;如果一个连续可微的函数$f$的梯度$\nabla f(x)$是$\beta$-Lipschitz的，即：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\mid\mid\nabla f(x)-\nabla f(y)\mid\mid \leq\beta\mid\mid x-y\mid\mid,&lt;/script&gt;&lt;br /&gt;
那么我们称 $f(x)$ 是$\beta$-smooth的,更一般表示为：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\mid\mid\nabla f(x)-\nabla f(y)\mid\mid_D \leq\beta\mid\mid x-y\mid\mid_P,&lt;/script&gt;&lt;br /&gt;
其中$\mid\mid\cdot\mid\mid_P$是范数norm，$\mid\mid\cdot\mid\mid_D$是对偶范数dual norm。&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;函数的$\alpha$-strongly convex和 $\beta$-smoothness有对偶关系，如果函数$f$是关于对偶范数 $\mid\mid\cdot\mid\mid_D$ 的$\beta$-smooth,那么$f^&lt;em&gt;$是关于范数$\mid\mid\cdot\mid\mid_P$的$\frac{1}{\beta}$-strongly convex。 其中$f^&lt;/em&gt;=\max_y(y^Tx-f(y))$是函数$f(x)$的共轭函数。&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;Wiki中也可以查阅详细定义与解释。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;为了定义出平滑的$\max$算子: $\max_{\Omega}$，再次使用共轭：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;{\max}_{\Omega}(x)=max_{\Omega}^{**}(x) = \sup_{y\in R^d} y^Tx - {\max}^*_{\Omega} (y)=\sup_{y\in\Delta^d}y^Tx-\gamma\Omega(y)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;由此，前面提到的映射${\prod}&lt;em&gt;{\Omega}: R^d\rightarrow \Delta^d$定义为：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;{\prod}_{\Omega}(x):=\arg\max_{y\in \Delta^d}y^Tx-\gamma\Omega(y)=\nabla max_{\Omega}(x)&lt;/script&gt;&lt;br /&gt;
&amp;gt; 1. 上述式子可由：$\max&lt;/em&gt;{\Omega}(x)=(y^&lt;em&gt;)^Tx-\max^&lt;/em&gt;&lt;em&gt;{\Omega}(y^*)\Longleftrightarrow y^* \in \partial\max&lt;/em&gt;{\Omega}(x)$ 证得。&lt;br /&gt;
&amp;gt; 2. $\partial\max_{\Omega}(x)={\nabla\max_{\Omega}(x)}$只有唯一解（$y^*$是单形体的顶点）。故$\prod_{\Omega}$ 是梯度的映射函数。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;强凸的重要性：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对$\Omega$的$\beta$-strongly convex的假设是很重要的，如果函数$f:R^d\rightarrow R$ 的共轭函数 $f^*$ 是关于对偶范数 $\mid\mid\cdot\mid\mid_D$ 的$\frac{1}{\beta}$-smooth, 那么 $f$ 就是关于范数 $\mid\mid\cdot\mid\mid_P$ 的 $\beta$-strongly convex。那么这足以确保 $\max_{\Omega}$是$\frac{1}{\gamma\beta}$-smooth, 或者说 $\max_{\Omega}$ 处处可微，且它的梯度 $\prod_{\Omega}$ 在对偶范数 $\mid\mid\cdot\mid\mid_D$上是 $\frac{1}{\gamma\beta}$-Lipschitz的.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;训练问题（$\prod_{\Omega}$即表示Attention的输出）：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;前向时，需要解决的问题是：如何计算 $\prod_{\Omega}$ ?&lt;/li&gt;
  &lt;li&gt;反向时，需要解决的问题是：如何计算 $\prod_{\Omega}$ 的雅可比矩阵？或说如何计算 $\max_{\Omega}$ 的Hessian矩阵？&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;## 3. 验证方法：用正则项 $\Omega$恢复Softmax与Sparsemax&lt;/p&gt;

&lt;p&gt;在推导新的Attention机制前，先展示如何用正则项$\Omega$恢复出softmax和sparsemax。&lt;/p&gt;

&lt;h3 id=&quot;softmax&quot;&gt;3.1 Softmax：&lt;/h3&gt;

&lt;p&gt;选择$\Omega(y)=\sum_{i=1}^d y_i \log y_i$ ,即负熵。则它的共轭函数为 $log\;sum\;exp$, 即$f^*(y)=\log \sum_{i=1}^d e^{y_i}$.&lt;br /&gt;
&amp;gt;证明此时$\Omega(y)$的共轭函数为 $log\;sum\;exp$ :&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;若函数 $f(x)=\log \sum_{i=1}^n e^{x_i}$ ,则由$f^*(y)=sup y^Tx-f(x)$ 对$x$求偏导得：$y_i=\frac{e^{x_i}}{\sum_{i=1}^n e^{x_i}}$ , 即$1^Ty=1,\sum y_i =1$.&lt;/p&gt;

  &lt;p&gt;故有$e^{x_i}=y_i\sum e^{x_i} \Rightarrow$&lt;/p&gt;

  &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{array}{l}
f^*(y) &amp; =\sum y_i x_i - log\sum e^{x_i} \\
&amp; =\sum y_i \log(y_i\sum e^{x_i})-\log\sum e^{x_i} \\
&amp; = \sum y_i\log y_i + \sum y_i \log\sum e^{x_i}-\log\sum e^{x_i} \\
&amp; = \sum y_i\log y_i
\end{array} %]]&gt;&lt;/script&gt;&lt;br /&gt;
由于$f(x)$是凸函数（可证），所以$f^{**}=log\;sum\;exp$.&lt;br /&gt;
也可以查阅&lt;a href=&quot;https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf&quot;&gt;《Convex optimization》&lt;/a&gt;的习题3.25, 里面有详细证明。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果 $f(x)=\gamma g(x)$,则对于 $\gamma &amp;gt;0$,有 $f^&lt;em&gt;(y)=\gamma g^&lt;/em&gt;(y/\gamma)$. 则${\max}&lt;em&gt;{\Omega}(x)=\gamma \log\sum&lt;/em&gt;{i=1}^d e^{x_i / \gamma}$.&lt;/p&gt;

&lt;p&gt;由于$\Omega(y)$负熵在 $\mid\mid\cdot\mid\mid_1$ 是 1-strongly convex，所以${\max}&lt;em&gt;{\Omega}$在 $\mid\mid\cdot\mid\mid&lt;/em&gt;{\infty}$ 上是$\frac{1}{\gamma}$-smooth.&lt;/p&gt;

&lt;p&gt;通过对 ${\max}_{\Omega}$ 的 $x$ 求偏导即得softmax：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;{\prod}_{\Omega}(x) = \frac{\partial {\max}_{\Omega}}{\partial x}=\frac{e^{x/\gamma}}{\sum_{i=1}^d e^{x_i/\gamma}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;其中 $\gamma$越小，输出的 $softmax$就越尖锐。&lt;a href=&quot;https://arxiv.org/abs/1503.02531?context=cs&quot;&gt;Distilling the Knowledge in a Neural Network&lt;/a&gt; 里面也涉及到$\gamma$的设计。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;### 3.2 Sparsemax：&lt;/p&gt;

&lt;p&gt;选择 $\Omega(y)=\frac{1}{2}\mid\mid y\mid\mid^2_2$, 它也被称为Moreau-Yosida正则化常用于近端算子理论。&lt;/p&gt;

&lt;p&gt;由于 $\frac{1}{2}\mid\mid y\mid\mid^2_2$ 在$\mid\mid\cdot\mid\mid_2$ 中是 1-strongly convex，所以在${\max}_{\Omega}$在$\mid\mid\cdot\mid\mid_2$上是$\frac{1}{\gamma}$-smooth.&lt;/p&gt;

&lt;p&gt;由此能得：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;{\prod}_{\Omega}(x)=P_{\Delta^d}(x/\gamma) = \arg\min_{y\in \Delta^d}\mid\mid y-x/\gamma\mid\mid^2&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;当$\gamma=1$时，上式就为Sparsemax（softmax的一个稀疏替代）。&lt;/p&gt;

&lt;p&gt;这推导得出：调控 $\gamma$ 可以控制稀疏性。根据sparsemax的论文&lt;a href=&quot;https://arxiv.org/pdf/1602.02068.pdf&quot;&gt;From softmax to sparsemax: A sparse model of attention and multi-label classification&lt;/a&gt;中的公式 $9$可以知道 ${\prod}&lt;em&gt;{\Omega}$ 的雅可比矩阵为:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;J_{ {\prod}_{\Omega}}(x)=\frac{1}{\gamma}J_{P_{\Delta^d}}(x/\gamma)=\frac{1}{\gamma}\big(diag(s)-ss^T/\mid\mid s\mid\mid_1\big)&lt;/script&gt;&lt;br /&gt;
&amp;gt; 其中 $s\in{0,1}^d$ 指示着 ${\prod}&lt;/em&gt;{\Omega}(x)$ 的非$0$元素。&lt;br /&gt;
&amp;gt; &lt;br /&gt;
&amp;gt; ${\prod}_{\Omega}(x)$ 是Lipschitz 连续，处处可导。&lt;br /&gt;
&amp;gt; &lt;br /&gt;
&amp;gt; 详情建议阅读sparsemax的论文。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;## 4. Fusedmax与Oscarmax 新Attention机制&lt;/p&gt;

&lt;p&gt;论文提出 **论点 $1$：如果 $\Omega$ 函数可微，那可以计算出 ${\prod}&lt;em&gt;{\Omega}$ 的雅可比矩阵 .**&lt;br /&gt;
&amp;gt; 论文附录 $A.1$ 已证明，只要提供了$\Omega$的Jacobian和Hessian，那就可以根据论文提供的公式计算出 $J&lt;/em&gt;{ {\prod}_{\Omega}}$ .&lt;/p&gt;

&lt;p&gt;那来一个简单例子吧。&lt;/p&gt;

&lt;h3 id=&quot;squared-p-norms-p-&quot;&gt;4.1. 示例：Squared p-norms（平方 p-范式）&lt;/h3&gt;

&lt;p&gt;Squared p-norms作为单纯形上可微函数的一个有用例子：$\Omega(y)=\frac{1}{2}\mid\mid y\mid\mid^2_p = \big(\sum_{i=1}^d y_i^p \big)^{2/p}$ , 其中 $y\in\Delta^d$ 且 $p\in(1,2]$.&lt;/p&gt;

&lt;p&gt;我们已知 squared p-norm 在$\mid\mid\cdot\mid\mid_p$是strongly convex, 这也指出，当$\frac{1}{p} + \frac{1}{q} = 1$时， ${\max}_{\Omega}$ 在 $\mid\mid\cdot\mid\mid_q$ 是$\frac{1}{\gamma(p-1)}$-smooth 。计算出sq-pnorm-max为：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;{\prod}_{\Omega}(x)= \arg\min_{y\in \Delta^d} \frac{\gamma}{2}\mid\mid y\mid\mid^2_p - y^Tx&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;论点$1$ 所需要的梯度和Hessian可以通过 $\nabla\Omega(y)=\frac{y^{p-1}}{\mid\mid y\mid\mid^{p-2}&lt;em&gt;p}$ 以及下式得到所需要的$J&lt;/em&gt;{ {\prod}_{\Omega}}$.&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;H_{\Omega}(y)=diag(d)+ uu^T,\quad where \quad d=\frac{(p-1)}{\mid\mid y\mid\mid^{p-2}_p}y^{p-2} \quad and \quad u=\sqrt{\frac{(2-p)}{\mid\mid y\mid\mid^{2p-2}_p}} y^{ p-1}&lt;/script&gt;&lt;br /&gt;
&amp;gt; sq-pnorm-max 的 $p = 2$ 时就恢复为 sparsemax 一样鼓励稀疏输出。 &lt;br /&gt;
&amp;gt; &lt;br /&gt;
&amp;gt; 但$1&amp;lt;p&amp;lt;2$ 时，$y^&lt;em&gt;=[0,1]$和$y^&lt;/em&gt;=[0,1]$ 间的转换将会更平滑。所以在实验中采用 $p=1.5$ 。详情可以查阅论文中对比图的分析与实验。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;### 4.2. Fusedmax与Oscarmax&lt;/p&gt;

&lt;p&gt;上文已经采用Squared p-norm 例子展示了论点$1$（或说这一套解决方案）的可行性之后，接下论文将提出适合作为Attention机制的可微且$\beta$-strongly convex 的 $\Omega$ 函数。&lt;br /&gt;
&amp;gt; 下面会讲到Fusedmax和Oscarmax，其中会涉及到TV(全变分)和OSCAR(用于回归的八边形收缩和聚类算法)。 &lt;br /&gt;
&amp;gt; &lt;br /&gt;
&amp;gt; TV和OSCAR在文章后面会有单独解释。&lt;/p&gt;

&lt;h4 id=&quot;fusedmax&quot;&gt;Fusedmax&lt;/h4&gt;
&lt;p&gt;当输入是连续且顺序是有一定意义的时（如自然语言），我们希望能&lt;strong&gt;鼓励让连续区域的文本段有着相同的Attention值&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;为此，基于&lt;a href=&quot;https://web.stanford.edu/group/SOL/papers/fused-lasso-JRSSB.pdf&quot;&gt;Sparsity and smoothness via the fused lasso&lt;/a&gt;的fused lasso（或称 1-d total variation(TV))，选择$\Omega(y)=\frac{1}{2}\mid\mid y\mid\mid^2_2 + \lambda\sum_{i=1}^{d-1}\mid y_{i+1} - y_i\mid$, 即为强凸项和1-d TV惩罚项的和。 故可以得到：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;{\prod}_{\Omega}(x)= \arg\min_{y\in {\Delta}^d}\frac{1}{2}\mid\mid y-x/\gamma\mid\mid^2 + \lambda\sum^{d-1}_{i=1}\mid y_{i+1}- y_i\mid.&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;oscarmax&quot;&gt;Oscarmax&lt;/h4&gt;

&lt;p&gt;由于TV让连续区域聚集（即鼓励连续区域的attention值相等），这样的前提假设太严格，于是作者参考&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2007.00843.x&quot;&gt;Simultaneous regression shrinkage, variable selection, and&lt;br /&gt;
supervised clustering of predictors with OSCAR&lt;/a&gt;中的OSCAR惩罚函数，以来鼓励对元素进行聚类，让同一集群的元素它们的Attention值相等。即&lt;strong&gt;鼓励对可能不连续的单词组给予同等重视&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;选择$\Omega(y)=\frac{1}{2}\mid\mid y\mid\mid^2_2 + \lambda\sum_{i&amp;lt;j} \max(\mid y_i\mid,\mid y_j\mid)$， 故可得到：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
{\prod}_{\Omega}(x)= \arg\min_{y\in {\Delta}^d}\frac{1}{2}\mid\mid y-x/\gamma\mid\mid^2 + \lambda\sum_{i&lt;j}\max(\mid y_i\mid,\mid y_j\mid). %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;fusedmaxoscarmax&quot;&gt;4.3. Fusedmax与Oscarmax的计算与其雅可比矩阵&lt;/h3&gt;

&lt;p&gt;根据论文中的论点$2$和论点$3$, 就可得出Fusedmax和Oscarmax分别对应的雅可比矩阵了。&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
[J_{P_{TV}}(x)]_{i,j} =\left\{
             \begin{array}{l}
             \frac{1}{\mid G^*_i\mid}  &amp; if\; j\in G^*_i \\
             0 &amp; o.w. 
             \end{array}\right. %]]&gt;&lt;/script&gt; &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
[J_{P_{OSC}}(x)]_{i,j} =\left\{
             \begin{array}{l}
             \frac{sign(z^*_i z^*_j)}{\mid G^*_i\mid}  &amp; if\; j\in G^*_i \; and \; z^*_i\neq 0\\
             0 &amp; o.w. 
             \end{array}\right. %]]&gt;&lt;/script&gt; &lt;br /&gt;
&amp;gt; $P_{TV}(x):=\arg\min_{y\in {R}^d}\frac{1}{2}\mid\mid y-x\mid\mid^2 + \lambda\sum^{d-1}&lt;em&gt;{i=1}\mid y&lt;/em&gt;{i+1}- y_i\mid.$&lt;br /&gt;
&amp;gt;$P_{OSC}(x):=\arg\min_{y\in {R}^d}\frac{1}{2}\mid\mid y-x\mid\mid^2 + \lambda\sum_{i&amp;lt;j}\max(\mid y_i\mid,\mid y_j\mid)$&lt;br /&gt;
&amp;gt; &lt;br /&gt;
&amp;gt; $G^&lt;em&gt;_i={j\in [d]:\mid  z^&lt;/em&gt;_i\mid = \mid z^&lt;em&gt;_j\mid }.$ 其中$\mid G^&lt;/em&gt;_i\mid$ 表示 $i$ 组的元素数量。&lt;br /&gt;
&amp;gt; &lt;br /&gt;
&amp;gt; 论文附录 $A.2$ 有详细的证明过程。&lt;br /&gt;
&amp;gt; &lt;br /&gt;
&amp;gt; 论文的附录中还有大量实验结果，可以加深理解。&lt;/p&gt;

&lt;p&gt;来看一个 法语-英语翻译 Attention实验效果：&lt;br /&gt;
&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/fusedmax_oscarmax.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;## 5. TV与OSCAR的简单介绍：&lt;/p&gt;

&lt;h3 id=&quot;tvtotal-variation-&quot;&gt;5.1. TV(Total variation, 全变分)&lt;/h3&gt;

&lt;p&gt;TV(Total variation, 全变分)，也称为全变差，在图像复原中常用到。TV是在一函数其数值变化的差的总和，具体可查阅&lt;a href=&quot;https://zh.wikipedia.org/zh-cn/%E6%80%BB%E5%8F%98%E5%B7%AE&quot;&gt;wiki-Total_variation&lt;/a&gt;或&lt;a href=&quot;https://zh.wikipedia.org/zh-cn/%E6%80%BB%E5%8F%98%E5%B7%AE&quot;&gt;wiki-总变差&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;实值函数ƒ定义在区间$[a, b] \in R$的总变差是一维参数曲线$x \rightarrow ƒ(x) , x \in [a,b]$的弧长。 连续可微函数的总变差，可由如下的积分给出:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;V^a_b(f)=\int^b_a \mid f&#39;(x)\mid \mathrm{d}x&lt;/script&gt;&lt;br /&gt;
任意实值或虚值函数ƒ定义在区间[a,b]上的总变差，由下面公式定义($p$为区间$[a,b]$中的所有分划):&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;V^a_b(f)=sup_P\sum^{n_p -1}_{i=0}\mid f(x_{i+1}) - f(x_{i})\mid&lt;/script&gt;&lt;/p&gt;

&lt;/blockquote&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;TV图解&lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;当绿点遍历整个函数时，&amp;lt;p&amp;gt;绿点在y-轴上的投影红点走过的&lt;strong&gt;路程&lt;/strong&gt;&amp;lt;p&amp;gt;就是该函数的总变分TV&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Total_variation.gif&quot; alt=&quot;Total_variation&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;因为细节或假细节（如噪音）区域较多的信号则TV值较大，那假如我们得到观察信号$x_i$, 希望对$x_i$进行去噪，就可以通过引入最小化$x_i$的全变分，得到去噪且保持了图像边缘的图像。即对原复原函数引入TV正则项，如一维去噪：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\min_y \frac{1}{2}\sum_i(x_i-y_i)^2 + \lambda \sum_{i=1}^n\mid y_{i+1} - y_i \mid&lt;/script&gt;&lt;br /&gt;
更多解释可参考&lt;a href=&quot;https://www.zhihu.com/question/47162419&quot;&gt;如何理解全变分（Total Variation，TV）模型？&lt;/a&gt;和&lt;a href=&quot;https://blog.csdn.net/hanlin_tan/article/details/52448803&quot;&gt;浅谈图象的全变分和去噪&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;论文中说Fused Lasso也称为1d-TV ，公式为：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}_{fused} = {\arg\min}_{\beta}\frac{1}{2}{\mid\mid y-\beta\mid\mid}_2^2 + \lambda \cdot \mid\mid D\beta\mid\mid_1&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;在1维中 : $\hat{\beta}&lt;em&gt;{fused} = {\arg\min}&lt;/em&gt;{\beta}\frac{1}{2}{\mid\mid y-\beta\mid\mid}&lt;em&gt;2^2 + \lambda \cdot \sum&lt;/em&gt;{i=1}^{n-1}\mid\beta_i - \beta_j\mid_1 $ ， 则此时$D$ 为(若n=5)：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\left(
 \begin{matrix}
   1 &amp; -1 &amp; 0 &amp; 0 &amp; 0\\
   0 &amp; 1 &amp; -1 &amp; 0 &amp; 0\\
   0 &amp; 0 &amp; 1 &amp; -1 &amp; 0\\
   0 &amp; 0 &amp; 0 &amp; 1 &amp; -1
  \end{matrix}
  \right) %]]&gt;&lt;/script&gt;&lt;br /&gt;
将上述方法应用在1维数据得到结果如下(图来自&lt;a href=&quot;http://euler.stat.yale.edu/~tba3/stat612/lectures/lec22/lecture22.pdf&quot;&gt;The Generalized Lasso，Lecture 22&lt;/a&gt;）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Fuse_lasso_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看出，数据最终呈现连续区域的聚集，即空间聚集（spatial clutering)，也就得到了稀疏形式的数据表示。 得到更平滑的数据表示，也能防止过拟合的数据表示。&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;但是TV让连续区域聚集（即鼓励连续区域的值相等），这样的前提假设太严格，于是作者参考OSCAR惩罚函数，以来鼓励对Attention的输出值进行聚集，让同集群的Attention值相等。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;p&gt;### 5.2. OSCAR (用于回归的八边形收缩和聚类算法)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2007.00843.x&quot;&gt;Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with OSCAR&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;OSCAR(用于回归的八边形收缩和聚类算法，octagonal shrinkage and clustering algorithm for regression)可以被解释为同时实现聚类和回归. 是做稀疏和分组变量选择，类似于Elastic Net.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://people.ee.duke.edu/~lcarin/Minhua3.20.09.pdf&quot;&gt;OSCAR-PPT解释&lt;/a&gt;：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp;Penalized Regression一般形式： &amp; \hat{\beta} &amp;  = \min_{\beta}\mid\mid y- x\beta\mid\mid^2_2 \quad s.t. \quad f(\beta)\le t \\
&amp;Ridge Regression: &amp; f(\beta) &amp;=\mid\mid\beta\mid\mid^2_2=\sum^{p}_{j=1}\beta^2_j\\
&amp;LASSO：  &amp; f(\beta) &amp; =\mid\mid\beta\mid\mid_1=\sum^{p}_{j=1}\mid\beta\mid \\
&amp;Group LASSO:  &amp; f(\beta) &amp; =\sum^{G}_{g=1}\mid\mid\beta_g\mid\mid_2\\
&amp;Elastic Net(弹性网络）:  &amp; f(\beta) &amp; =\alpha\mid\mid\beta\mid\mid^2_2+(1-\alpha)\mid\mid\beta\mid\mid_1\\
&amp;OSCAR:  &amp; f(\beta) &amp; =\mid\mid\beta\mid\mid_1 + c\sum_{j &lt; k} \max\{\mid\beta\mid_j,\mid\beta\mid_k\}, L_1与pair-wise L_{\infty}组合。
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;OSCAR是做稀疏和分组变量选择，类似于Elastic Net.&lt;/li&gt;
  &lt;li&gt;与Group-LASSO相比，它不需要群组结构的先验知识。&lt;/li&gt;
  &lt;li&gt;它比Elastic Net有更强的假设，OSCAR假定：相关变量（correlated variables）的回归系数的绝对值相同。&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Elastic Net&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;OSCAR&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/oscar_elasticnet.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/oscar_oscar.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$L_1$和$L_2$范式组合&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$L_1$和$L_{\infty}$范式组合&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;看上图也就能理解“八边形收缩”这名称了. 该OSCAR基于惩罚最小二乘法，将一些系数收缩至恰好为零。同时这惩罚函数能产生值相同的系数，鼓励相关的预测因子(即指$x_i$)它们对最终结果有着相同的影响，从而形成单个系数表示预测因子群集。&lt;/p&gt;

&lt;p&gt;至于更详细的理解，就阅读下原论文吧&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2007.00843.x&quot;&gt;Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with OSCAR&lt;/a&gt;。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;## 6. 后语&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;论文中提出新的Attention机制：fusedmax和oscarmax，它们是能被反向传播训练的神经网络所使用。基于论文给出的公式计算即可完成该Attention机制的前向和反向的计算。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;文中的实验都是文本语言方面（textual entailment，machine translation, summarization）， 但如果想用于视觉方面，应该需要适当修改 fusedmax。因为fusedmax所指的连续的区域应当是二维局域图，而非一维连续点（卷积层中）。而oscarmax因为可以对不连续的元素进行Attention值上的聚集，故可以直接应用在图像方面。&lt;/p&gt;

&lt;p&gt;Attention的输出值进行稀疏，这个原因和理由容易理解（即稀疏的优点）。但为什么要对元素进行聚集且赋予相同的Attention值呢？我觉得主要的原因还是防止过拟合的数据表示，即防止Attention机制过拟合。当然，如果翻翻聚类方面的论文或许有更好的解释。&lt;/p&gt;

&lt;p&gt;文中很多公式在其他论文中已证出，想完整地看懂这篇文章，还需要好好把引用的论文理解理解。&lt;br /&gt;
&amp;gt;随便点开都是一屏幕公式推导&lt;/p&gt;

&lt;p&gt;由于我数学方面的功底不好，论文涉及的一些背后知识都是现查现学.&lt;/p&gt;

&lt;p&gt;欢迎讨论指错，轻喷就好 = =。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/38897903&quot;&gt;若出现格式问题，可移步查看知乎同款文章：Fusedmax与Oscarmax：稀疏及结构化的Attention正则化框架&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Sun, 01 Jul 2018 00:00:00 +0800</pubDate>
        <link>http://luonango.github.io/2018/07/01/Fusedmax%E4%B8%8EOscarmax%E7%A8%80%E7%96%8F%E5%8F%8A%E7%BB%93%E6%9E%84%E5%8C%96%E7%9A%84Attention%E6%AD%A3%E5%88%99%E5%8C%96%E6%A1%86%E6%9E%B6/</link>
        <guid isPermaLink="true">http://luonango.github.io/2018/07/01/Fusedmax%E4%B8%8EOscarmax%E7%A8%80%E7%96%8F%E5%8F%8A%E7%BB%93%E6%9E%84%E5%8C%96%E7%9A%84Attention%E6%AD%A3%E5%88%99%E5%8C%96%E6%A1%86%E6%9E%B6/</guid>
        
        <category>Attention</category>
        
        <category>正则化</category>
        
        <category>稀疏</category>
        
        
      </item>
    
      <item>
        <title>卷积网络的特征通道</title>
        <description>&lt;h1 id=&quot;section&quot;&gt;卷积网络的特征通道&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;从实际的角度出发，由于深度神经网络受到大量矩阵运算的限制，往往需要海量存储及计算资源。削减神经网络中卷积单元的冗余是解决这个问题的重要研究之一，它可并分为空间（spatial）及通道（channel）两个方向。&lt;/p&gt;

&lt;p&gt;先谈谈通道上的解决方法。&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;alexnet&quot;&gt;从Alexnet说起：&lt;/h1&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Alexnet&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/AlexNet_1.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;单个GTX 580 GPU只有3GB内存,不能满足当时的参数规模&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;将网络分布在两个GPU上，每个GPU中放置一半核（或神经元），GPU间的通讯只在某些层进行&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;如第3层的核需要从第2层中所有核映射输入。然而，第4层的核只需要从第3层中位于同一GPU的那些核映射输入&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;即为双通道网络结构，减少了大量的连接参数消耗&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;
&lt;p&gt;# Inception系列也是多通道&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Inception_v1&lt;/th&gt;
      &lt;th&gt;Inception_v2-v3&lt;/th&gt;
      &lt;th&gt;Xception&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Inception_v1_2.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Inception_v2_3.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Xception_1.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Xception中主要采用&lt;strong&gt;depthwise separable convolution&lt;/strong&gt;技术（mobileNets中提出，深度学习模型加速网络的核心技术）。&lt;/p&gt;

&lt;p&gt;假设输入的是A个特征图，最终输出B个特征图。Xception的卷积就是将传统的卷积操作分成两步：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;然后用A个1*1的卷积核正常卷积生成的A个特征图。求和，最后生成A个特征图&lt;/li&gt;
  &lt;li&gt;用B个3*3卷积核，一对一卷积B个特征图。不求和，直接拼生成B个特征图；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;##　mobileNets与Xception使用depthwise的区别：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;mobileNets中与Xception中的执行顺序刚好相反。&lt;/li&gt;
  &lt;li&gt;在mobileNet中主要就是为了降低网络的复杂度。&lt;/li&gt;
  &lt;li&gt;在Xception中作者加宽了网络，使得参数数量和Inception v3差不多，然后在这前提下比较性能。Xception目的不在于模型压缩，而是提高性能。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;iccvigcinterleaved-group-convolution&quot;&gt;2017_ICCV_微软的交错组卷积（IGC，Interleaved Group Convolution）&lt;/h1&gt;

&lt;h2 id=&quot;icg&quot;&gt;ICG模块&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;交错组卷积模块：含两个组卷积过程&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Interleaved_Group_Convolutions_for_Deep_Neural_Networks_1.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;第一次组卷积：不同组的卷积不存在交互，不同组的输出通道并不相关。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;第二次组卷积：让不同组的通道相关联。每组的输入通道均来自第一组输出的不同的组的通道（交错组成）&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;section-1&quot;&gt;特点：&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;从通道角度出发，解决了神经网络基本卷积单元中的冗余问题。&lt;/li&gt;
  &lt;li&gt;可以在无损性能的前提下，缩减模型、提升计算速度，有助于深度网络在&lt;strong&gt;移动端&lt;/strong&gt;的部署.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;极端和最优分配下的分析&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Xception 是特例：
    &lt;ul&gt;
      &lt;li&gt;如果第一次组的每组只有一个输出通道，那么就变成了特殊的组卷积，即 channel-wise convolution，第二次组卷积成为 1X1 的卷积，这与 Xception 相似（顺序相反）。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;普通卷积：
    &lt;ul&gt;
      &lt;li&gt;如果第一次组卷积过程里仅有一组，那么这个过程就变成了普通卷积，第二次组卷积过程则相当于分配给每个通过一个不同的权重。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;分配情况：
    &lt;ul&gt;
      &lt;li&gt;极端情况下:网络的性能会随着通道数及组数的变化而变化&lt;/li&gt;
      &lt;li&gt;最优性能配置点存在于两个极端情况之间。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;# 2017_Face++的通道随机分组ShuffleNet&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;模块构造&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Pointwise组卷积、Channel-Shuffle 和 Depthwise&lt;/strong&gt; 组成的模块&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/ShuffleNet_An_Extremely_Efficient_Convolutional_Neural_Network_for_Mobile_Devices_1.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GConv 指组卷积。 group convolution&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pointwise：卷积核为1*1的组卷积(不采用组卷积则计算量太大）。 pointwise group convolution&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Depthwise：前面提过，先分别组卷积，再1×1卷积核一起卷积。 depthwise separable convolution&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Channel Shuffle: 将通道洗牌，交错合并组的分块&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;基于ShuffleNet的ResNet&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/ShuffleNet_An_Extremely_Efficient_Convolutional_Neural_Network_for_Mobile_Devices_2.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;a图是普通Residual block&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;b图中将Residual block用：&lt;strong&gt;Pointwise组卷积+ Shuffle + 3×3的Depthwise + Pointwise组卷积&lt;/strong&gt; 替代&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;c图是实验采用的Residual block&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;section-4&quot;&gt;特点：&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;采用 &lt;strong&gt;Channel-Shuffle、Pointwise组卷积 和 Depthwise&lt;/strong&gt; 来修改原来的ResNet单元&lt;/li&gt;
  &lt;li&gt;Pointwise组卷积和Depthwise主要为了减少了计算量&lt;/li&gt;
  &lt;li&gt;与交错组卷积IGC单元（Interleaved Group Convolution）相似。&lt;/li&gt;
  &lt;li&gt;大幅度降低网络计算量，专用于计算能力非常有限的移动端设备.&lt;/li&gt;
  &lt;li&gt;超越mobilenet、媲美AlexNet的准确率.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;该论文和IGC在shuffle上相似，且两者都是17年7月份提交。IGC未引用该文，而此文中提到：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;最近，另一项并行工作[41]也采用这个思想进行两阶段卷积。然而，[41]没有&lt;strong&gt;专门调查&lt;/strong&gt;通道shuffle本身的有效性和在小模型设计上的使用。&lt;/li&gt;
  &lt;li&gt;Such “random shuffle” operation has different purpose and been seldom exploited later. Very recently, another concurrent work[41] also adopt this idea for a two-stage convolution. However,[41] did not &lt;strong&gt;specially investigate&lt;/strong&gt; the effectiveness of channel shuffle itself and its usage in tiny model design.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;senet-2017imagenet&quot;&gt;考虑通道的不平等关系SENet (2017_ImageNet冠军)&lt;/h1&gt;

&lt;h2 id=&quot;section-5&quot;&gt;模块构造&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Squeeze-and-Excitation 模块&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Squeeze-and-Excitation_Networks_1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;图中输入是X，有$c_1$特征通道。经过一系列卷积变换后，得$c_2$个大小为$w×h$的通道（map）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Squeeze&lt;/strong&gt;： 进行特征压缩，每个二维通道变为一个实数，如图得$1×1×c_2$.它们代表相应的特征通道的全局分布.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Excitation&lt;/strong&gt;: 学习特征通道间的相关性。根据输入，调节各通道的权重&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Reweight&lt;/strong&gt;: 以对应的通道权重，乘回对应通道，完成对原始特征的重标定&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;section-6&quot;&gt;部署示例&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;SE_Inception图示&lt;/th&gt;
      &lt;th&gt;SE_ResNet图示&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Squeeze-and-Excitation_Networks_4.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Squeeze-and-Excitation_Networks_5.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;##　实验情况：增加小的计算消耗，获得大的性能提升&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Squeeze-and-Excitation_Networks_3.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Squeeze-and-Excitation_Networks_6.jpg&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;section-7&quot;&gt;特点：&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;考虑特征通道间的关系，动态调整各通道的特征响应值&lt;/li&gt;
  &lt;li&gt;构造简单、容易被部署。增加很小的计算消耗，获得高的性能提升&lt;/li&gt;
  &lt;li&gt;或许可用于辅助网络修剪/压缩的工作&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;section-8&quot;&gt;参考&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;2012-ImageNet-Classification-with-Deep-Convolutional-Neural-Networks&lt;/li&gt;
  &lt;li&gt;2016-Xception: Deep Learning with Depthwise Separable Convolutions&lt;/li&gt;
  &lt;li&gt;2017-Interleaved Group Convolutions for Deep Neural Networks&lt;/li&gt;
  &lt;li&gt;2017-ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices&lt;/li&gt;
  &lt;li&gt;2017-Squeeze-and-Excitation Networks&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-9&quot;&gt;小想法：&lt;/h3&gt;

&lt;p&gt;在组通道网络中，采用SENet思想，对组通道进行加权（而非单通道）。让SENet为一个特例（组为单通道）。&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Tue, 01 May 2018 00:00:00 +0800</pubDate>
        <link>http://luonango.github.io/2018/05/01/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%9A%E9%81%93/</link>
        <guid isPermaLink="true">http://luonango.github.io/2018/05/01/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%9A%E9%81%93/</guid>
        
        <category>CNN</category>
        
        <category>网络结构</category>
        
        <category>feature maps</category>
        
        
      </item>
    
      <item>
        <title>CapsulesNet的解析</title>
        <description>&lt;h1 id=&quot;capsulesnet&quot;&gt;CapsulesNet的解析&lt;/h1&gt;

&lt;h2 id=&quot;section&quot;&gt;前言：&lt;/h2&gt;

&lt;p&gt;本文先简单介绍传统CNN的局限性及Hinton提出的Capsule性质，再详细解析Hinton团队近期发布的基于动态路由及EM路由的CapsuleNet论文。&lt;/p&gt;

&lt;h2 id=&quot;hintoncnn&quot;&gt;Hinton对CNN的思考&lt;/h2&gt;

&lt;p&gt;Hinton认为卷积神经网络是不太正确的，它既不对应生物神经系统，也不对应认知神经科学，甚至连CNN本身的目标都是有误的。&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;在生物神经系统上不成立&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;反向传播难以成立。神经系统需要能够精准地求导数，对矩阵转置，利用链式法则，这种解剖学上从来也没有发现这样的系统存在的证据。&lt;/li&gt;
  &lt;li&gt;神经系统含有分层，但是层数不高，而CNN一般极深。生物系统传导在ms量级（GPU在us量级），比GPU慢但效果显著。&lt;/li&gt;
  &lt;li&gt;灵长类大脑皮层中大量存在皮层微柱，其内部含有上百个神经元，并且还存在内部分层。与我们使用的CNN不同，它的一层还含有复杂的内部结构。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;cnn&quot;&gt;在认知神经科学上CNN也靠不住脚&lt;/h4&gt;
&lt;p&gt;人会不自觉地根据物体形状建立“坐标框架”(coordinate frame)。并且坐标框架的不同会极大地改变人的认知。人的识别过程受到了空间概念的支配，判断物体是否一样时，我们需要通过旋转把坐标框架变得一致，才能从直觉上知道它们是否一致，但是CNN没有类似的“坐标框架”。如人类判断下图两字母是否一致：&lt;br /&gt;
&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Capsules_net_Mental_rotation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;cnn-1&quot;&gt;CNN的目标不正确&lt;/h4&gt;

&lt;p&gt;先解释下不变性（Invariance)和同变性（Equivariance）。不变性是指物体本身不发生任何变化；同变性指物体可以发生变化，但变化后仍是这种物体。如将MNIST图片中的数字进行平移（该数字形状无变化），就实现了该数字的不变性。如果数字进行了立体的旋转翻转等，但人类仍能识别出这个数字，这就是该数字的同变性。&lt;/p&gt;

&lt;p&gt;显然，我们希望的是CNN能够实现对物体的同变性（Equivariance），虽然CNN在卷积操作时实现了同变性（卷积操作是对视野内的物体进行矩阵转换，实现了视角变换），但主要由于Pooling，从而让CNN引入了不变性。这从而让CNN实现对物体的不变性而不是同变性。&lt;/p&gt;

&lt;p&gt;比如CNN对旋转没有不变性（即旋转后的图片和原图CNN认为是不一样的），我们平时是采用数据增强方式让达到类似意义上的旋转不变性（CNN记住了某图片的各种角度，但要是有个新的旋转角度，CNN仍会出问题）。当CNN对旋转没有不变性时，也就意味着舍弃了“坐标框架”。&lt;/p&gt;

&lt;p&gt;虽然以往CNN的识别准确率高且稳定，但我们最终目标不是为了准确率，而是为了得到对内容的良好表示，从而达到“理解”内容。&lt;/p&gt;

&lt;h2 id=&quot;hintoncapsules&quot;&gt;Hinton提出的Capsules&lt;/h2&gt;

&lt;p&gt;基于上述种种思考，Hinton认为物体和观察者之间的关系（比如物体的姿态），应该由一整套激活的神经元表示，而不是由单个神经元或者一组粗编码（coarse-coded）表示（即一层中有复杂的内部结构）。这样的表示，才能有效表达关于“坐标框架”的先验知识。且构成的网络必须得实现物体同变性。&lt;/p&gt;

&lt;p&gt;这一套神经元指的就是Capsule。Capsule是一个高维向量，用一组神经元而不是一个来代表一个实体。并且它还隐含着所代表的实体出现的概率。&lt;/p&gt;

&lt;p&gt;Hinton认为存在的两种同变性(Equivariance)及capsule的解决方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;位置编码（place-coded）：视觉中的内容的位置发生了较大变化，则会由不同的 Capsule 表示其内容。&lt;/li&gt;
  &lt;li&gt;速率编码（rate-coded）：视觉中的内容为位置发生了较小的变化，则会由相同的 Capsule 表示其内容，但是内容有所改变。&lt;/li&gt;
  &lt;li&gt;两者的联系是，高层的 capsule 有更广的域 (domain)，所以低层的 place-coded 信息到高层会变成 rate-coded。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;dynamic-routing-between-capsules&quot;&gt;第一篇《Dynamic Routing Between Capsules》的解析&lt;/h2&gt;

&lt;p&gt;好的，那让我们看看Hinton 团队2017年10月公布的论文：&lt;a href=&quot;https://arxiv.org/abs/1710.09829&quot;&gt;Dynamic Routing Between Capsules&lt;/a&gt;（Sara Sabour为第一作者）&lt;/p&gt;

&lt;p&gt;官方源代码已发布：&lt;a href=&quot;https://github.com/Sarasra/models/tree/master/research/capsules&quot;&gt;Tensorflow代码&lt;/a&gt; 。不得不提下，由于官方代码晚于论文，论文发布后很多研究者尝试复现其代码，获得大家好评的有&lt;a href=&quot;https://github.com/naturomics/CapsNet-Tensorflow&quot;&gt;HuadongLiao的CapsNet-Tensorflow&lt;/a&gt;、&lt;a href=&quot;https://github.com/XifengGuo/CapsNet-Keras&quot;&gt;Xifeng Guo的CapsNet-Keras&lt;/a&gt;等等。&lt;/p&gt;

&lt;p&gt;该论文中的Capsule是一组神经元（即向量），表示特定类型实体的实例化参数。而其向量的长度表示该实体存在的概率、向量的方向表示实例化的参数。同一层的 capsule 将通过变换矩阵对高层的 capsule 的实例化参数进行预测。当多个预测一致时（文中使用动态路由使预测一致），高层的 capsule 将变得活跃。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;动态路由方法&lt;/h3&gt;

&lt;p&gt;有很多方法实现capsule的总体思路，该论文只展示了动态路由方法（之后不久Hinton用EM-Routing的方法实现capsule的整体思路，相应解析在后面）。&lt;/p&gt;

&lt;p&gt;由于想让Capsule的输出向量的长度，来表示该capsule代表的实体在当前的输入中出现的概率，故需要将输出向量的长度（模长）限制在$[0,1]$。文中采用Squashing的非线性函数作为激活函数来限制模长，令当前层是$j$层，$v_j$为$capsule_j$的输出向量，$s_j$是$capsule_j$的所有输入向量。&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;v_j=\frac{\mid\mid s_j\mid \mid ^2}{1+\mid\mid s_j\mid\mid ^2}\frac{s_j}{\mid\mid s_j\mid \mid }&lt;/script&gt;&lt;br /&gt;
那$s_j$怎么来呢？别急，我们定义一些概念先：&lt;/p&gt;

&lt;p&gt;令$u_i$是前一层(第$i$层）所有$capsule_i$的输出向量，且$\hat{u}&lt;em&gt;{j|i}$是$u_i$经过权重矩阵$W&lt;/em&gt;{ij}$变换后的预测向量。&lt;br /&gt;
那除了第一层，其他层的$s_j$都是前一层所有$capsule_i$的预测向量$\hat{u}&lt;em&gt;{j|i}$的加权和（加权系数为$c&lt;/em&gt;{ij}$）。&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;s_j = \sum_i c_{ij}\hat{u}_{j|i} \quad,\qquad \hat{u}_{j|i} = W_{ij}u_i&lt;/script&gt;&lt;br /&gt;
其中的耦合系数$c_{ij}$就是在迭代动态路由过程中确定的，且每个$capsule_i$与高层所有$capsule_j$的耦合系数$c_{ij}$之和为1。它是通过‘路由softmax’得出：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;c_{ij}=\frac{exp(b_{ij})}{\sum_k exp(b_{ik})}&lt;/script&gt;&lt;br /&gt;
$b_{ij}$可理解为当前$j$层的输出向量$v_j$与前一层所有$capsule_i$的预测向量$\hat{u}_{j|i}$的相似程度，它在动态路由中的迭代公式为：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;b_{ij}=b_{ij}+\hat{u}_{j|i}v_j&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;那么，当得到$l$层的$capsule_i$所有输出向量$u_i$，求$l+1$层的输出向量的流程为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;a) 通过权重矩阵$W_{ij}$，将$u_i$变换得到预测向量$\hat{u}_{j&lt;/td&gt;
          &lt;td&gt;i}$。权重矩阵$W$是通过反向传播更新的。&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;b) 进入动态路由进行迭代（通常迭代3次就可以得到较好结果）:
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Capsules_net_6.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;c) 得到第$l+1$ 层$capsule_j$的输出向量$v_j$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以看看&lt;a href=&quot;https://www.jiqizhixin.com/articles/2017-11-05&quot;&gt;机器之心绘制的层级结构图&lt;/a&gt;来加深理解：&lt;br /&gt;
&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/CapsNet_1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;capsule&quot;&gt;Capsule网络结构&lt;/h3&gt;
&lt;p&gt;解决了动态路由的算法流程后，我们再来看下论文设计的简单的网络结构：&lt;br /&gt;
&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Capsules_net_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;第一层ReLU Conv1层的获取&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;普通的卷积层方法，使用256个9×9的卷积核、步幅为1、ReLU作为激活函数，得到256×20×20的张量输出。与前面的输入图片层之间的参数数量为:256×1×9×9+256=20992.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;第二层PrimaryCaps层的获取&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;从普通卷积层构建成Capsule结构的PrimaryCaps层，用了256×32×8个9×9的卷积核，步幅为2，32×8个bias，得到32×8×6×6的张量输出（即32×6×6个元素数为8的capsule）。&lt;/li&gt;
      &lt;li&gt;与前面层之间的参数总量为：256×32×8×9×9+32×8=5308672.（不考虑routing里面的参数）&lt;/li&gt;
      &lt;li&gt;值得注意的是，官方源码中接着对这32×6×6个capsule进行动态路由过程（里面包括了Squashing操作），得到新的输出。而论文中没提及到此处需要加上动态路由过程，导致一些研究人员复现的代码是直接将输出经过Squashing函数进行更新输出（部分复现者已在复现代码中添加了动态路由过程）。&lt;/li&gt;
      &lt;li&gt;PrimaryCaps层的具体构建可查看源码中的layers.conv_slim_capsule函数。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;第三层DigitCaps层的获取&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;从PrimaryCaps层有32&lt;em&gt;6&lt;/em&gt;6=1152个capsule，DigitCaps有10个，所以权重矩阵$W_{ij}$一共有1152×10个，且$W_{ij}=[8×16]$，即$i \in [0,8), j\in[0,16)$。另外还有10*16个偏置值。&lt;/li&gt;
      &lt;li&gt;进行动态路由更新，最终得到10*16的张量输出。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;参数的更新&lt;/strong&gt;：
    &lt;ul&gt;
      &lt;li&gt;权重矩阵 $W_{ij}、bias$通过反向传播进行更新。&lt;/li&gt;
      &lt;li&gt;动态路由中引入的参数如$c_{ij}、b_{ij}$均在动态路由迭代过程中进行更新。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-3&quot;&gt;损失函数&lt;/h3&gt;
&lt;p&gt;解决了论文设计的网络结构后，我们来看下论文采用损失函数（Max-Margin Loss形式）：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;L_c=T_c \max\big(0,m^+ - \mid\mid v_c\mid\mid\big)^2 + \lambda\big(1-T_c\big) \max\big(0,\mid\mid v_c\mid\mid - m^-\big)^2&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;其中$c$表示类别，$c$存在时，指示函数$T_c=1$,否则$T_c=0$。$m^+、m^-$分别为上边界和下边界，$\mid\mid v_c\mid\mid$为$v_c$的L2范数。论文设置$\lambda=0.5$，降低第二项的loss的系数，防止活泼的（模长较大）capsule倾向于缩小模长，从而防止网络训练差（系数大则求导的数值绝对值大，则第二项loss反馈的更新会更有力）。上下边界一般不为1或0，是为了防止分类器过度自信。&lt;/li&gt;
  &lt;li&gt;总loss值是每个类别的$L_c$的和:  $L=\sum_{c} L_c$&lt;/li&gt;
  &lt;li&gt;该损失函数与softmax区别在于：
    &lt;ul&gt;
      &lt;li&gt;softmax倾向于提高单一类别的预测概率而极力压低其他类别的预测概率，且各类别的预测概率和为1。适用于单类别场景中的预测分类。&lt;/li&gt;
      &lt;li&gt;而此损失函数，要么提高某类别的预测概率（若出现了该类 ），要么压低某类别的预测概率（若未出现该类），不同类别间的预测概率互不干扰，每个类别的预测概率均在$[0,1]$中取值。适用于多类别并存场景中的预测分类。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-4&quot;&gt;重构与表征&lt;/h3&gt;

&lt;p&gt;重构的思路很简单，利用DigitCaps中的capsule向量，重新构建出对应类别的图像。文章中使用额外的重构损失来促进 DigitCaps 层对输入数字图片进行编码：&lt;br /&gt;
![](&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Capsules_net_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;由于DigitCaps中每个capsule向量都代表着一个实体(数字)，文章采用掩盖全部非正确数字的capsule，留下代表正确数字实体的capsule来重构图片。&lt;/p&gt;

&lt;p&gt;此处的重构损失函数是采用计算在最后的 FC Sigmoid 层采用的输出像素点与原始图像像素点间的欧几里德距离。且在训练中，为了防止重构损失主导了整体损失（从而体现不出Max-Margin Loss作用），文章采用 0.0005 的比例缩小了重构损失。&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;实验结果&lt;/h3&gt;

&lt;h4 id=&quot;mnist&quot;&gt;采用MNIST数据集进行分类预测：&lt;/h4&gt;
&lt;p&gt;在此之前，一些研究者使用或不使用集成+数据增强，测试集错误率分别为0.21%和0.57%。而本文在单模型、无数据增强情况下最高达到0.25%的测试集错误率。具体如下图：&lt;br /&gt;
&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Dynamic_Routing_10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;multimnist&quot;&gt;采用MultiMNIST数据集进行重构实验：&lt;/h4&gt;

&lt;p&gt;采用混合数字图片的数据集进行重构，如下图，对一张两数字重叠的图片进行重构，重构后的数字用不同颜色显示在同一张图上。$L:(l_1,l_2)$表示两数字的真实标签，$R:(r_1,r_2)$表示预测出并重构的两数字，带$*$标识的那两列表示重构的数字既不是标签数字也不是预测出的数字（即挑取其他capsule进行重构）。&lt;br /&gt;
&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Dynamic_Routing_11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;## 第二篇《Matrix Capsules with EM routing》的解析&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://openreview.net/pdf?id=HJWLfGWRb&quot;&gt;Matrix Capsules with EM routing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;紧接着上一篇，Hinton以第一作者发布的了这篇论文，现已被ICLR接收。那这一篇与前一篇动态路由Capsule有什么不同呢？&lt;/p&gt;

&lt;p&gt;论文中提出三点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(1). 前一篇采用capsule输出向量(pose vector)的模长作为实体出现的概率，为了保证模长小于1，采用了无原则性质的非线性操作，从而让那些活泼的capsule在路由迭代中被打压。&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;(2).计算两个capsule的一致性（相似度）时，前一篇用两向量间夹角的余弦值来衡量（即指相似程度的迭代：$b_{ij}=b_{ij}+\hat{u}_{j&lt;/td&gt;
          &lt;td&gt;i} v_j$）。与混合高斯的对数方差不同，余弦不能很好区分”好的一致性”和”非常好的一致性”。&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;(3).前一篇采用的pose是长度为$n$的向量，变换矩阵$W_{ij}$具有$n_i&lt;em&gt;n_j$个参数（如$W_{ij}=[8&lt;/em&gt;16], n_i=8,n_j=16$）。 而本文采用的带$n$个元素的矩阵作为pose，这样变换矩阵$W_{ij}$具有$n$个参数（如$n=4*4)$。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从而Hinton设计了新的Capsule的结构：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;结构&lt;/th&gt;
      &lt;th&gt;图示&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;a). 4*4的pose矩阵,表示该capsule代表的实体,对应第(3)点.&lt;br /&gt;&lt;br /&gt;b). 1个激活值,表示该capsule代表实体出现的概率,对应第(1)点.&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Capsules_matrix_7.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;且上一篇采用的是动态路由的方法完成了Capsule网络，而这儿将则采用EM路由算法。这也对应着上述第（2）、（3）点。&lt;/p&gt;

&lt;h3 id=&quot;em-routing&quot;&gt;理解EM Routing前的准备&lt;/h3&gt;
&lt;p&gt;我们先定义下标符号：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$i$: 指$L$层中的某个capsule。&lt;/li&gt;
  &lt;li&gt;$c$: 指$L+1$层中的某个capsule。&lt;/li&gt;
  &lt;li&gt;$h$: 指Pose矩阵中的某维，共16维。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了更好理解EM-Routing算法的过程，我们先理理前期思路：&lt;/p&gt;

&lt;p&gt;与前一篇方法类似，$L$层的$Cpasule_i$的输出($pose$)需要经过矩阵变换$W_{ic}=[4&lt;em&gt;4]$，得到它对$L+1$层的$Capsule_c$的$pose$的投票$V_{ic}$(维度和$pose$一样都是$[4&lt;/em&gt;4]$)之后，才能进入Routing更新。而当$h$为$4*4$中的某一维时，让$V_{ich} = pose_{ih} * W_{ich}$，这样就可以得到$V_{ic}$了，这也对应着上述(3)。&lt;/p&gt;

&lt;p&gt;换种解释说：$L$层的$Capsule_i$要投票给$L+1$层的$Capsule_c$，但是不同的$Capsule_c$可能需要不同变化的$Capsule_i$。所以对于每个$Capsule_c$，$Capsule_i$都有一个转换矩阵$W_{ic}$，$Capsule_i$转换后的$V_{ic}$就称投票值，而$V_{ich}$是指在$V_{ic}$在$h$维（一共4*4=16维）上的值。且变换矩阵$W_{ic}$是反向传播更新的。&lt;/p&gt;

&lt;p&gt;文中对每个Capsule的pose建立混合高斯模型，让pose的每一维都为一个单高斯分布。即$Capsule_c$的pose中的$h$维为一个单高斯模型，$P_{ich}$是指$pose_{ch}$的值为$V_{ich}$的概率。&lt;br /&gt;
令$\mu_{ch}$和$\sigma_{ch}^2$分别为$Capsule_c$在$h$维上的均值和方差，则：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P_{ich}=\frac{1}{\sqrt{2 \pi \sigma_{ch}^2}}{exp\big({-\frac{(V_{ich}-\mu_{ch})^2}{2\sigma_{ch}^2}}}\big)&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;ln(P_{ich})=  -\frac{(V_{ich}-\mu_{ch})^2}{2\sigma_{ch}^2} - ln(\sigma_{ch})-\frac{ln(2\pi)}{2}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;再令$r_i$为$Capsule_i$的激活值，就可以写出该$Capsule_c$在$h$维度上的损失$cost_{ch}$:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;cost_{ch}=\sum_i -r_i ln(P_{ich})=\frac{\sum_i{r_i\sigma_{ch}^2}}{2\sigma_{ch}^2} + \big(ln(\sigma_{ch})+ \frac{ln(2\pi)}{2}\big)\sum_i r_i=\big(ln(\sigma_{ch}+k)\big)\sum_i r_i&lt;/script&gt;&lt;br /&gt;
值得注意的是，这里的$\sum_i$ 并不是指$L$层的所有$capsule_i$，而是指可以投票给$Capsule_c$的$Capsule_i$，这点之后会解释。 另外式子中的$k$是一个常数，它可以通过反向传播进行更新，这个在后面也会提到。&lt;/p&gt;

&lt;p&gt;$Capsule_c$的激活值可用下面公式得出：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;a_c=logistic\big(\lambda\big(b_c-\sum_h cost_{ch}\big)\big)&lt;/script&gt;&lt;br /&gt;
其中$-b_c$代表$Capsule_c$在$h$维上的代价均值，它可以通过反向传播进行更新。而$\lambda$是温度倒数，是超参，我们可以在训练过程中逐步改变它的值。文章中的logistic函数采用sigmoid函数。&lt;/p&gt;

&lt;p&gt;好的，我们现在整理下在EM Routing要用的参数：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;参数&lt;/th&gt;
      &lt;th&gt;描述&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$Capsule_i$、$Capsule_c$&lt;/td&gt;
      &lt;td&gt;分别指为$L$层、$L+1$层的某个$Capsule$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{ic}$&lt;/td&gt;
      &lt;td&gt;$Capsule_i$投票给$capsule_c$前的变换矩阵，通过反向传播进行更新&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$V_{ic}$&lt;/td&gt;
      &lt;td&gt;$Capsule_i$经过矩阵变换后准备给$capsule_c$的投票值&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$P_{ic}$&lt;/td&gt;
      &lt;td&gt;$Capsule_i$的投票值在$Capsule_c$中的混合高斯概率&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$cost_{ch}$&lt;/td&gt;
      &lt;td&gt;$Capsule_c$在$h$维度上的损失&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\mu_{ch}$、$\sigma_{ch}^2$&lt;/td&gt;
      &lt;td&gt;Capsulec在h维上的均值和方差&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$k$&lt;/td&gt;
      &lt;td&gt;$cost_{ch}$式子中的一个常数，通过反向传播进行更新&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$a_c$&lt;/td&gt;
      &lt;td&gt;$Capsule_c$的激活值&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$-b_c$&lt;/td&gt;
      &lt;td&gt;$Capsule_c$在$h$维上的代价均值，通过反向传播进行更新&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\lambda$&lt;/td&gt;
      &lt;td&gt;温度倒数，是超参，在迭代过程中逐步增大它的值&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;###　EM Routing的流程：&lt;/p&gt;

&lt;h4 id=&quot;section-6&quot;&gt;总流程：&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/EM_ROUTING_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;梳理符号：
    &lt;ul&gt;
      &lt;li&gt;$R_{ic}$表示$Capsule_i$给$Capsule_c$投票的加权系数&lt;/li&gt;
      &lt;li&gt;$M_c$、$S_c$ 表示$Capsule_c$的Pose的期望和方差&lt;/li&gt;
      &lt;li&gt;$size(L+1)$ 表示$Capsule_i$要投票给$L+1$层的Capsule的总数，即与$Capsule_i$有关系的$Capsule_c$的总数&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;传入$L$层的所有$Capsule_i$的激活值$a$和矩阵转换后的投票值$V$，输出$L+1$层所有$Capsule_c$的激活值$a^{‘}$和Pose最终的期望值。&lt;/li&gt;
  &lt;li&gt;对投票加权系数初始化后就进行EM算法迭代（一般迭代3次）：
    &lt;ul&gt;
      &lt;li&gt;对每个$Capsule_c$，M-step得到它的Pose的期望和方差&lt;/li&gt;
      &lt;li&gt;对每个$Capsule_i$，E-step中得到它更新后的对所有$Capsule_c$投票的加权系数。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;再次提醒，这里$Capsule_i$不是投票给所有而是部分$Capsule_c$。此处的”所有“是为了表述方便。具体理由之后会解释。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;m-step&quot;&gt;分析M-Step步骤:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/EM_ROUTING_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;梳理符号：
    &lt;ul&gt;
      &lt;li&gt;$cost_h$即为$cost_{ch}$，是$Capsule_c$在$h$维度上的损失.&lt;/li&gt;
      &lt;li&gt;$\beta_v$即为$cost_{ch}$式子中的一个常数k（前面提及过），通过反向传播进行更新.&lt;/li&gt;
      &lt;li&gt;$\beta_a$即为$Capsule_c$在$h$维上的代价均值$-b_c$的负数(前面提及过)&lt;/li&gt;
      &lt;li&gt;$\lambda$即为温度倒数。是超参，在迭代中将逐步增大它的值（前面提及过），通过反向传播进行更新&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;对于某$Capsule_c$，传入所有$Capsule_i$对它的投票加权系数$R_{:c}$、所有$Capsule_i$的激活值$a$、所有$Capsule_i$矩阵转换后对它的投票值$V_{:c:}$，输出该$Capsule_c$的激活值以及pose期望方差。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;e-step&quot;&gt;分析E-Step步骤:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/EM_ROUTING_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;梳理符号：
    &lt;ul&gt;
      &lt;li&gt;$p_c$即为$P_{ic}$，是$Capsule_i$的投票值在$Capsule_c$中的混合高斯概率，前面提及过。&lt;/li&gt;
      &lt;li&gt;$r$即为$r_i$， 是$Capsule_i$给所有$Capsule_c$投票的加权系数&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;对于某$Capsule_i$， 传入它对所有$Capsule_c$的投票$V_i$、所有$Capsule_c$的激活值以及pose的均值和方差，得到它更新后的对所有$Capsule_c$投票的加权系数$r_i$。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;经过一轮的公式符号轰炸，我们就明白了EM-Routing的整体流程。&lt;/p&gt;

&lt;h3 id=&quot;matrix-capsules-&quot;&gt;Matrix Capsules 网络模型&lt;/h3&gt;

&lt;p&gt;接下来我们要看下Hinton设计的Matrix Capsule的网络模型：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/EM_ROUTING_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;由于涉及到Capsule的卷积操作，此处先定义一些概念，以ConvCaps1层为例子，在ConvCaps1层中：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;含有C个channel，每个channel含有6*6=36个capsule。&lt;/li&gt;
  &lt;li&gt;不同的channel含有不同的类型capsule，同一channel的capsule的类型相同但位置不同。&lt;/li&gt;
  &lt;li&gt;任一个capsule均有6*6-1=35个相同类型的capsule，均有C-1个位置相同的capsule。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;relu-conv1&quot;&gt;第一层ReLU Conv1的获取：&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;普通的卷积层，使用A个5×5的卷积核、步幅为2、ReLU作为激活函数，得到A×14×14的张量输出。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;primarycaps&quot;&gt;第二层PrimaryCaps的获取:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;从普通卷积层构建成Capsule结构的PrimaryCaps层，用了A×B×(16+1)个1×1的卷积核，步幅为1，得到B×14×14个pose和B&lt;em&gt;14&lt;/em&gt;14个激活值，即有B&lt;em&gt;14&lt;/em&gt;14个capsule。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;convcaps1&quot;&gt;第三层ConvCaps1的获取：&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;此处将PrimaryCaps层（即$L$层）中的$Capsule_i$的pose进行矩阵转换后，得到对应的投票矩阵$V$。将$V$和全部$Capsule_i$的激活值传入RM-Routing中，即可得到C&lt;em&gt;6&lt;/em&gt;6个$Capsule_c$的pose及激活值。&lt;/li&gt;
  &lt;li&gt;此处是Capsule卷积操作，卷积核大小为K，步幅为2。
    &lt;ul&gt;
      &lt;li&gt;对于$L+1$层的某个$Capsule_c$，需要得到投票给它的那些$Capsule_i$的投票矩阵$V_{:c}$
        &lt;ul&gt;
          &lt;li&gt;在$L$层只有K×K&lt;em&gt;B个$Capsule_i$投票给该$Capsule_c$，对这K×K&lt;/em&gt;B个$Capsule_i$的pose分别进行矩阵转换，即可得到投票矩阵$V_{ic}$&lt;/li&gt;
          &lt;li&gt;这里有K&lt;em&gt;K&lt;/em&gt;B&lt;em&gt;C个转换矩阵$W_{ic}$，每个$W_{ic}$是4&lt;/em&gt;4的矩阵（或说16维的向量）。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;这两层间转换矩阵$W_{ic}$共有：(K&lt;em&gt;K&lt;/em&gt;B)&lt;em&gt;C&lt;/em&gt;6&lt;em&gt;6&lt;/em&gt;个 （而不是14&lt;em&gt;14&lt;/em&gt;B&lt;em&gt;C&lt;/em&gt;6*6).&lt;/li&gt;
      &lt;li&gt;与普通二维卷积一样，不过卷积的乘法操作改为EM-Routing。即被卷积的是$L$层的所有$Capsule_i$的投票矩阵$V_i$和激活值，卷积结果是C&lt;em&gt;6&lt;/em&gt;6个$Capsule_c$的pose及激活值。
        &lt;ul&gt;
          &lt;li&gt;每个$L+1$层的$Caspule_c$都采用capsule卷积（EM-Routing）对应$L$层的K&lt;em&gt;K&lt;/em&gt;B个$Capsule_i$，从而得到该$Caspule_c$的pose和激活值。&lt;/li&gt;
          &lt;li&gt;对于$L$层的中心位置的B个$Capsule_i$，它们每个$Capsule_i$，都只投票给卷积时卷积核滑过它们的对应的$Capsule_c$（共K&lt;em&gt;K&lt;/em&gt;C个）。而$L$层的边缘位置的每个$Capsule_i$投票给$L+1$层的$Capsule_c$个数将小于K&lt;em&gt;K&lt;/em&gt;C个。如$L$层最左上位置的B个$Capsule_i$，它们只能投给$L+1$层最左上角的C个$Capsule_c$（只有$L+1$层的这个位置执行卷积时候卷积核才滑过$L$层最左上角）&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;convcaps2&quot;&gt;第四层ConvCaps2的获取：&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;capsule卷积层，与ConvCaps1一样的操作。&lt;/li&gt;
  &lt;li&gt;采用的卷积核K=3，步幅=1 。得到D&lt;em&gt;4&lt;/em&gt;4个capsule的pose与激活值。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;class-capsules&quot;&gt;第五层Class Capsules的获取：&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;从capsule卷积层到最后一层的操作，和前面的做法不同。
    &lt;ul&gt;
      &lt;li&gt;相同类型（即同一channels）的capsule共享转换矩阵，所以两层间共有D&lt;em&gt;E个转换矩阵$W_j$，每个$W_j$是4&lt;/em&gt;4的矩阵（或说16维的向量）。&lt;/li&gt;
      &lt;li&gt;拉伸后的扁长层（Class Capsules层）无法表达capsule的位置信息，而前面ConvCaps2层的每个$capsule_i$都有对应的位置信息。为了防止位置信息的丢失，作者将每个$Capsule_i$的位置信息（即坐标）分别加到它们的投票矩阵$V_ij$的一二维上。随着训练学习，共享转换矩阵$W_j$能将$V_ij$的一二维与$Capsule_i$的位置信息联系起来，从而让Class Capsules层的$Capsule_j$的pose的一二维携带位置信息。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;得到E个capsule， 每个capsule的pose表示对应calss实体，而激活值表示该实体存在的概率。&lt;/li&gt;
  &lt;li&gt;这样就可以单独拿出capsule的激活值做概率预测，拿capsule的pose做类别实体重构了。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;整体的Matrix Capsules网络模型就梳理完成了。现在还剩下损失函数了。&lt;/p&gt;

&lt;h3 id=&quot;spread-loss&quot;&gt;损失函数：传播损失(Spread Loss):&lt;/h3&gt;

&lt;p&gt;为了使训练过程对模型的初始化以及超参的设定没那么敏感，文中采用传播损失函数来最大化被激活的目标类与被激活的非目标类之间的概率差距。a_t表示target的激活值，a_i表示Class_Capsules中除t外第i个的激活值：&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;L_i=\big(\max \big(0,m-(a_t-a_i)\big)\big)^2 , \quad  L=\sum_{i\neq t} L_i&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;m将从0.2的小幅度开始，在训练期间将其线性增加到0.9，避免无用胶囊的存在。那为什么要这样做呢？ &lt;br /&gt;
小编的理解是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当模型训练得挺好时候（中后期），每个激活值a_i的比较小而a_t比较大。此时m需要设置为接近1的数值。设置m为0.9而不是1，这是为了防止分类器过度自信，是要让分类器更专注整体的分类误差。&lt;/li&gt;
  &lt;li&gt;当模型初步训练时候，很多capsules起的作用不大，最终的激活值a_i和a_t相差不大，若此时m采用较大值如0.9,就会掩盖了(a_t-a_i)在参数更新的作用，而让m主导了参数更新。
    &lt;ul&gt;
      &lt;li&gt;比如两份参数W1和W2对同样的样本得到的$a_t-a_i$值有：$W1_{a_t-a_i} &amp;lt; W2_{a_t-a_i}$ ，那显然W2参数优于W1参数，即W1参数应该得到较大幅度的更新。但由于处于模型初步阶段，$W_{a_t-a_i}$值很小，若此时m较大，则m值主导了整体loss。换句话说，m太大会导致W1和W2参数更新的幅度相近，因为$a_t-a_i$被忽略了。&lt;/li&gt;
      &lt;li&gt;不过参数的更新幅度取决于对应的导数，由于此处的spread loss含有平方，所以m值的设置会关系到参数的导数，从而影响到参数更新的幅度 （有些loss由于公式设计问题会导致从loss看不出参数更新的幅度，如若此处将Spread loss的平方去掉，参数的更新就和m无关了）。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-7&quot;&gt;实验结果&lt;/h3&gt;

&lt;p&gt;作者采用smallNORB数据集（具有5类玩具的灰度立体图片：飞机、汽车、卡车、人类和动物）上进行了实验。选择smallNORB作为capsule网络的基准，因为它经过精心设计，是一种纯粹的形状识别任务，不受上下文和颜色的混淆，但比MNIST更接近自然图像。下图为在不同视角上的smallNORB物体图像：&lt;br /&gt;
&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/EM_ROUTING_6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;该smallNORB数据集，CNN中的基准线是：测试集错误率5.2%，参数总量4.2M。而作者使用小型的capsule网络（A=64, B = 8, C = D = 16，参数总量68K），达到2.2%的测试集错误率，这也击败了当前最好的记录。&lt;/p&gt;

&lt;p&gt;作者还实验了其他数据集。采用上图的Matrix Capsules网络及参数，在MNIST上就达到了0.44％的测试集错误率。如果让A=256，那在Cifar10上的测试集错误率将达到11.9％.&lt;/p&gt;

&lt;p&gt;文章后面还讨论了在对抗样本上，capsule模型和传统卷积模型的性能。实验发现，在白箱对抗攻击时，capsule模型比传统的卷积模型更能抵御攻击。而在黑箱对抗攻击时，两种模型差别不大。感兴趣的话可以看看论文中对这部分实验的设置及分析。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-8&quot;&gt;参考链接：&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.09829&quot;&gt;Dynamic Routing Between Capsules&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=HJWLfGWRb&quot;&gt;Matrix Capsules with EM routing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/29435406&quot;&gt;浅析 Hinton 最近提出的 Capsule 计划&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kndrck.co/posts/capsule_networks_explained/&quot;&gt;Capsule Networks Explained&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jiqizhixin.com/articles/2017-11-05&quot;&gt;先读懂CapsNet架构然后用TensorFlow实现：全面解析Hinton的提出的Capsule&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jiqizhixin.com/articles/capsule-implement-sara-sabour-Feb02&quot;&gt;Capsule官方代码开源之后，机器之心做了份核心代码解读&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/30970675&quot;&gt;CapsulesNet 的解析及整理&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jhui.github.io/2017/11/14/Matrix-Capsules-with-EM-routing-Capsule-Network/&quot;&gt;Understanding Matrix capsules with EM Routing (Based on Hinton’s Capsule Networks)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Sarasra/models/tree/master/research/capsules&quot;&gt;Dynamic Routing官方源代码-Tensorflow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/naturomics/CapsNet-Tensorflow&quot;&gt;Dynamic Routing:HuadongLiao的CapsNet-Tensorflow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/XifengGuo/CapsNet-Keras&quot;&gt;Dynamic Routing:Xifeng Guo的CapsNet-Keras&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Thu, 01 Mar 2018 00:00:00 +0800</pubDate>
        <link>http://luonango.github.io/2018/03/01/CapsulesNet/</link>
        <guid isPermaLink="true">http://luonango.github.io/2018/03/01/CapsulesNet/</guid>
        
        <category>CNN</category>
        
        <category>图像分类</category>
        
        <category>Capsule网络</category>
        
        
      </item>
    
      <item>
        <title>杂乱的几篇论文阅读笔记</title>
        <description>&lt;h1 id=&quot;section&quot;&gt;随便记录一些论文&lt;/h1&gt;

&lt;h2 id=&quot;section-1&quot;&gt;模型压缩加快&lt;/h2&gt;

&lt;h3 id=&quot;deep-compression2016deep-compression-compression-deep-neural-networks-with-pruning-trained-quantization-and-huffman-coding&quot;&gt;1. 模型压缩Deep Compression：2016《Deep Compression: Compression Deep Neural Networks With Pruning, Trained Quantization And Huffman Coding》&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Han S, Mao H, Dally W J.
2016 ICLR最佳论文
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;参考：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/may0324/article/details/52935869&quot;&gt;Deep Compression阅读理解及Caffe源码修改&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Deep Compreession 实现有三步：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Prunning（权值修剪）、Quantization（权值共享和量化）、Huffman Encoding（Huffman编码)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Prunning（权值修剪）:
    &lt;ul&gt;
      &lt;li&gt;1.正常训练模型得到权值&lt;/li&gt;
      &lt;li&gt;2.将低于阈值的权值设为0&lt;/li&gt;
      &lt;li&gt;3.重新训练得到的新权值&lt;/li&gt;
      &lt;li&gt;权值修剪后的稀疏网络用CSC(compressed sparse column)或者CSR(compressed sparse row)表示。&lt;/li&gt;
      &lt;li&gt;用下标法记录权值参数:记录非零值和其在数组的下标，并用差分形式表示下标（记录和上一个数值的位置距离，如果4bits不够表示距离，则填1111，并把那个非零值记为0.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Quantization（权值共享和量化）：
    &lt;ul&gt;
      &lt;li&gt;K-mean方法吧所有参数聚成$2^n$个类，聚类初始化有：&lt;/li&gt;
      &lt;li&gt;同一聚类的参数相等，对网络调优。梯度下降时候，归于同一类的节点的梯度相加，施加到该类的聚类中心上（而不是每个节点参数上）&lt;/li&gt;
      &lt;li&gt;使用n比特编码的聚类中心替代原有参数 &lt;br /&gt;
&amp;gt; * 随机初始化： 随机产生K个观察值做为聚类中心。&lt;br /&gt;
&amp;gt; * 密度分布初始化： 现将累计概率密度CDF的y值分布线性划分，然后根据每个划分点的y值找到与CDF曲线的交点，再找到该交点对应的x轴坐标，将其作为初始聚类中心。 &lt;br /&gt;
&amp;gt; * 线性初始化： 将原始数据的最小值到最大值之间的线性划分作为初始聚类中心&lt;br /&gt;
&amp;gt; * 而 &lt;strong&gt;线性初始化方式&lt;/strong&gt;则能更好地保留大权值中心，因此文中采用这一方式。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Huffman Encoding（Huffman编码):
    &lt;ul&gt;
      &lt;li&gt;编码为n位的非零数据取值；&lt;/li&gt;
      &lt;li&gt;编码为4位的非零元素下标。&lt;/li&gt;
      &lt;li&gt;这两者的分布都不均匀，可以使用Huffman编码进一步压缩存储&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;squeezenet2016squeezenet--alexnet-level-accuracy-with-50x-fewer-parameters-and-05mb-model-size&quot;&gt;2. 模型压缩SqueezeNet：2016《SqueezeNet- AlexNet-level accuracy with 50x fewer parameters and 0.5MB model size》&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Iandola F N, Han S, Moskewicz M W, et al.
UC Berkeley和Stanford
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;参考：&lt;br /&gt;
- &lt;a href=&quot;https://www.jianshu.com/p/8e269451795d&quot;&gt;神经网络瘦身：SqueezeNet&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;用了Deep Compression的方法（上一篇）&lt;/li&gt;
  &lt;li&gt;“多层小卷积核”策略成功的根源：
    &lt;ul&gt;
      &lt;li&gt;内存读取耗时要远大于计算耗时。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Dense→Sparse→Dense 的SDS训练法:
    &lt;ul&gt;
      &lt;li&gt;本文还有一个神奇的发现：使用裁剪之后的模型为初始值，再次进行训练调优所有参数，正确率能够提升4.3%。&lt;/li&gt;
      &lt;li&gt;稀疏相当于一种正则化，有机会把解从局部极小中解放出来。这种方法称为DSD(dense-&amp;gt;sparse-&amp;gt;dense)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;igcinterleaved-group-convolutioninterleaved-group-convolutions-for-deep-neural-networks&quot;&gt;3. 交错组卷积（IGC，Interleaved Group Convolution）《Interleaved Group Convolutions for Deep Neural Networks》&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;微软 ICCV 2017
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;参考：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;amp;mid=2649441412&amp;amp;idx=1&amp;amp;sn=76b8e24616a4cdc07fbf985798ef4942&amp;amp;chksm=82c0ad00b5b724163561a9174f8213d365ca87f5d73c7ec278ac358293b55e7dcb9e488b1eb8&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=0731zmN8ulFwvXY4HRh8vpKs#rd&quot;&gt;王井东详解ICCV 2017入选论文：通用卷积神经网络交错组卷积&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从通道角度出发，解决了神经网络基本卷积单元中的冗余问题。可以在无损性能的前提下，缩减模型、提升计算速度，有助于深度网络在移动端的部署.&lt;/p&gt;

&lt;p&gt;每个交错组卷积模块包括两个组卷积过程。不同组的卷积不存在交互，不同组的输出通道并不相关。&lt;/p&gt;

&lt;p&gt;为了让不同组的通道相关联，引入第二次组卷积：第二次的每组的输入通道均来自第一组输出的不同的组的通道（交错组成）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Interleaved_Group_Convolutions_for_Deep_Neural_Networks_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Xception可以看成是特例：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果第一次组的每组只有一个输出通道，那么就变成了特殊的组卷积，即 channel-wise convolution，第二次组卷积成为 1X1 的卷积，这与 Xception 相似；&lt;/li&gt;
  &lt;li&gt;如果第一次组卷积过程里仅有一组，那么这个过程就变成了普通卷积，第二次组卷积过程则相当于分配给每个通过一个不同的权重。&lt;/li&gt;
  &lt;li&gt;极端情况下:网络的性能会随着通道数及组数的变化而变化，最优性能配置点存在于两个极端情况之间。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;shufflenetshufflenet-an-extremely-efficient-convolutional-neural-network-for-mobile-devices-&quot;&gt;4. 通道随机分组ShuffleNet：《ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices 》&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Face++ 2017 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;参考：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://m.blog.csdn.net/u014380165/article/details/75137111&quot;&gt;ShuffleNet算法详解&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/28749411&quot;&gt;变形卷积核、可分离卷积？卷积神经网络中十大拍案叫绝的操作&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;降低深度网络计算量。&lt;/p&gt;

&lt;p&gt;采用&lt;strong&gt;Channel-Shuffle、Pointwise和Depthwise&lt;/strong&gt;来修改原来的ResNet单元.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Channel Shuffle:&lt;br /&gt;
&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/ShuffleNet_An_Extremely_Efficient_Convolutional_Neural_Network_for_Mobile_Devices_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;pointwise group convolution：
    &lt;ul&gt;
      &lt;li&gt;带组的卷积核为1*1的卷积。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;depthwise separable convolution，将传统卷积过程分两步：
    &lt;ul&gt;
      &lt;li&gt;如先用M个3*3卷积核一对一卷积成M个特征图。卷积时候互不相交。&lt;/li&gt;
      &lt;li&gt;接着用N个1*1的卷积核对这M个特征图全部卷积（正常卷积），生成N个特征图。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;ShuffleNet的核心:&lt;/p&gt;

  &lt;p&gt;用pointwise group convolution，channel shuffle和depthwise separable convolution,代替ResNet block的相应层构成了ShuffleNet uint，达到了减少计算量和提高准确率的目的。&lt;/p&gt;

  &lt;p&gt;channel shuffle解决了多个group convolution叠加出现的边界效应，pointwise group convolution和depthwise separable convolution主要减少了计算量&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;增强了全局信息的流通，超少量的参数。&lt;/p&gt;

&lt;p&gt;相比Xception和Mobilenet，ShuffleNet都获得了性能上的提升，至于速度带来的提升则不是很明显。&lt;br /&gt;
以及超越mobilenet、媲美AlexNet的准确率.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;微软亚洲研究院MSRA最近也有类似的工作，他们提出了一个IGC单元（Interleaved Group Convolution,如上），即通用卷积神经网络交错组卷积，形式上类似进行了两次组卷积。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;要注意的是，Group conv是一种channel分组的方式，Depthwise +Pointwise是卷积的方式，只是ShuffleNet里面把两者应用起来了。&lt;/p&gt;

  &lt;p&gt;因此Group conv和Depthwise +Pointwise并不能划等号。&lt;/p&gt;

&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;senet-2017squeeze-and-excitation-networks&quot;&gt;5. SENet 2017《Squeeze-and-Excitation Networks》&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2017 ImageNet 冠军
CVPR，国内自动驾驶创业公司Momenta
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;section-2&quot;&gt;特点：&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;考虑特征通道间的关系，动态调整各通道的特征响应值&lt;/li&gt;
  &lt;li&gt;构造简单、容易被部署。增加很小的计算消耗，获得高的性能提升&lt;/li&gt;
  &lt;li&gt;可能用于辅助网络修剪/压缩的工作&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-3&quot;&gt;模块构造&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Squeeze-and-Excitation 模块&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Squeeze-and-Excitation_Networks_1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;图中输入是X，有$c_1$特征通道。经过一系列卷积变换后，得$c_2$个大小为$w×h$的通道（map）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Squeeze&lt;/strong&gt;： 顺着空间维度进行特征压缩，每个二维特征通道变成一个实数，如图得到$1×1×c_2$.表征着特征通道相应的全局分布.全局平均池化来生成各通道的统计量&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Excitation&lt;/strong&gt;: 考察各通道的依赖程度,W被学习表示特征通道间的相关性。最后sigmoid的输出就是各通道的权重，根据输入数据调节各通道特征的权重，有助于增强特征的可分辨性&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Reweight&lt;/strong&gt;: 通过乘法逐通道加权到先前特征，完成对原始特征的重标定。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;section-4&quot;&gt;示例：部署简单，如插件&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;SE_Inception图示&lt;/th&gt;
      &lt;th&gt;SE_ResNet图示&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Squeeze-and-Excitation_Networks_4.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Squeeze-and-Excitation_Networks_5.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;####　实验情况：增加小的计算消耗，获得大的性能提升&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;.&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Squeeze-and-Excitation_Networks_3.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Squeeze-and-Excitation_Networks_6.jpg&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;
&lt;p&gt;## 模型变换&lt;/p&gt;

&lt;h3 id=&quot;deformable-convolutional-networks&quot;&gt;1. 可变形卷积网络2017《Deformable Convolutional Networks》&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;微软亚洲研究院.可变卷积和可变ROI采样
Dai J, Qi H, Xiong Y, et al. 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;参考：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/msracver/Deformable-ConvNets&quot;&gt;https://github.com/msracver/Deformable-ConvNets&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://valser.org/thread-1261-1-1.html&quot;&gt;【VALSE 前沿技术选介17-02期】可形变的神经网络&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cnblogs.com/lillylin/p/6277094.html&quot;&gt;目标检测方法——R-FCN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://m.blog.csdn.net/yucicheung/article/details/78113843&quot;&gt;Deformable ConvNets论文笔记&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对卷积核每个采样点位置都增加偏移变量。用convolution去学习dilatation值，和STN异曲同工。&lt;/p&gt;

&lt;p&gt;引入了两种模块：&lt;strong&gt;deformable convolution&lt;/strong&gt; 、 &lt;strong&gt;deformable RoI pooling&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;对象尺度，姿态，视点和部分形变的几何变换一般有两种方法：&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;建立足够期望变化的训练数据集，通常增加数据样本，如仿射变换等，从数据集中学习鲁棒表示。但需要昂贵的训练和复杂的模型参数&lt;/li&gt;
    &lt;li&gt;使用变换不变的特征和算法，如SIFT（尺度不变特征变换）、基于滑动窗口的检测范例。&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;上述的几何变换是固定已知的。这样的先验知识的假设阻止了未知的几何变换，并且，人工设计的特征和算法对于过度复杂的变换是困难不可行的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Deformable Convolution:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;将2D偏移量加入标准卷积中的网格区，使网格区自由形变（再进行卷积）。偏移量通过附加的卷积层从前面的特征图中学习。因此，变形以局部的，密集的和自适应的方式受到输入特征的限制。&lt;/li&gt;
      &lt;li&gt;
        &lt;table&gt;
          &lt;thead&gt;
            &lt;tr&gt;
              &lt;th&gt;.&lt;/th&gt;
              &lt;th&gt;.&lt;/th&gt;
            &lt;/tr&gt;
          &lt;/thead&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td&gt;(a)是标准卷积的采样网格。&lt;br /&gt;(b)是变形后采样位置和变形卷积中的偏移量箭头&lt;br /&gt;(c)和(d)是形变后的特殊情况。&lt;br /&gt;&lt;br /&gt;表明可变形卷积推广了各种尺度(各向异性)纵横比和旋转的变换&lt;/td&gt;
              &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Deformable_Convolutional_Networks_1.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;table&gt;
          &lt;thead&gt;
            &lt;tr&gt;
              &lt;th&gt;.&lt;/th&gt;
              &lt;th&gt;.&lt;/th&gt;
            &lt;/tr&gt;
          &lt;/thead&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td&gt;在相同的输入特征图上应用卷积层，获得偏移。2N的通道表示N个通道的2维偏移。&lt;br /&gt;&lt;br /&gt;训练时，同时学习偏移量的卷积核参数以及普通卷积核参&lt;/td&gt;
              &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Deformable_Convolutional_Networks_2.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deformable RoI(region-of-interest) pooling&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;为RoI（region-of-interest）池的常规bin分区中的每个bin位置添加一个偏移量。（池化时，池化区的每个位置都加入对应偏移量，再进行池化）。&lt;br /&gt;类似地，从前面的特征映射和RoI学习偏移，使得具有不同形状的对象的自适应部分定位成为可能。&lt;/li&gt;
      &lt;li&gt;
        &lt;table&gt;
          &lt;thead&gt;
            &lt;tr&gt;
              &lt;th&gt;.&lt;/th&gt;
              &lt;th&gt;.&lt;/th&gt;
            &lt;/tr&gt;
          &lt;/thead&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td&gt;fc层产生归一化偏移量，再归一化偏移量（让偏移的学习不因RoI尺寸不同而变化）&lt;/td&gt;
              &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Deformable_Convolutional_Networks_3.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Position-Sensitive (PS) RoI Pooling&lt;/strong&gt; （目标检测方法R-FCN提及到）
    &lt;ul&gt;
      &lt;li&gt;位置敏感RoI池是完全卷积的,&lt;a href=&quot;https://www.cnblogs.com/lillylin/p/6277094.html&quot;&gt;目标检测R-FCN中率先提出position sensitive score map概念&lt;/a&gt;。&lt;/li&gt;
      &lt;li&gt;
        &lt;table&gt;
          &lt;thead&gt;
            &lt;tr&gt;
              &lt;th&gt;deformable PS RoI pooling&lt;/th&gt;
            &lt;/tr&gt;
          &lt;/thead&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td&gt;&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Deformable_Convolutional_Networks_4.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
              &lt;td&gt;下面分支：通过一个卷积层，所有的 input 特征图先被转化。每个物体类别都生成$k^2$个分数图（如果物体类别是C，那么总共有C+1个,C类物体外加1个背景）&lt;br /&gt;&lt;br /&gt;上面分支：通过卷积层，生成完全的空间像素偏移域。&lt;br /&gt;&lt;br /&gt;对每个RoI（同时还指定了某个类别），PS RoI pooling会被应用在这些域上以获取归一化的偏移量$\Delta \hat{p}&lt;em&gt;{ij}$，然后通过上述deformable RoI pooling中一样的方法变成真正的偏移量$\Delta p&lt;/em&gt;{ij}$。&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;梯度是通过双线性运算反向传播，详情阅读论文公式及附录。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-5&quot;&gt;集成方面：&lt;/h2&gt;

&lt;h3 id=&quot;section-6&quot;&gt;1. 数据上的集成：&lt;/h3&gt;

&lt;h4 id=&quot;data-distillation-towards-omni-supervised-learning&quot;&gt;对图片转换，半监督《Data Distillation: Towards Omni-Supervised Learning》&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Ilija Radosavovic Piotr Dollar Ross Girshick Georgia Gkioxari Kaiming He 
Facebook AI Research (FAIR)
人体姿态估计   物体识别框
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;一些参考：&lt;br /&gt;
  &lt;a href=&quot;https://www.zhihu.com/question/264009268/answer/275800268&quot;&gt;如何评价FAIR的最新工作Data Distillation？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;思想主要如图：&lt;br /&gt;
&lt;img src=&quot;https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Data_Distillation_Towards_Omni-Supervised_Learning_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;数据变换：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;选择多变换推理的几何变换（使用&lt;strong&gt;缩放和水平翻转&lt;/strong&gt;）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;在未标记的数据上生成标签（聚合多变换推断（Multi-transform inference）的结果）：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;将输入的数据进行多种变换，都应用于相同的模型，然后汇总结果。&lt;/li&gt;
  &lt;li&gt;某数据可能某单个变换比任何其他的变换得到的模型预测更好。
    &lt;ul&gt;
      &lt;li&gt;观察发现，聚合预测产生新的知识，使用这些信息生成的标签，原则上能让模型进行学习。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;生成的标签可能是 &lt;strong&gt;“软标签”&lt;/strong&gt;（即是概率向量，而非分类标签），重新训练时候如果考虑到概率值，则要重新设计损失函数。且对于结构化输出空间的问题，如对象检测或人体姿态估计，&lt;strong&gt;平均输出没有意义&lt;/strong&gt;。&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;故本文&lt;strong&gt;只聚合那些和真实标签有相同结构、类型（或分布）的“硬标签”&lt;/strong&gt;：
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;额外设计专用逻辑结构&lt;/strong&gt;来处理，如非极大值抑制来组合多组方框之类的做法。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;期望预测的框和关键点足够可靠，可以生成良好的培训标签：
        &lt;ul&gt;
          &lt;li&gt;以预测的检测分数作为预测质量的评估，&lt;strong&gt;超过某个分数阈值的预测才能产生注释&lt;/strong&gt;。&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;专用逻辑结构&lt;/strong&gt;：(作者自己设计)发现如果使得“每个未标记图像的预测标注实例的平均数量”大致等于“每个真实标记图像的标注实例的平均数量”，则分数阈值运行良好。虽然这种启发式假定未标记和标记的图像遵循&lt;strong&gt;类似的分布&lt;/strong&gt;，但是作者发现它是稳健的并且即使在假设不成立的情况下也工作良好。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;看法：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;从数据而非模型来思考集成&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;不知道 &lt;strong&gt;数据转换是静态转换，还是可学习的转换（学习转换矩阵Ｗ)&lt;/strong&gt;，文中似乎是前者，但后者或许更佳。&lt;/li&gt;
  &lt;li&gt;感觉是作者添加的&lt;strong&gt;专用逻辑结构起了很大作用&lt;/strong&gt;。即&lt;strong&gt;添加了发现的规律&lt;/strong&gt;，增加了额外有用信息（先验知识），从而督促了网络学习吧。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;　发现如果“每个未标记图像的预测标注实例的平均数量”大致等于“每个真实标记图像的标注实例的平均数量”，则分数阈值运行良好.&lt;/p&gt;
&lt;/blockquote&gt;

</description>
        <pubDate>Mon, 01 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://luonango.github.io/2018/01/01/%E6%9D%82%E4%B9%B1%E7%9A%84%E5%87%A0%E7%AF%87%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
        <guid isPermaLink="true">http://luonango.github.io/2018/01/01/%E6%9D%82%E4%B9%B1%E7%9A%84%E5%87%A0%E7%AF%87%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
        
        <category>CNN</category>
        
        <category>网络结构</category>
        
        <category>模型压缩</category>
        
        <category>集成</category>
        
        
      </item>
    
      <item>
        <title>表情识别_一些整理</title>
        <description>&lt;h1 id=&quot;section&quot;&gt;表情识别_一些整理&lt;/h1&gt;

&lt;p&gt;记录表情识别的一些些进展。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/24483573&quot;&gt;什么是人脸表情识别技术？&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;&lt;strong&gt;基于特征的面部表情识别&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;置信点集的几何位置&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;这些点的多尺度多方向Gabor小波系数&lt;/p&gt;

    &lt;p&gt;二者既可以独立使用也可以结合使用。张正友博士的研究结果表明，Gabor小波系数更为有效&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;面部表情识别难点&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;不同人表情变化&lt;/li&gt;
  &lt;li&gt;同一人上下文变化&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;自动FER系统需要解决&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;面部检测和定位&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;人脸特征提取&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;表情识别&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;面部检测和定位&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;定位问题前人已经做得很好.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;联合级联检测与校准（the joint cascade detection and alignment (JDA) detector）&lt;/li&gt;
  &lt;li&gt;基于深度卷积神经网络（DCNN）；&lt;/li&gt;
  &lt;li&gt;混合树（Mot）。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;人脸特征提取&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;为了找到人脸最合适的表示方式，从而便于识别&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;整体模板匹配系统&lt;/strong&gt; :
    &lt;ul&gt;
      &lt;li&gt;模板可以是像素点或是向量&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;基于几何特征系统&lt;/strong&gt; :
    &lt;ul&gt;
      &lt;li&gt;广泛采用主成份分析和多层神经网络来获取人脸的低维表示，并在图片中检测到主要的特征点和主要部分。通过特征点的距离和主要部分的相对尺寸得到特征向量&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基于特征的方法比基于模板的方法计算量更大，但是对尺度、大小、头部方向、面部位置不敏感。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;①首先定位一系列特征点：
②再通过图像卷积抽取特征点的Gabor小波系数，以Gabor特征的匹配距离作为相似度的度量标准。在特征点：
③提取特征之后，表情识别就成为了一个传统的分类问题。可以通过多层神经网络来解决： 
准则是最小化交叉熵（Cross-entropy）：
t是label，y是实际输出。

从结果看，Gabor方法优于几何方法，二者结合效果更佳
[链接](https://zhuanlan.zhihu.com/p/24552881)
可以看到，隐含层单元达到5-7个时，识别率已经趋于稳定，那就是说5-7个单元已经足够了。
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;SIFT特征描述算子、SURF特征描述算子、ORB特征描述算子、HOG特征描述、LBP特征描述以及Harr特征描述。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;人脸图像处理&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;去掉无关噪声，统一人脸图片尺寸。&lt;/li&gt;
  &lt;li&gt;转为灰度图、标准直接方图均衡化。去掉不平衡光照&lt;/li&gt;
  &lt;li&gt;化为0均值，单位方差向量。（或者归一化到[-1,1])&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;网络模型&lt;/strong&gt;&lt;br /&gt;
基本网络模型：  略。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;加入随机扰动： 增加对脸部偏移和旋转的鲁棒性。 如随机仿射扭曲图像。&lt;/li&gt;
  &lt;li&gt;扰动下的learning与voting： 由于数据有扰动，损失函数应当包含所有扰动情况。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;多网络学习&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;模型顶端放置多网络（Multiple Network）增强性能。&lt;/p&gt;

    &lt;p&gt;典型的就是对输出求均值。观察表明，随机初始化不仅导致网络参数变化，同时使得不同网络对不同数据的分类能力产生差别。因此，平均权重可能是次最优的因为voting没有变化。更好的方法是对每个网络适应地分配权重，使得整体网络互补。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;为了学习权重W，先独立训练不同初始化的CNN，在权重上轻易损失函数：&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;最优整体对数似然损失&lt;/li&gt;
      &lt;li&gt;最有整体合页损失&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;hr /&gt;

&lt;p&gt;###衡量目标检测器：&lt;br /&gt;
  - 检测框，一般为IOU（交并比）&lt;br /&gt;
  - 分类指标：Precision,Accuracy,Recall,Miss Rate,,FPPI(False Positive per Image)&lt;/p&gt;

&lt;p&gt;###设计检测器&lt;br /&gt;
窗口–&amp;gt; 特征提取–&amp;gt;分类–&amp;gt;检测结果&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;窗口&lt;/th&gt;
      &lt;th&gt;特征提取&lt;/th&gt;
      &lt;th&gt;分类&lt;/th&gt;
      &lt;th&gt;检测结果&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;silding window&lt;/td&gt;
      &lt;td&gt;Harr&lt;/td&gt;
      &lt;td&gt;SVM&lt;/td&gt;
      &lt;td&gt;NMS&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MCG&lt;/td&gt;
      &lt;td&gt;HOG&lt;/td&gt;
      &lt;td&gt;Softmax&lt;/td&gt;
      &lt;td&gt;边框回归&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Selective Search&lt;/td&gt;
      &lt;td&gt;LBP&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;窗口合并&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RPN&lt;/td&gt;
      &lt;td&gt;CNN&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. silding window: 滑动窗口
2. MCG ：组合聚合(Multiscale Combinatorial Grouping，MCG)方法，传统方法的巅峰之作。
3. Selective Search: RCNN使用的ROI提取方法
4. RPN：Region Proposal Networks, Faster R-CNN则直接利用RPN网络来计算候选框
5. NMS: Non-maximum suppression,非极大抑制
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;section-2&quot;&gt;** 图像&amp;amp;&amp;amp;表情分类&amp;amp;&amp;amp;人脸识别**&lt;/h1&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;lbp&quot;&gt;1994-特征提取之&lt;strong&gt;LBP&lt;/strong&gt;（局部二值模式）&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;参考：
    &lt;ul&gt;
      &lt;li&gt;T. Ojala, M. Pietikäinen, and D. Harwood (1994), “Performance evaluation of texture measures with classification based on Kullback discrimination of distributions”, Proceedings of the 12th IAPR International Conference on Pattern Recognition (ICPR 1994), vol. 1, pp. 582 - 585.&lt;/li&gt;
      &lt;li&gt;T. Ojala, M. Pietikäinen, and D. Harwood (1996), “A Comparative Study of Texture Measures with Classification Based on Feature Distributions”, Pattern Recognition, vol. 29, pp. 51-59.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/songzitea/article/details/17686135&quot;&gt;LPB特征分析&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;描述图像局部纹理特征的算子,具有旋转不变性和灰度不变性等显著的优点,纹理分类上能提取强大的特征。&lt;/li&gt;
  &lt;li&gt;感觉卷积结构已经很好包括了。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;adaboostviola-jones&quot;&gt;2001-基于&lt;strong&gt;Adaboost和级联&lt;/strong&gt;的人脸检测器，称&lt;strong&gt;Viola-Jones检测器&lt;/strong&gt;：简单特征的优化级联在快速目标检测中的应用.&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;参考：
    &lt;ul&gt;
      &lt;li&gt;Viola P, Jones M. Rapid object detection using a boosted cascade of simple features[C]// Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on. IEEE, 2003:I-511-I-518 vol.1.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/wjgaas/p/3618557.html#commentform&quot;&gt;Viola–Jones object detection framework–Rapid Object Detection using a Boosted Cascade of Simple Features中文翻译 及 matlab实现(见文末链接)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;一个视觉目标检测的机器学习法，能快速地处理图像而且能实现高检测速率。这项工作可分为三个创新性研究成果。
    &lt;ul&gt;
      &lt;li&gt;第一个是一种新的图像表征说明，称为&lt;strong&gt;积分图&lt;/strong&gt;，它允许我们的检测的特征得以很快地计算出来。&lt;/li&gt;
      &lt;li&gt;第二个是一个学习算法，基于&lt;strong&gt;Adaboost自适应增强法&lt;/strong&gt;，可以从一些更大的设置和产量极为有效的分类器中选择出几个关键的视觉特征。&lt;/li&gt;
      &lt;li&gt;第三个成果是一个方法：用一个&lt;strong&gt;“级联”的形式不断合并分类器&lt;/strong&gt;，这样便允许图像的背景区域被很快丢弃,从而将更多的计算放在可能是目标的区域上。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;这个级联可以视作一个目标特定的注意力集中机制，它不像以前的途径提供统计保障，保证舍掉的地区不太可能包含感兴趣的对象。在人脸检测领域，此系统的检测率比得上之前系统的最佳值。在实时监测的应用中，探测器以每秒15帧速度运行，不采用帧差值或肤色检测的方法。&lt;/li&gt;
  &lt;li&gt;称为Viola-Jones检测器&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;opencvhaarhaar-like&quot;&gt;2002-opencv的“Haar分类器”：用对角特征&lt;strong&gt;Haar-like特征&lt;/strong&gt;对检测器进行扩展。&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;参考：
    &lt;ul&gt;
      &lt;li&gt;Lienhart R, Maydt J. An extended set of Haar-like features for rapid object detection[C]// International Conference on Image Processing. 2002. Proceedings. IEEE, 2002:I-900-I-903 vol.1.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/ello/archive/2012/04/28/2475419.html#!comments&quot;&gt;浅析人脸检测之Haar分类器方法&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Haar分类器 =  Haar-like特征 + 积分图方法 + AdaBoost + 级联&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Haar分类器用到了Boosting算法中的AdaBoost算法，只是把AdaBoost算法训练出的强分类器进行了级联，并且在底层的特征提取中采用了高效率的矩形特征和积分图方法.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;#### 2010年前-DPM、LSVM目标检测  &lt;br /&gt;
  - 参考：&lt;br /&gt;
      + &lt;a href=&quot;http://blog.csdn.net/masibuaa/article/details/17924671&quot;&gt;使用判别训练的部件模型进行目标检测 Object Detection with Discriminatively Trained Part Based Models&lt;/a&gt;&lt;br /&gt;
      + &lt;a href=&quot;http://blog.csdn.net/masibuaa/article/details/17533419&quot;&gt;判别训练的多尺度可变形部件模型 A Discriminatively Trained, Multiscale, Deformable Part Model&lt;/a&gt;&lt;br /&gt;
      + &lt;a href=&quot;http://blog.csdn.net/holybin/article/details/28292991&quot;&gt;目标检测之LatentSVM和可变形部件模型（Deformable Part Model，DPM）&lt;/a&gt;&lt;br /&gt;
      + &lt;a href=&quot;http://masikkk.com/article/DPM-model-explanation/&quot;&gt;有关可变形部件模型(Deformable Part Model)的一些说明&lt;/a&gt;&lt;br /&gt;
      + &lt;a href=&quot;http://blog.csdn.net/ttransposition/article/details/12966521&quot;&gt;DPM(Deformable Parts Model)–原理(一)&lt;/a&gt;&lt;br /&gt;
  -  Dalal和Triggs的检测器即&lt;strong&gt;Dalal-Triggs检测器模型&lt;/strong&gt;，在&lt;strong&gt;PASCAL 2006目标检测挑战赛上表现最好&lt;/strong&gt;,他使用基于HOG特征的单独滤波器(模版)来表示目标。它使用滑动窗口方法，将滤波器应用到图像的所有可能位置和尺度。可以将此检测器看做一个分类器，它将一张图片以及图片中的一个位置和尺度作为输入，然后判断在指定位置和尺度是否有目标类别的实例。&lt;br /&gt;
  - &lt;strong&gt;Deformable Part Model(DPM)&lt;/strong&gt;和 &lt;strong&gt;LatentSVM&lt;/strong&gt; 结合用于目标检测由大牛P.Felzenszwalb提出，代表作是以下3篇paper：&lt;br /&gt;
       + [1] P. Felzenszwalb, D. McAllester, D.Ramaman. A Discriminatively Trained, Multiscale, Deformable Part Model. Proceedingsof the IEEE CVPR 2008.&lt;br /&gt;
       + [2] P. Felzenszwalb, R. Girshick, D.McAllester, D. Ramanan. Object Detection with Discriminatively Trained PartBased Models. IEEE Transactions on Pattern Analysis and Machine Intelligence,Vol. 32, No. 9, September 2010.&lt;br /&gt;
       + [3] P. Felzenszwalb, R. Girshick, D.McAllester. Cascade Object Detection with Deformable Part Models. Proceedingsof the IEEE CVPR 2010.&lt;br /&gt;
  - [2]阐述了如何利用&lt;strong&gt;DPM（Deformable Part Model，DPM）来做检测&lt;/strong&gt;（特征处理+分类阶段），[3]阐述了如何利用&lt;strong&gt;cascade思想来加速检测&lt;/strong&gt;。综合来说，作者的思想是&lt;strong&gt;HogFeatures+DPM+LatentSVM的结合&lt;/strong&gt;：&lt;br /&gt;
      + 1、通过Hog特征模板来刻画每一部分，然后进行匹配。并且采用了金字塔，即在不同的分辨率上提取Hog特征。&lt;br /&gt;
      + 2、利用提出的Deformable PartModel，在进行object detection时，detect window的得分等于part的匹配得分减去模型变化的花费。&lt;br /&gt;
      + 3、在训练模型时，需要训练得到每一个part的Hog模板，以及衡量part位置分布cost的参数。文章中提出了LatentSVM方法，将deformable part model的学习问题转换为一个分类问题：利用SVM学习，将part的位置分布作为latent values，模型的参数转化为SVM的分割超平面。具体实现中，作者采用了迭代计算的方法，不断地更新模型。&lt;br /&gt;
  -  DPM是一个非常成功的目标检测算法，&lt;strong&gt;DPM连续获得VOC（Visual Object Class）2007,2008,2009年的检测冠军&lt;/strong&gt;。成为众多分类器、分割、人体姿态和行为分类的重要部分。&lt;br /&gt;
  -  DPM可以做到人脸检测和关键点定位的一气呵成，但是其计算量太大导致时间消耗过高。&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;msrajoint-cascade-face-detection-and-alignmen-eccv&quot;&gt;2014-MSRA的新技术《Joint Cascade Face Detection and Alignmen》 [ECCV]&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;参考：
    &lt;ul&gt;
      &lt;li&gt;Chen D, Ren S, Wei Y, et al. Joint Cascade Face Detection and Alignment[C]// European Conference on Computer Vision. Springer, Cham, 2014:109-122.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/sciencefans/p/4394861.html#!comments&quot;&gt;人脸识别技术大总结1——Face Detection &amp;amp; Alignment&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/u010333076/article/details/50637342&quot;&gt;论文《Joint Cascade Face Detection and Alignment》笔记&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/FaceDetect/jointCascade_py&quot;&gt;github:FaceDetect/jointCascade_py&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;结合了 cascade 和 alignment,在30ms的时间里完成detection和alignment，PR曲线很高，时效性高，内存占用却非常低，在一些库上虐了Face++和Google Picasa。&lt;/li&gt;
  &lt;li&gt;步骤：
    &lt;ul&gt;
      &lt;li&gt;1.样本准备：首先作者调用opencv的Viola-Jones分类器，将recal阀值设到99%，这样能够尽可能地检测出所有的脸，但是同时也会有非常多的不是脸的东东被检测出来。于是，检测出来的框框们被分成了两类：是脸和不是脸。这些图片被resize到96*96。&lt;/li&gt;
      &lt;li&gt;2.特征提取.作者采用了三种方法：
        &lt;ul&gt;
          &lt;li&gt;第一种：把window划分成6*6个小windows，分别提取SIFT特征，然后连接着36个sift特征向量成为图像的特征。&lt;/li&gt;
          &lt;li&gt;第二种：先求出一个固定的脸的平均shape（27个特征点的位置，比如眼睛左边，嘴唇右边等等），然后以这27个特征点为中心提取sift特征，然后连接后作为特征。&lt;/li&gt;
          &lt;li&gt;第三种：用他们组去年的另一个成果Face Alignment at 3000 FPS via Regressing Local Binary Features (CVPR14) ，也就是图中的3000FPS方法，回归出每张脸的shape，然后再以每张脸自己的27个shape points为中心做sift，然后连接得到特征。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;3.分类：将上述的三种特征分别扔到线性SVM中做分类，训练出一个能分辨一张图是不是脸的SVM模型。&lt;/li&gt;
      &lt;li&gt;作者将用于关键点校准的回归树结合上用于检测的弱分类器重建为一棵新的决策树，并命名为 classification/regression decision tree （分类回归决策树）,输出一个用于判决人脸得分的同时还会输出关键点的增量，可以说这两步是完全同步的，并且采用的是相同的特征。越靠前(根节点）的层里，较多的结点用于分类，较少的结点用于回归；越靠后的层（叶节点）里，较少的结点用于分类，较多的结点用于回归。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;deepid1---cvpr-&quot;&gt;2014-DeepID1   [CVPR ]&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;参考：
    &lt;ul&gt;
      &lt;li&gt;Sun Y, Wang X, Tang X. Deep Learning Face Representation from Predicting 10,000 Classes[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2014:1891-1898.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/zzq1989/p/4373680.html&quot;&gt;Deep Learning Face Representation from Predicting 10,000 Classes论文笔记&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;步骤：
    &lt;ul&gt;
      &lt;li&gt;人脸标注裁剪出60个patches，分别训练出60个CNN模型ConvNets（softmax多分类）。&lt;/li&gt;
      &lt;li&gt;每个ConvNets提取两个160维的特征（两个指镜像，160维是ConvNet模型的倒数第二层提取出),一张图片提取19200维（160×60×2）。所有这些160维特征，称为Deep hidden identity feature(DeepID)。&lt;/li&gt;
      &lt;li&gt;接下来是Face Verification（人脸验证）过程，即传入两张图片，判断是否同一人。传入两张图片，分别获得DeepID特征，将他们（为60个组拼接而成，一组表示一个patch且有640维，640维= 160维特征×2张镜像x2张图片的某patch）送到&lt;strong&gt;联合贝叶斯(Joint Bayesian，JB)&lt;/strong&gt;或者一个神经网络( Neural Network，NN)进行Face Verification。&lt;/li&gt;
      &lt;li&gt;联合贝叶斯网络准确率显然高于NN，所以最终实验作者采用的是JB。这个实验也说明，随着网络输入种类(人数)的增大，能够提高网络的泛化能力，更利于提取的DeepID特征进行分类。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;当时的人脸识别方法：&lt;strong&gt;过完全的低级别特征+浅层模型&lt;/strong&gt;。而ConvNet 能够有效地提取高级视觉特征。已有的DL方法：
    &lt;ul&gt;
      &lt;li&gt;Huang【CVPR2012】的生成模型+非监督；&lt;/li&gt;
      &lt;li&gt;Cai 【2012】的深度非线性度量学习；&lt;/li&gt;
      &lt;li&gt;Sun【CVPR2013】的监督学习+二类分类（人脸校验 verfication），是作者2013年写的。本文是1W类的多分类问题。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;1张图片通过Deep ConvNets得到DeepID特征向量，再通过PCA降维特征向量。最终在LFW库上测试可以得到97.20%的精度.对齐人脸后能达到97.45%。而目前（2014年）几种流行算法数据:&lt;img src=&quot;pictures/LFW_Face_Method_before2014.png&quot; alt=&quot;2014年前&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;#### 2014-DeepID2   [CVPR ]&lt;br /&gt;
  - 参考：&lt;br /&gt;
    * Sun Y, Wang X, Tang X. Deep Learning Face Representation by Joint Identification-Verification[J]. Advances in Neural Information Processing Systems, 2014, 27:1988-1996.&lt;br /&gt;
    * &lt;a href=&quot;http://blog.csdn.net/stdcoutzyx/article/details/41497545&quot;&gt;DeepID2——强大的人脸分类算法&lt;/a&gt;&lt;br /&gt;
    * &lt;a href=&quot;http://blog.csdn.net/stdcoutzyx/article/details/42091205&quot;&gt;DeepID人脸识别算法之三代&lt;/a&gt;&lt;br /&gt;
  - 在DeepID的softmax使用Logistic Regression作为最终的目标函数，即识别信号。而DeepID2继续&lt;strong&gt;添加验证信号&lt;/strong&gt;，两个信号使用加权的方式进行了组合。 &lt;strong&gt;识别信号（identification）增大类间距离，验证信号（verification）减少类内距离&lt;/strong&gt;。&lt;br /&gt;
  - 由于验证信号的计算需要两个样本，所以整个卷积神经网络的训练过程也就发生了变化，之前是将全部数据切分为小的batch来进行训练。现在则是每次迭代时随机抽取两个样本，然后进行训练。&lt;br /&gt;
  - 首先使用&lt;strong&gt;SDM(supervised descent method)算法&lt;/strong&gt;对每张人脸检测出21个landmarks，然后根据这些landmarks，再加上位置、尺度、通道、水平翻转等因素，每张人脸形成了400张patch，使用200个CNN对其进行训练，水平翻转形成的patch跟原始图片放在一起进行训练。这样，就形成了400×160维的向量。这样形成的特征维数太高，所以要进行特征选择，不同于之前的DeepID直接采用PCA的方式，DeepID2先对patch进行选取，使用&lt;strong&gt;前向-后向贪心算法&lt;/strong&gt;选取了25个最有效的patch，这样就只有25×160维向量，然后使用PCA进行降维，降维后为180维，然后再输入到联合贝叶斯模型中进行分类&lt;br /&gt;
  - 在LFW数据库上得到了99.15%人脸准确率。&lt;br /&gt;
  - &lt;a href=&quot;http://www.cnblogs.com/Anita9002/p/7095380.html&quot;&gt;机器学习–详解人脸对齐算法SDM-LBF&lt;/a&gt; ,   &lt;a href=&quot;机器学习----人脸对齐的算法-ASM.AAM..CLM.SDM&quot;&gt;机器学习—-人脸对齐的算法-ASM.AAM..CLM.SDM&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;#### 2015-DeepID2+   [CVPR]&lt;br /&gt;
  - 参考： &lt;br /&gt;
    * Sun Y, Wang X, Tang X. Deeply learned face representations are sparse, selective, and robust[C]// Computer Vision and Pattern Recognition. IEEE, 2015:2892-2900.&lt;br /&gt;
    * &lt;a href=&quot;http://blog.csdn.net/yuanchheneducn/article/details/51034463&quot;&gt;DeepID1 DeepID2 DeepID2+ DeepID3&lt;/a&gt;&lt;br /&gt;
    * &lt;a href=&quot;http://blog.csdn.net/stdcoutzyx/article/details/42091205&quot;&gt;DeepID人脸识别算法之三代&lt;/a&gt;&lt;br /&gt;
  - 相比于DeepID2，DeepID2+做了如下三点修改：&lt;br /&gt;
    * DeepID特征从160维提高到512维。&lt;br /&gt;
    * 训练集将CelebFaces+和WDRef数据集进行了融合，共有12000人，290000张图片。&lt;br /&gt;
    * 将DeepID层不仅和第四层和第三层的max-pooling层连接，还连接了第一层和第二层的max-pooling层。&lt;br /&gt;
    * DeepID2+自动就对遮挡有很好的鲁棒性&lt;br /&gt;
    * 最后的DeepID2+的网络结构中ve是验证信号和识别信号加权和的监督信号;FC-n表示第几层的max-pooling。&lt;img src=&quot;pictures/DeepID2_plus_net.png&quot; alt=&quot;DeepID2_plus_net&quot; /&gt;&lt;br /&gt;
  - &lt;strong&gt;适度稀疏与二值化&lt;/strong&gt;: DeepID2+有一个性质，即对每个人照片，最后的DeepID层都大概有半数的单元是激活的，半数的单元是抑制的。而不同的人，激活或抑制的单元是不同的。基于此性质。使用阈值对最后输出的512维向量进行了二值化处理，发现效果降低仅[0.5%,1%],但二值化后会有好处，即通过计算汉明距离就可以进行检索了。然后精度保证的情况下，可以使人脸检索变得速度更快，更接近实用场景。&lt;br /&gt;
  - 在后续调查极度深网络的效果，在VGG和GooLeNet的基础上进行构建合适的结构&lt;strong&gt;DeepID3&lt;/strong&gt;，结果发现DeepID3的结果和DeepID2+相当。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;#### 2015-CNN级联实现Viola-Jones人脸检测器 [CVPR]&lt;br /&gt;
  - 参考：&lt;br /&gt;
    * Li H, Lin Z, Shen X, et al. A convolutional neural network cascade for face detection[C]// Computer Vision and Pattern Recognition. IEEE, 2015:5325-5334.&lt;br /&gt;
    * &lt;a href=&quot;http://blog.csdn.net/u010333076/article/details/50637317&quot;&gt;论文《A Convolutional Neural Network Cascade for Face Detection》笔记&lt;/a&gt;&lt;br /&gt;
    * &lt;a href=&quot;https://github.com/anson0910/CNN_face_detection&quot;&gt;github:anson0910/CNN_face_detection&lt;/a&gt;&lt;br /&gt;
  - 基于Viola-Jones，提出级联的CNN网络结构用于人脸识别，贡献如下：&lt;br /&gt;
    * 提出了一种级联的CNN网络结构用于高速的人脸检测。&lt;br /&gt;
    * 设计了一种边界校订网络用于更好的定位人脸位置。&lt;br /&gt;
    * 提出了一种多分辨率的CNN网络结构，有着比单网络结构更强的识别能力，和一个微小的额外开销。&lt;br /&gt;
    * 在FDDB上达到了当时最高的分数。&lt;br /&gt;
  - 级联的三个网络结构，其读入的图片分辨率和网络的复杂度是逐级递增的。前面的简单网络拒绝绝大部分非人脸区域，将难以分辨的交由下一级更复杂的网络以获得更准确的结果。&lt;br /&gt;
  - 要想在CNN结构下实现Viola-Jones瀑布级连结构，就要保证瀑布的前端足够简单并有较高的召回率且能够拒绝大部分非人脸区域，将图片缩放可以满足需求。&lt;br /&gt;
  - 这三个网络用于&lt;strong&gt;矫正人脸检测框&lt;/strong&gt;的边界，往往得分最高的边界框并非最佳结果，经过校准后其能更好的定位人脸，其矫正原理是对原图做45次变换（&lt;strong&gt;坐标移动比例、尺寸缩放比例&lt;/strong&gt;），然后每个变换后的边界框都有一个得分，对于得分高于某个设定的阈值时，将其累加进原边界，最后结果取平均，就是最佳边界框。&lt;br /&gt;
  - &lt;img src=&quot;pictures/A_Convolutional_Neural_Network_Cascade_for_Face_Detection_1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;#### 2015-Google的FaceNet [CVPR]&lt;br /&gt;
  - 参考：&lt;br /&gt;
    * Schroff F, Kalenichenko D, Philbin J. FaceNet: A unified embedding for face recognition and clustering[J]. 2015:815-823.&lt;br /&gt;
    * &lt;a href=&quot;http://blog.csdn.net/stdcoutzyx/article/details/46687471&quot;&gt;FaceNet–Google的人脸识别&lt;/a&gt;&lt;br /&gt;
    * &lt;a href=&quot;http://blog.csdn.net/jyshee/article/details/52558192&quot;&gt;Understanding FaceNet&lt;/a&gt;&lt;br /&gt;
  - 通过卷积网络后的特征，经过L2归一化后得到特征，再采用LMNN(Large margin nearest neighbor,最大间隔最近邻居)中的Triplet Loss，从而替换掉以往的Softmax结构，Triplet loss是尽可能增大类间距离，减少类内距离。&lt;br /&gt;
  - 三元组选择对模型收敛、效率等很重要，文中提出两种方法：&lt;br /&gt;
    * 1 每N步线下在数据的子集上生成一些triplet&lt;br /&gt;
    * 2 在线生成triplet，在每一个mini-batch中选择hard pos/neg 样例。&lt;br /&gt;
    * 论文采用了第2种。为使mini-batch中生成的triplet合理，生成时确保每个mini-batch中每人图片数量均等，再随机加入反例。生成triplet的时，要找出所有的anchor-pos对，然后对每个anchor-pos对找出其hard neg样本。&lt;br /&gt;
  - LFW上的效果：&lt;br /&gt;
    * 直接取LFW图片的中间部分进行训练，达98.87%左右。&lt;br /&gt;
    * 使用额外的人脸对齐工具，效果99.63左右，超过DeepID。&lt;br /&gt;
  - FaceNet不像DeepFace和DeepID需要对齐。且FaceNet得到最终表示后不用像DeepID那样需要再训练模型进行分类，它直接计算距离就即可。&lt;br /&gt;
  - Triplet Loss的目标函数&lt;strong&gt;不是&lt;/strong&gt;这论文首次提出。&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;joint-training-of-cascaded-cnn-for-face-detectioncvpr&quot;&gt;2016-人脸检测中级联卷积神经网络的联合训练《Joint Training of Cascaded CNN for Face Detection》[CVPR]&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;参考：
    &lt;ul&gt;
      &lt;li&gt;Qin H, Yan J, Li X, et al. Joint Training of Cascaded CNN for Face Detection[C]// Computer Vision and Pattern Recognition. IEEE, 2016:3456-3465.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://note.youdao.com/share/?id=bd65a89a78c201d5c06d6636e9f45069&amp;amp;type=note#/&quot;&gt;有道云笔记：Joint Training of Cascaded CNN for Face Detection&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;本文是对《A Convolutional Neural Network Cascade for Face Detection》（CNN级联实现Viola-Jones人脸检测器）的优化，优化手段就是&lt;strong&gt;用BP进行联合训练&lt;/strong&gt;。作者提出了联合训练以达到CNN级联端对端的优化。作者展示了用于训练CNN的反向传播算法可以被用于训练CNN级联。联合训练可以被用于简单的CNN级联和RPN，fast RCNN。&lt;/li&gt;
      &lt;li&gt;现有最好的方法（2016）基本都是用&lt;strong&gt;多阶段机制&lt;/strong&gt;。第一个阶段提出region proposal 第二个阶段是用来detection的网络。级联cnn和faster rcnn都是这个流程。这些方法都没有联合训练，都是用贪心算法去优化的。本文中提出的方法是使用bp去优化。所以不同网络的每一层都可以被联合优化。&lt;/li&gt;
      &lt;li&gt;使用联合训练的情况如下：
        &lt;ul&gt;
          &lt;li&gt;1.detection network 和calibration network可以共享multi-loss network用于detection 和bounding box 回归。&lt;/li&gt;
          &lt;li&gt;2 因为multi-resolution得到了使用，那么后一个network将会包括前一个network，那么可以让卷积层在三个stage中共享。&lt;/li&gt;
          &lt;li&gt;3.某一个stage的network的参数可以被别的branch联合优化。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;可以使用联合训练的方法训练RPN+fast RCNN。&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;pictures/Joint_Training_of_Cascaded_CNN_for_Face_Detection_1.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;咦做表情识别呢？？怎么突然就不见了  = =&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;## 表情识别&lt;/p&gt;

&lt;h4 id=&quot;image-based-static-facial-expression-recognition-with-multiple-deep-network-learning&quot;&gt;2015-多种方法联合改进《Image based Static Facial Expression Recognition with Multiple Deep Network Learning》&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;1 多种串联方式检测人脸&lt;br /&gt;
&lt;img src=&quot;pictures/Image_based_Static_Facial_Expression_Recognition_with_Multiple_Deep_Network_Learning_1.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;2 五个卷积层，三个随机池层和三个完全连接的层的网络结构。&lt;/li&gt;
  &lt;li&gt;3 较完整地对训练图片进行随机裁剪、翻转、旋转、倾斜等等。
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;pictures/Image_based_Static_Facial_Expression_Recognition_with_Multiple_Deep_Network_Learning_2.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;其中θ是从三个不同值随机采样的旋转角度：{ - π/18，0，π/18}。&lt;/li&gt;
      &lt;li&gt;s1和s2是沿着x和y方向的偏斜参数，并且都是从{-0.1,0,0.1}随机采样的。&lt;/li&gt;
      &lt;li&gt;c是随机尺度参数。定义为c = 47 /（47 - δ），其中δ是[0，4]上随机采样的整数。&lt;/li&gt;
      &lt;li&gt;实际上，用下面的逆映射产生变形的图像：
        &lt;ul&gt;
          &lt;li&gt;&lt;img src=&quot;pictures/Image_based_Static_Facial_Expression_Recognition_with_Multiple_Deep_Network_Learning_4.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;其中A是歪斜，旋转和缩放矩阵的组成。输入（x’∈[0,47]，y’∈[0,47]）是变形图像的像素坐标。简单地计算逆映射以找到对应的（x，y）。由于所计算的映射大多包含非整数坐标，因此使用双线性插值来获得扰动的图像像素值&lt;/li&gt;
          &lt;li&gt;t1和t2是两个平移参数，其值从{0，δ}被采样并且与c耦合。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;4 扰动的学习和投票:
    &lt;ul&gt;
      &lt;li&gt;架构最后有P个Dense(7)，这P个扰动样本的输出结果的平均投票，（合并后）作为该图像的预测值。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;5 预训练：
    &lt;ul&gt;
      &lt;li&gt;样本随机扰动&lt;/li&gt;
      &lt;li&gt;训练时候增加了超过25%或者连续5次增加train loss ，则降低学习率为之前的一半，并重新加载之前损失最好的模型，继续训练.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;6 克服过拟合问题:
    &lt;ul&gt;
      &lt;li&gt;冻结所有卷积图层的参数，只允许在完全连接的图层上更新参数。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;7 上述都是单一网络，现采用集成多网络方式。
    &lt;ul&gt;
      &lt;li&gt;常用方法是对输出响应进行简单平均。&lt;/li&gt;
      &lt;li&gt;此处采用自适应地为每个网络分配不同的权重，即要学习集合权重w。采用独立地训练多个不同初始化的CNN并输出他们的训练响应。在加权的集合响应上定义了损失，其中w优化以最小化这种损失。在测试中，学习的w也被用来计算整体的测试响应。&lt;/li&gt;
      &lt;li&gt;在本文中，我们考虑以下两个优化框架：
        &lt;ul&gt;
          &lt;li&gt;最大似然方法&lt;/li&gt;
          &lt;li&gt;最小化Hinge loss&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;figure class=&quot;half&quot;&gt;
  &amp;lt;img src=&quot;pictures/Image_based_Static_Facial_Expression_Recognition_with_Multiple_Deep_Network_Learning_6.png&quot;, width=&quot;380&quot;&amp;gt;
  &amp;lt;img src=&quot;pictures/Image_based_Static_Facial_Expression_Recognition_with_Multiple_Deep_Network_Learning_7.png&quot;, width=&quot;380&quot;&amp;gt;
&lt;/figure&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;&amp;lt;/figure&amp;gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;#### 2015-用LBP特征作为CNN输入《Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns》对照明变化有鲁棒性&lt;br /&gt;
作者采用了：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;4个cnn模型VGG S,VGG M-2048,VGG M-4096和GoogleNet&lt;/li&gt;
  &lt;li&gt;5种不同特征作为CNN输入 （RGB, LBP，以及作者额外三种处理的LBP特征）&lt;/li&gt;
  &lt;li&gt;进行了20次实验。实验中10个最好的model中只有一个是RGB作为输入的。&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;half&quot;&gt;
    &amp;lt;img src=&quot;pictures/Emotion_Recognition_in_the_Wild_via_Convolutional_Neural_Networks_and_Mapped_Binary_Patterns_1.png&quot;, width=&quot;600&quot;&amp;gt; 
&lt;/figure&gt;
&lt;p&gt;由于LBP的差值不能反映两点间的差异，作者提出了mapping方法让其差能代表两点真实差距。&lt;br /&gt;
将图像转换为LBP代码，使模型对照明亮度变化具有鲁棒性。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;#### 2015-不同CNN及组合+数据改进《Hierarchical Committee of Deep CNNs with Exponentially-Weighted Decision Fusion for Static Facial Expression Recognition》&lt;br /&gt;
EmotiW 2015的冠军，和《2015-Image based Static Facial Expression Recognition with Multiple Deep Network Learning》类似的方法。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;先对图片做align,&lt;/li&gt;
  &lt;li&gt;然后设计了三种CNN,由不同的输入,不同的训练数据和不同的初始化训练了216个model,&lt;/li&gt;
  &lt;li&gt;然后用自己提出的方法将这些model组合起来&lt;/li&gt;
  &lt;li&gt;都是想办法增加训练集,让一张图片生成多张,&lt;/li&gt;
  &lt;li&gt;又比如训练多个model结合起来&lt;/li&gt;
  &lt;li&gt;就经常见到的那些方法。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;#### 2013-级联网络5个人脸特征点定位《Deep Convolutional Network Cascade for Facial Point Detection》&lt;/p&gt;

&lt;p&gt;2013年CVPR，&lt;a href=&quot;http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm&quot;&gt;论文的主页&lt;/a&gt;&lt;br /&gt;
比较经典的做法，分为3大部分进行一步步定位。分别是level1,level2,level3。每个level中有好几个cnn模型。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;其中level1（深层网络）是对关键点进行准确估计，&lt;/li&gt;
  &lt;li&gt;接下的两层将对小局部区域进行精细估计（浅层网络）。&lt;/li&gt;
  &lt;li&gt;level1中几个cnn模型分别交错预测关键点（一个cnn模型预测多个点，不同cnn模型预测点有合集）。&lt;/li&gt;
  &lt;li&gt;全部level中会多次预测同一关键点，取平均值。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;#### 2013-改进的内外轮廓68人脸特征点定位《Extensive Facial Landmark Localization with Coarse-to-fine Convolutional Network Cascade》[ICCV]&lt;br /&gt;
2013年face++的作品ICCV.&lt;br /&gt;
这篇是基于face++的前一篇定位5个特征点《2013-Deep Convolutional Network Cascade for Facial Point Detection》的。&lt;/p&gt;

&lt;p&gt;参考： &lt;a href=&quot;http://blog.csdn.net/hjimce/article/details/50099115&quot;&gt;基于改进Coarse-to-fine CNN网络的人脸特征点定位&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;也是采用由粗到细，和上一篇步骤类似，这篇分开了内特征点和轮廓特征点的定位。&lt;br /&gt;
让cnn预测bounding box而不是其他人脸检测器（让人脸框住最小化无关背景，如内特征点时可以把人脸轮廓那些部分不纳入bounding box）。各个部分分开训练（五官分开训练，减少因为不同种类的点定位难度不同而影响训练）&lt;/p&gt;

&lt;p&gt;下图中表示了全步骤：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;level1是两个cnn来预测bounding box（crop输出）。&lt;/li&gt;
  &lt;li&gt;level2是对定位的粗估计&lt;/li&gt;
  &lt;li&gt;level3是细定位（裁剪后的五官进行训练预测）&lt;/li&gt;
  &lt;li&gt;level4是将level3中图片进行旋转再预测（这层的精度提高很小）。&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;half&quot;&gt;
    &amp;lt;img src=&quot;pictures/Extensive_Facial_Landmark_Localization_with_Coarse-to-fine_Convolutional_Network_Cascade_1.png&quot;, width=&quot;700&quot;&amp;gt; 
&lt;/figure&gt;

&lt;hr /&gt;
&lt;p&gt;#### 2016-两个Inception+ SDM提取特征点来截更小的人脸框《Going Deeper in Facial Expression Recognition using Deep Neural Networks》&lt;br /&gt;
人脸特征点来获取更小的人脸框（提高4-10%）、两个Inception的CNN框架。&lt;/p&gt;
&lt;figure class=&quot;half&quot;&gt;
    &amp;lt;img src=&quot;pictures/Going_Deeper_in_Facial_Expression_Recognition_using_Deep_Neural_Networks_1.png&quot;, width=&quot;900&quot;&amp;gt; 
&lt;/figure&gt;

&lt;hr /&gt;
&lt;p&gt;#### 2013-fer2013第一名71.2%：损失函数为L2-SVM的表情分类《Deep Learning using Linear Support Vector Machines》&lt;br /&gt;
将损失函数Softmax改为L2-SVM，相比L1-SVM,它具有可微，并且对误分类有更大的惩罚。&lt;/p&gt;

&lt;p&gt;SVM约束函数如右公式4。改写为无约束优化问题如公式5,即为L1-SVM的最初形式。公式6为L2-SVM的形式。&lt;/p&gt;
&lt;figure class=&quot;half&quot;&gt;
    &amp;lt;img src=&quot;pictures/Deep_Learning_using_Linear_Support_Vector_Machines_1.png&quot;, width=&quot;400&quot;&amp;gt;

&lt;/figure&gt;
&lt;figure class=&quot;half&quot;&gt;
    &amp;lt;img src=&quot;pictures/Deep_Learning_using_Linear_Support_Vector_Machines_2.png&quot;, width=&quot;400&quot;&amp;gt;
    &amp;lt;img src=&quot;pictures/Deep_Learning_using_Linear_Support_Vector_Machines_3.png&quot;, width=&quot;400&quot;&amp;gt;
&lt;/figure&gt;
&lt;p&gt;L2-svm通过  $arg_t max (w^Tx)t \quad (t_n \in \lbrace −1, +1\rbrace)$来判断x的类别。&lt;/p&gt;

&lt;p&gt;在反向传播需要对其求导。如公式10为L1-SVM的，它是不可微。而公式11是L2-SVM的且是可微的。&lt;br /&gt;
且L2-SVM比L1-SVM效果好些。&lt;/p&gt;

&lt;p&gt;Softmax和L2-SVM在FER2013中的效果如图:&lt;/p&gt;

&lt;figure class=&quot;half&quot;&gt;
    &amp;lt;img src=&quot;pictures/Deep_Learning_using_Linear_Support_Vector_Machines_6.png&quot;, width=&quot;500&quot;&amp;gt; 
&lt;/figure&gt;

&lt;hr /&gt;

&lt;p&gt;考试啦，先暂停看论文。。 Ort&lt;/p&gt;

&lt;p&gt;之后我转向图像分类网络的研究了= =&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Wed, 01 Nov 2017 00:00:00 +0800</pubDate>
        <link>http://luonango.github.io/2017/11/01/%E8%A1%A8%E6%83%85%E8%AF%86%E5%88%AB_%E4%B8%80%E4%BA%9B%E6%95%B4%E7%90%86/</link>
        <guid isPermaLink="true">http://luonango.github.io/2017/11/01/%E8%A1%A8%E6%83%85%E8%AF%86%E5%88%AB_%E4%B8%80%E4%BA%9B%E6%95%B4%E7%90%86/</guid>
        
        <category>表情识别</category>
        
        <category>人脸识别</category>
        
        <category>图像分类</category>
        
        
      </item>
    
  </channel>
</rss>
