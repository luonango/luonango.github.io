<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Every failure is leading towards success.">
    <meta name="keywords"  content="BY, BY Blog, Nango明楠的博客, luonango, Nango明楠, iOS, Apple, iPhone">
    <meta name="theme-color" content="#000000">
    
    <title>Fusedmax与Oscarmax - Nango明楠的博客 | BY Blog</title>

    <!-- Web App Manifest -->
    <link rel="manifest" href="/pwa/manifest.json">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/favicon.ico">

    <!-- Safari Webpage Icon    by-BY -->
    <link rel="apple-touch-icon" href="/img/apple-touch-icon.png">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="http://luonango.github.io/2018/07/01/Fusedmax%E4%B8%8EOscarmax%E7%A8%80%E7%96%8F%E5%8F%8A%E7%BB%93%E6%9E%84%E5%8C%96%E7%9A%84Attention%E6%AD%A3%E5%88%99%E5%8C%96%E6%A1%86%E6%9E%B6/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>

</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">BY Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    
                    <li>
                        <a href="/about/">About</a>
                    </li>
                    
                    <li>
                        <a href="/tags/">Tags</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    var __HuxNav__ = {
        close: function(){
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        },
        open: function(){
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }

    // Bind Event
    $toggle.addEventListener('click', function(e){
        if ($navbar.className.indexOf('in') > 0) {
            __HuxNav__.close()
        }else{
            __HuxNav__.open()
        }
    })

    /**
     * Since Fastclick is used to delegate 'touchstart' globally
     * to hack 300ms delay in iOS by performing a fake 'click',
     * Using 'e.stopPropagation' to stop 'touchstart' event from 
     * $toggle/$collapse will break global delegation.
     * 
     * Instead, we use a 'e.target' filter to prevent handler
     * added to document close HuxNav.  
     *
     * Also, we use 'click' instead of 'touchstart' as compromise
     */
    document.addEventListener('click', function(e){
        if(e.target == $toggle) return;
        if(e.target.className == 'icon-bar') return;
        __HuxNav__.close();
    })
</script>


    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/post-bg-ios9-web.jpg" width="0" height="0"> -->
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],
            displayMath: [['$$','$$']],
            }
        });
    </script>
</head>
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        position: relative;
        background-image: url('/img/post-bg-ios9-web.jpg')
    }

    
</style>
<header class="intro-header" >
    <div class="header-mask"></div>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/#Attention" title="Attention">Attention</a>
                        
                        <a class="tag" href="/tags/#正则化" title="正则化">正则化</a>
                        
                        <a class="tag" href="/tags/#稀疏" title="稀疏">稀疏</a>
                        
                    </div>
                    <h1>Fusedmax与Oscarmax</h1>
                    
                    
                    <h2 class="subheading">稀疏及结构化的Attention正则化框架</h2>
                    
                    <span class="meta">Posted by BY on July 1, 2018</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

				<h1 id="fusedmaxoscarmaxattention">2017_Fusedmax与Oscarmax_稀疏及结构化的Attention正则化框架</h1>

<div class="highlighter-rouge"><pre class="highlight"><code>2017_NIPS
康奈尔大学, Vlad Niculae
</code></pre>
</div>

<p>论文提出<strong>让Attention输出稀疏</strong>且更关注<strong>输入数据的片段或组</strong>的正则化机制，它还能直接加入神经网络进行前向与反向传播。</p>

<p>论文地址: <a href="https://papers.nips.cc/paper/6926-a-regularized-framework-for-sparse-and-structured-neural-attention.pdf">A Regularized Framework for Sparse and Structured Neural Attention</a></p>

<h2 id="section">1. 简单介绍:</h2>

<p><strong>Attention的关键是映射函数</strong>，它对输入元素的相对重要性进行编码，将数值映射为概率。而常用的关注机制<strong>Softmax会产生密集的注意力</strong>。因为softmax的输出都大于0，故其输入的所有元素对最终决策都有或多或少的影响。</p>

<p>为了克服softmax这种缺点，<a href="https://arxiv.org/pdf/1602.02068.pdf">From Softmax to Sparsemax:A Sparse Model of Attention and Multi-Label Classification</a> 提出了能够<strong>输出稀疏概率的Sparsemax</strong>，这能为Attention提供更多的解释性。</p>

<p>本文基于Sparsemax，提出的方法既可以得到稀疏的输出，又可以作为一个诱导稀疏的惩罚模型，可作用于当前的其他模型上。 本文提出的通用框架是<strong>建立在$\max$运算符上的，采用强凸（strong convex）函数进行调整后得到的算子是可微的，它的梯度定义了从输入数值到概率的映射，适合作为Attention机制</strong>。</p>

<ul>
  <li>基于fused lasso提出了<strong>Fusedmax鼓励网络对连续区域（或说文本段）的Attention值相同</strong>。</li>
  <li>限定连续区域的Attention值相同的假设太严苛，文中又基于oscar提出<strong>Oscarmax的关注机制，鼓励网络对不连续的单词组的Attention值相同</strong>。</li>
</ul>

<p>上述只是文章简单概括。 疑问有很多，比如：建立在$\max$算子是什么意思？强凸函数怎么调整？梯度定义了映射函数？Fusedmax和Oscarmax又是怎么做到的？</p>

<p>那下面就一步步解释文章思路。</p>

<hr />
<p>## 2. 论文的思路:</p>

<p>先给出一些定义：</p>

<ul>
  <li>集合${1,2,…,d}$定义为$[d]$，$d-1$维的单形体为$\Delta^d:={x\in R^d:\mid\mid x\mid\mid_1=1,x \geq0}$， 在欧几里得的投影为$P_{\Delta^d}(x):={\arg\min}_{y\in \Delta^d}\mid\mid y-x\mid\mid^2$</li>
  <li>如果函数$f:R^d\rightarrow R\bigcup {\infty}$, 则其凸共轭(convex conjugate)为$f^{<em>}(x):=\sup_{y\in dom\;f}y^Tx-f(y)$.  给定范数$\mid\mid\cdot\mid\mid$,它的对偶定义为$\mid\mid x\mid\mid_</em> :=\sup_{\mid\mid y\mid\mid \leq 1}y^T x$. 用$\partial f(y)$ 表示函数$f$在$y$处的次微分 <br />
&gt; 次微分subdifferential,凸函数$f(x)=\mid x\mid$在原点的次微分是区间$[−1, 1]$.</li>
  <li>函数$f$的Jacobian(雅可比)$J_{g}(y)\in R^{d\times d}$,Hessian(海森矩阵)$H_{f}(y)\in R^{d\times d}$</li>
</ul>

<p>现在我们看下$max$算子（虽然是$R^d \rightarrow \Delta^d$的映射函数，但不适合作为Attention机制):<br />
<script type="math/tex">\max(x):=\max_{i\in [d]} x_i = \sup_{y\in \Delta^d} y^Tx</script></p>

<p>由于在单形体的线性上确界总是顶点，也即为标准基向量${e_i}^d_{i=1}$的其中之一。 这也容易看出这个上确界$y^<em>$ 就是$\max(x)$的一个次梯度$\partial \max(x) ={e_{i^</em>} : i^* \in \arg\max_{i\in [d]} x_i}$.</p>

<p>我们将这些次梯度看作一种映射：$\prod:R^d \rightarrow \Delta^d$,它将所有的概率质量都放在一个元素上(即$\max$操作只有一个元素获得非0输出):$\prod(x) = e_i,\; for \;any\; e_i \in \partial \max(x)$.</p>

<p>因为这映射函数不连续（存在阶跃断点），这不适合通过梯度下降法进行优化,显然这种属性我们是不希望的.</p>

<p>从这可以看出，$\max(x)$的次梯度$y^*$是$\prod:R^d \rightarrow \Delta^d$映射，<strong>如果$\max(x)$的变种函数是连续可二次微分，那$y^<em>$也就可以用作Attention，且也能够让梯度下降方法进行优化了（梯度为$y^</em>$的导数）</strong>。<br />
&gt;注意，此处$\prod(x)$也表示Attention的输出，如果$\prod(x)$可导，就可以嵌入普通神经网络进行梯度下降优化了。</p>

<p>受到<a href="http://luthuli.cs.uiuc.edu/~daf/courses/optimization/MRFpapers/nesterov05.pdf">Y.Nesterov. Smooth minimization of non-smooth functions</a>的启发,本文运用了Smooth技术. 对于$\max(x)$的共轭函数${\max}^<em>(y)$:<br />
<script type="math/tex">% <![CDATA[
{\max}^*(y)=\left\{
             \begin{array}{l}
             0, & if\; y\in \Delta^d \\
             \infty, & o.w. 
             \end{array}\right. %]]></script> <br />
&gt;该共轭函数的证明在<a href="http://papers.nips.cc/paper/5710-smooth-and-strong-map-inference-with-linear-convergence">Smooth and strong: MAP inference with linear<br />
convergence, Appendix B</a> 中。其实通过求解共轭函数的方法就可以得解：$max^</em>(y)=\sup(y^Tx-\sup z^Tx)$, 对$x$求偏导得$y^<em>=z^</em>$再带入原式即可。</p>

<p>那现在将正则化添加到共轭函数中：<br />
<script type="math/tex">% <![CDATA[
{\max}^*_{\Omega}(y)=\left\{
             \begin{array}{l}
             \gamma\Omega(y), & if\; y\in \Delta^d \\
             \infty, & o.w. 
             \end{array}\right. %]]></script> <br />
其中假设函数$\Omega:R^d\rightarrow R$是关于norm $\mid\mid\cdot\mid\mid$的<strong>$\beta$-strongly convex</strong>($\beta强凸$)。$\gamma$ 控制着正则强度。</p>

<blockquote>

  <p>参考<a href="https://cswhjiang.github.io/2015/04/08/strong-convexity-and-smoothness/">Strong Convexity and Smoothness</a>：</p>

  <ul>
    <li>函数$f$是 $\alpha$-strong convex($\alpha &gt; 0$)需要满足条件：<br />
<script type="math/tex">f(y)-f(x)\geq \nabla f(x)^T(y-x) + \frac{\alpha}{2}\mid\mid y-x \mid\mid^2_P</script></li>
    <li>
      <p>如果一个连续可微的函数$f$的梯度$\nabla f(x)$是$\beta$-Lipschitz的，即：<br />
<script type="math/tex">\mid\mid\nabla f(x)-\nabla f(y)\mid\mid \leq\beta\mid\mid x-y\mid\mid,</script><br />
那么我们称 $f(x)$ 是$\beta$-smooth的,更一般表示为：<br />
<script type="math/tex">\mid\mid\nabla f(x)-\nabla f(y)\mid\mid_D \leq\beta\mid\mid x-y\mid\mid_P,</script><br />
其中$\mid\mid\cdot\mid\mid_P$是范数norm，$\mid\mid\cdot\mid\mid_D$是对偶范数dual norm。</p>
    </li>
    <li>函数的$\alpha$-strongly convex和 $\beta$-smoothness有对偶关系，如果函数$f$是关于对偶范数 $\mid\mid\cdot\mid\mid_D$ 的$\beta$-smooth,那么$f^<em>$是关于范数$\mid\mid\cdot\mid\mid_P$的$\frac{1}{\beta}$-strongly convex。 其中$f^</em>=\max_y(y^Tx-f(y))$是函数$f(x)$的共轭函数。</li>
  </ul>

  <p>Wiki中也可以查阅详细定义与解释。</p>
</blockquote>

<p>为了定义出平滑的$\max$算子: $\max_{\Omega}$，再次使用共轭：<br />
<script type="math/tex">{\max}_{\Omega}(x)=max_{\Omega}^{**}(x) = \sup_{y\in R^d} y^Tx - {\max}^*_{\Omega} (y)=\sup_{y\in\Delta^d}y^Tx-\gamma\Omega(y)</script></p>

<p>由此，前面提到的映射${\prod}<em>{\Omega}: R^d\rightarrow \Delta^d$定义为：<br />
<script type="math/tex">{\prod}_{\Omega}(x):=\arg\max_{y\in \Delta^d}y^Tx-\gamma\Omega(y)=\nabla max_{\Omega}(x)</script><br />
&gt; 1. 上述式子可由：$\max</em>{\Omega}(x)=(y^<em>)^Tx-\max^</em><em>{\Omega}(y^*)\Longleftrightarrow y^* \in \partial\max</em>{\Omega}(x)$ 证得。<br />
&gt; 2. $\partial\max_{\Omega}(x)={\nabla\max_{\Omega}(x)}$只有唯一解（$y^*$是单形体的顶点）。故$\prod_{\Omega}$ 是梯度的映射函数。</p>

<p><strong>强凸的重要性：</strong></p>

<p>对$\Omega$的$\beta$-strongly convex的假设是很重要的，如果函数$f:R^d\rightarrow R$ 的共轭函数 $f^*$ 是关于对偶范数 $\mid\mid\cdot\mid\mid_D$ 的$\frac{1}{\beta}$-smooth, 那么 $f$ 就是关于范数 $\mid\mid\cdot\mid\mid_P$ 的 $\beta$-strongly convex。那么这足以确保 $\max_{\Omega}$是$\frac{1}{\gamma\beta}$-smooth, 或者说 $\max_{\Omega}$ 处处可微，且它的梯度 $\prod_{\Omega}$ 在对偶范数 $\mid\mid\cdot\mid\mid_D$上是 $\frac{1}{\gamma\beta}$-Lipschitz的.</p>

<p><strong>训练问题（$\prod_{\Omega}$即表示Attention的输出）：</strong></p>

<ul>
  <li>前向时，需要解决的问题是：如何计算 $\prod_{\Omega}$ ?</li>
  <li>反向时，需要解决的问题是：如何计算 $\prod_{\Omega}$ 的雅可比矩阵？或说如何计算 $\max_{\Omega}$ 的Hessian矩阵？</li>
</ul>

<hr />
<p>## 3. 验证方法：用正则项 $\Omega$恢复Softmax与Sparsemax</p>

<p>在推导新的Attention机制前，先展示如何用正则项$\Omega$恢复出softmax和sparsemax。</p>

<h3 id="softmax">3.1 Softmax：</h3>

<p>选择$\Omega(y)=\sum_{i=1}^d y_i \log y_i$ ,即负熵。则它的共轭函数为 $log\;sum\;exp$, 即$f^*(y)=\log \sum_{i=1}^d e^{y_i}$.<br />
&gt;证明此时$\Omega(y)$的共轭函数为 $log\;sum\;exp$ :</p>

<blockquote>
  <p>若函数 $f(x)=\log \sum_{i=1}^n e^{x_i}$ ,则由$f^*(y)=sup y^Tx-f(x)$ 对$x$求偏导得：$y_i=\frac{e^{x_i}}{\sum_{i=1}^n e^{x_i}}$ , 即$1^Ty=1,\sum y_i =1$.</p>

  <p>故有$e^{x_i}=y_i\sum e^{x_i} \Rightarrow$</p>

  <p><script type="math/tex">% <![CDATA[
\begin{array}{l}
f^*(y) & =\sum y_i x_i - log\sum e^{x_i} \\
& =\sum y_i \log(y_i\sum e^{x_i})-\log\sum e^{x_i} \\
& = \sum y_i\log y_i + \sum y_i \log\sum e^{x_i}-\log\sum e^{x_i} \\
& = \sum y_i\log y_i
\end{array} %]]></script><br />
由于$f(x)$是凸函数（可证），所以$f^{**}=log\;sum\;exp$.<br />
也可以查阅<a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">《Convex optimization》</a>的习题3.25, 里面有详细证明。</p>
</blockquote>

<p>如果 $f(x)=\gamma g(x)$,则对于 $\gamma &gt;0$,有 $f^<em>(y)=\gamma g^</em>(y/\gamma)$. 则${\max}<em>{\Omega}(x)=\gamma \log\sum</em>{i=1}^d e^{x_i / \gamma}$.</p>

<p>由于$\Omega(y)$负熵在 $\mid\mid\cdot\mid\mid_1$ 是 1-strongly convex，所以${\max}<em>{\Omega}$在 $\mid\mid\cdot\mid\mid</em>{\infty}$ 上是$\frac{1}{\gamma}$-smooth.</p>

<p>通过对 ${\max}_{\Omega}$ 的 $x$ 求偏导即得softmax：<br />
<script type="math/tex">{\prod}_{\Omega}(x) = \frac{\partial {\max}_{\Omega}}{\partial x}=\frac{e^{x/\gamma}}{\sum_{i=1}^d e^{x_i/\gamma}}</script></p>

<p>其中 $\gamma$越小，输出的 $softmax$就越尖锐。<a href="https://arxiv.org/abs/1503.02531?context=cs">Distilling the Knowledge in a Neural Network</a> 里面也涉及到$\gamma$的设计。</p>

<hr />
<p>### 3.2 Sparsemax：</p>

<p>选择 $\Omega(y)=\frac{1}{2}\mid\mid y\mid\mid^2_2$, 它也被称为Moreau-Yosida正则化常用于近端算子理论。</p>

<p>由于 $\frac{1}{2}\mid\mid y\mid\mid^2_2$ 在$\mid\mid\cdot\mid\mid_2$ 中是 1-strongly convex，所以在${\max}_{\Omega}$在$\mid\mid\cdot\mid\mid_2$上是$\frac{1}{\gamma}$-smooth.</p>

<p>由此能得：<br />
<script type="math/tex">{\prod}_{\Omega}(x)=P_{\Delta^d}(x/\gamma) = \arg\min_{y\in \Delta^d}\mid\mid y-x/\gamma\mid\mid^2</script></p>

<p>当$\gamma=1$时，上式就为Sparsemax（softmax的一个稀疏替代）。</p>

<p>这推导得出：调控 $\gamma$ 可以控制稀疏性。根据sparsemax的论文<a href="https://arxiv.org/pdf/1602.02068.pdf">From softmax to sparsemax: A sparse model of attention and multi-label classification</a>中的公式 $9$可以知道 ${\prod}<em>{\Omega}$ 的雅可比矩阵为:<br />
<script type="math/tex">J_{ {\prod}_{\Omega}}(x)=\frac{1}{\gamma}J_{P_{\Delta^d}}(x/\gamma)=\frac{1}{\gamma}\big(diag(s)-ss^T/\mid\mid s\mid\mid_1\big)</script><br />
&gt; 其中 $s\in{0,1}^d$ 指示着 ${\prod}</em>{\Omega}(x)$ 的非$0$元素。<br />
&gt; <br />
&gt; ${\prod}_{\Omega}(x)$ 是Lipschitz 连续，处处可导。<br />
&gt; <br />
&gt; 详情建议阅读sparsemax的论文。</p>

<hr />
<p>## 4. Fusedmax与Oscarmax 新Attention机制</p>

<p>论文提出 **论点 $1$：如果 $\Omega$ 函数可微，那可以计算出 ${\prod}<em>{\Omega}$ 的雅可比矩阵 .**<br />
&gt; 论文附录 $A.1$ 已证明，只要提供了$\Omega$的Jacobian和Hessian，那就可以根据论文提供的公式计算出 $J</em>{ {\prod}_{\Omega}}$ .</p>

<p>那来一个简单例子吧。</p>

<h3 id="squared-p-norms-p-">4.1. 示例：Squared p-norms（平方 p-范式）</h3>

<p>Squared p-norms作为单纯形上可微函数的一个有用例子：$\Omega(y)=\frac{1}{2}\mid\mid y\mid\mid^2_p = \big(\sum_{i=1}^d y_i^p \big)^{2/p}$ , 其中 $y\in\Delta^d$ 且 $p\in(1,2]$.</p>

<p>我们已知 squared p-norm 在$\mid\mid\cdot\mid\mid_p$是strongly convex, 这也指出，当$\frac{1}{p} + \frac{1}{q} = 1$时， ${\max}_{\Omega}$ 在 $\mid\mid\cdot\mid\mid_q$ 是$\frac{1}{\gamma(p-1)}$-smooth 。计算出sq-pnorm-max为：<br />
<script type="math/tex">{\prod}_{\Omega}(x)= \arg\min_{y\in \Delta^d} \frac{\gamma}{2}\mid\mid y\mid\mid^2_p - y^Tx</script></p>

<p>论点$1$ 所需要的梯度和Hessian可以通过 $\nabla\Omega(y)=\frac{y^{p-1}}{\mid\mid y\mid\mid^{p-2}<em>p}$ 以及下式得到所需要的$J</em>{ {\prod}_{\Omega}}$.<br />
<script type="math/tex">H_{\Omega}(y)=diag(d)+ uu^T,\quad where \quad d=\frac{(p-1)}{\mid\mid y\mid\mid^{p-2}_p}y^{p-2} \quad and \quad u=\sqrt{\frac{(2-p)}{\mid\mid y\mid\mid^{2p-2}_p}} y^{ p-1}</script><br />
&gt; sq-pnorm-max 的 $p = 2$ 时就恢复为 sparsemax 一样鼓励稀疏输出。 <br />
&gt; <br />
&gt; 但$1&lt;p&lt;2$ 时，$y^<em>=[0,1]$和$y^</em>=[0,1]$ 间的转换将会更平滑。所以在实验中采用 $p=1.5$ 。详情可以查阅论文中对比图的分析与实验。</p>

<hr />
<p>### 4.2. Fusedmax与Oscarmax</p>

<p>上文已经采用Squared p-norm 例子展示了论点$1$（或说这一套解决方案）的可行性之后，接下论文将提出适合作为Attention机制的可微且$\beta$-strongly convex 的 $\Omega$ 函数。<br />
&gt; 下面会讲到Fusedmax和Oscarmax，其中会涉及到TV(全变分)和OSCAR(用于回归的八边形收缩和聚类算法)。 <br />
&gt; <br />
&gt; TV和OSCAR在文章后面会有单独解释。</p>

<h4 id="fusedmax">Fusedmax</h4>
<p>当输入是连续且顺序是有一定意义的时（如自然语言），我们希望能<strong>鼓励让连续区域的文本段有着相同的Attention值</strong>。</p>

<p>为此，基于<a href="https://web.stanford.edu/group/SOL/papers/fused-lasso-JRSSB.pdf">Sparsity and smoothness via the fused lasso</a>的fused lasso（或称 1-d total variation(TV))，选择$\Omega(y)=\frac{1}{2}\mid\mid y\mid\mid^2_2 + \lambda\sum_{i=1}^{d-1}\mid y_{i+1} - y_i\mid$, 即为强凸项和1-d TV惩罚项的和。 故可以得到：<br />
<script type="math/tex">{\prod}_{\Omega}(x)= \arg\min_{y\in {\Delta}^d}\frac{1}{2}\mid\mid y-x/\gamma\mid\mid^2 + \lambda\sum^{d-1}_{i=1}\mid y_{i+1}- y_i\mid.</script></p>

<h4 id="oscarmax">Oscarmax</h4>

<p>由于TV让连续区域聚集（即鼓励连续区域的attention值相等），这样的前提假设太严格，于是作者参考<a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2007.00843.x">Simultaneous regression shrinkage, variable selection, and<br />
supervised clustering of predictors with OSCAR</a>中的OSCAR惩罚函数，以来鼓励对元素进行聚类，让同一集群的元素它们的Attention值相等。即<strong>鼓励对可能不连续的单词组给予同等重视</strong>。</p>

<p>选择$\Omega(y)=\frac{1}{2}\mid\mid y\mid\mid^2_2 + \lambda\sum_{i&lt;j} \max(\mid y_i\mid,\mid y_j\mid)$， 故可得到：<br />
<script type="math/tex">% <![CDATA[
{\prod}_{\Omega}(x)= \arg\min_{y\in {\Delta}^d}\frac{1}{2}\mid\mid y-x/\gamma\mid\mid^2 + \lambda\sum_{i<j}\max(\mid y_i\mid,\mid y_j\mid). %]]></script></p>

<h3 id="fusedmaxoscarmax">4.3. Fusedmax与Oscarmax的计算与其雅可比矩阵</h3>

<p>根据论文中的论点$2$和论点$3$, 就可得出Fusedmax和Oscarmax分别对应的雅可比矩阵了。<br />
<script type="math/tex">% <![CDATA[
[J_{P_{TV}}(x)]_{i,j} =\left\{
             \begin{array}{l}
             \frac{1}{\mid G^*_i\mid}  & if\; j\in G^*_i \\
             0 & o.w. 
             \end{array}\right. %]]></script> <br />
<script type="math/tex">% <![CDATA[
[J_{P_{OSC}}(x)]_{i,j} =\left\{
             \begin{array}{l}
             \frac{sign(z^*_i z^*_j)}{\mid G^*_i\mid}  & if\; j\in G^*_i \; and \; z^*_i\neq 0\\
             0 & o.w. 
             \end{array}\right. %]]></script> <br />
&gt; $P_{TV}(x):=\arg\min_{y\in {R}^d}\frac{1}{2}\mid\mid y-x\mid\mid^2 + \lambda\sum^{d-1}<em>{i=1}\mid y</em>{i+1}- y_i\mid.$<br />
&gt;$P_{OSC}(x):=\arg\min_{y\in {R}^d}\frac{1}{2}\mid\mid y-x\mid\mid^2 + \lambda\sum_{i&lt;j}\max(\mid y_i\mid,\mid y_j\mid)$<br />
&gt; <br />
&gt; $G^<em>_i={j\in [d]:\mid  z^</em>_i\mid = \mid z^<em>_j\mid }.$ 其中$\mid G^</em>_i\mid$ 表示 $i$ 组的元素数量。<br />
&gt; <br />
&gt; 论文附录 $A.2$ 有详细的证明过程。<br />
&gt; <br />
&gt; 论文的附录中还有大量实验结果，可以加深理解。</p>

<p>来看一个 法语-英语翻译 Attention实验效果：<br />
<img src="https://github.com/luonango/luonango.github.io/raw/master/img/pictures/fusedmax_oscarmax.png" alt="" /></p>

<hr />
<p>## 5. TV与OSCAR的简单介绍：</p>

<h3 id="tvtotal-variation-">5.1. TV(Total variation, 全变分)</h3>

<p>TV(Total variation, 全变分)，也称为全变差，在图像复原中常用到。TV是在一函数其数值变化的差的总和，具体可查阅<a href="https://zh.wikipedia.org/zh-cn/%E6%80%BB%E5%8F%98%E5%B7%AE">wiki-Total_variation</a>或<a href="https://zh.wikipedia.org/zh-cn/%E6%80%BB%E5%8F%98%E5%B7%AE">wiki-总变差</a>:</p>

<blockquote>
  <p>实值函数ƒ定义在区间$[a, b] \in R$的总变差是一维参数曲线$x \rightarrow ƒ(x) , x \in [a,b]$的弧长。 连续可微函数的总变差，可由如下的积分给出:<br />
<script type="math/tex">V^a_b(f)=\int^b_a \mid f'(x)\mid \mathrm{d}x</script><br />
任意实值或虚值函数ƒ定义在区间[a,b]上的总变差，由下面公式定义($p$为区间$[a,b]$中的所有分划):<br />
<script type="math/tex">V^a_b(f)=sup_P\sum^{n_p -1}_{i=0}\mid f(x_{i+1}) - f(x_{i})\mid</script></p>

</blockquote>

<table>
  <thead>
    <tr>
      <th>TV图解</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>当绿点遍历整个函数时，&lt;p&gt;绿点在y-轴上的投影红点走过的<strong>路程</strong>&lt;p&gt;就是该函数的总变分TV</td>
      <td><img src="https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Total_variation.gif" alt="Total_variation" /></td>
    </tr>
  </tbody>
</table>

<p>因为细节或假细节（如噪音）区域较多的信号则TV值较大，那假如我们得到观察信号$x_i$, 希望对$x_i$进行去噪，就可以通过引入最小化$x_i$的全变分，得到去噪且保持了图像边缘的图像。即对原复原函数引入TV正则项，如一维去噪：<br />
<script type="math/tex">\min_y \frac{1}{2}\sum_i(x_i-y_i)^2 + \lambda \sum_{i=1}^n\mid y_{i+1} - y_i \mid</script><br />
更多解释可参考<a href="https://www.zhihu.com/question/47162419">如何理解全变分（Total Variation，TV）模型？</a>和<a href="https://blog.csdn.net/hanlin_tan/article/details/52448803">浅谈图象的全变分和去噪</a></p>

<p>论文中说Fused Lasso也称为1d-TV ，公式为：<br />
<script type="math/tex">\hat{\beta}_{fused} = {\arg\min}_{\beta}\frac{1}{2}{\mid\mid y-\beta\mid\mid}_2^2 + \lambda \cdot \mid\mid D\beta\mid\mid_1</script></p>

<p>在1维中 : $\hat{\beta}<em>{fused} = {\arg\min}</em>{\beta}\frac{1}{2}{\mid\mid y-\beta\mid\mid}<em>2^2 + \lambda \cdot \sum</em>{i=1}^{n-1}\mid\beta_i - \beta_j\mid_1 $ ， 则此时$D$ 为(若n=5)：<br />
<script type="math/tex">% <![CDATA[
\left(
 \begin{matrix}
   1 & -1 & 0 & 0 & 0\\
   0 & 1 & -1 & 0 & 0\\
   0 & 0 & 1 & -1 & 0\\
   0 & 0 & 0 & 1 & -1
  \end{matrix}
  \right) %]]></script><br />
将上述方法应用在1维数据得到结果如下(图来自<a href="http://euler.stat.yale.edu/~tba3/stat612/lectures/lec22/lecture22.pdf">The Generalized Lasso，Lecture 22</a>）：</p>

<p><img src="https://github.com/luonango/luonango.github.io/raw/master/img/pictures/Fuse_lasso_1.png" alt="" /></p>

<p>可以看出，数据最终呈现连续区域的聚集，即空间聚集（spatial clutering)，也就得到了稀疏形式的数据表示。 得到更平滑的数据表示，也能防止过拟合的数据表示。</p>

<hr />

<blockquote>
  <p>但是TV让连续区域聚集（即鼓励连续区域的值相等），这样的前提假设太严格，于是作者参考OSCAR惩罚函数，以来鼓励对Attention的输出值进行聚集，让同集群的Attention值相等。</p>
</blockquote>

<hr />
<p>### 5.2. OSCAR (用于回归的八边形收缩和聚类算法)</p>

<p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2007.00843.x">Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with OSCAR</a></p>

<p>OSCAR(用于回归的八边形收缩和聚类算法，octagonal shrinkage and clustering algorithm for regression)可以被解释为同时实现聚类和回归. 是做稀疏和分组变量选择，类似于Elastic Net.</p>

<p><a href="http://people.ee.duke.edu/~lcarin/Minhua3.20.09.pdf">OSCAR-PPT解释</a>：<br />
<script type="math/tex">% <![CDATA[
\begin{align}
&Penalized Regression一般形式： & \hat{\beta} &  = \min_{\beta}\mid\mid y- x\beta\mid\mid^2_2 \quad s.t. \quad f(\beta)\le t \\
&Ridge Regression: & f(\beta) &=\mid\mid\beta\mid\mid^2_2=\sum^{p}_{j=1}\beta^2_j\\
&LASSO：  & f(\beta) & =\mid\mid\beta\mid\mid_1=\sum^{p}_{j=1}\mid\beta\mid \\
&Group LASSO:  & f(\beta) & =\sum^{G}_{g=1}\mid\mid\beta_g\mid\mid_2\\
&Elastic Net(弹性网络）:  & f(\beta) & =\alpha\mid\mid\beta\mid\mid^2_2+(1-\alpha)\mid\mid\beta\mid\mid_1\\
&OSCAR:  & f(\beta) & =\mid\mid\beta\mid\mid_1 + c\sum_{j < k} \max\{\mid\beta\mid_j,\mid\beta\mid_k\}, L_1与pair-wise L_{\infty}组合。
\end{align} %]]></script></p>

<ul>
  <li>OSCAR是做稀疏和分组变量选择，类似于Elastic Net.</li>
  <li>与Group-LASSO相比，它不需要群组结构的先验知识。</li>
  <li>它比Elastic Net有更强的假设，OSCAR假定：相关变量（correlated variables）的回归系数的绝对值相同。</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Elastic Net</th>
      <th style="text-align: center">OSCAR</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="https://github.com/luonango/luonango.github.io/raw/master/img/pictures/oscar_elasticnet.png" alt="" /></td>
      <td style="text-align: center"><img src="https://github.com/luonango/luonango.github.io/raw/master/img/pictures/oscar_oscar.png" alt="" /></td>
    </tr>
    <tr>
      <td style="text-align: center">$L_1$和$L_2$范式组合</td>
      <td style="text-align: center">$L_1$和$L_{\infty}$范式组合</td>
    </tr>
  </tbody>
</table>

<p>看上图也就能理解“八边形收缩”这名称了. 该OSCAR基于惩罚最小二乘法，将一些系数收缩至恰好为零。同时这惩罚函数能产生值相同的系数，鼓励相关的预测因子(即指$x_i$)它们对最终结果有着相同的影响，从而形成单个系数表示预测因子群集。</p>

<p>至于更详细的理解，就阅读下原论文吧<a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2007.00843.x">Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with OSCAR</a>。</p>

<hr />
<p>## 6. 后语</p>

<p><strong>论文中提出新的Attention机制：fusedmax和oscarmax，它们是能被反向传播训练的神经网络所使用。基于论文给出的公式计算即可完成该Attention机制的前向和反向的计算。</strong></p>

<p>文中的实验都是文本语言方面（textual entailment，machine translation, summarization）， 但如果想用于视觉方面，应该需要适当修改 fusedmax。因为fusedmax所指的连续的区域应当是二维局域图，而非一维连续点（卷积层中）。而oscarmax因为可以对不连续的元素进行Attention值上的聚集，故可以直接应用在图像方面。</p>

<p>Attention的输出值进行稀疏，这个原因和理由容易理解（即稀疏的优点）。但为什么要对元素进行聚集且赋予相同的Attention值呢？我觉得主要的原因还是防止过拟合的数据表示，即防止Attention机制过拟合。当然，如果翻翻聚类方面的论文或许有更好的解释。</p>

<p>文中很多公式在其他论文中已证出，想完整地看懂这篇文章，还需要好好把引用的论文理解理解。<br />
&gt;随便点开都是一屏幕公式推导</p>

<p>由于我数学方面的功底不好，论文涉及的一些背后知识都是现查现学.</p>

<p>欢迎讨论指错，轻喷就好 = =。</p>

<hr />

<p><a href="https://zhuanlan.zhihu.com/p/38897903">若出现格式问题，可移步查看知乎同款文章：Fusedmax与Oscarmax：稀疏及结构化的Attention正则化框架</a></p>

<hr />


                <hr style="visibility: hidden;">

                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2018/05/01/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%9A%E9%81%93/" data-toggle="tooltip" data-placement="top" title="卷积网络的特征通道">
                        Previous<br>
                        <span>卷积网络的特征通道</span>
                        </a>
                    </li>
                    
                    
                </ul>


                <!--Gitalk评论start  -->
                
                <!-- 引入Gitalk评论插件  -->
                <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
                <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
                <div id="gitalk-container"></div>
                <!-- 引入一个生产md5的js，用于对id值进行处理，防止其过长 -->
                <!-- Thank DF:https://github.com/NSDingFan/NSDingFan.github.io/issues/3#issuecomment-407496538 -->
                <script src="/js/md5.min.js"></script>
                <script type="text/javascript">
                    var gitalk = new Gitalk({
                    clientID: '54aa3c825e96e88854c0',
                    clientSecret: '14b921a4bcdb0552b99edf9da7e6f93577d9a40a',
                    repo: 'luonango.github.io',
                    owner: 'luonango',
                    admin: ['luonango'],
                    distractionFreeMode: true,
                    id: md5(location.pathname),
                    });
                    gitalk.render('gitalk-container');
                </script>
                
                <!-- Gitalk end -->

                

            </div>  

    <!-- Side Catalog Container -->
        
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
        

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
                				<a href="/tags/#图像分类" title="图像分类" rel="2">
                                    图像分类
                                </a>
                            
        				
                            
                				<a href="/tags/#CNN" title="CNN" rel="3">
                                    CNN
                                </a>
                            
        				
                            
                				<a href="/tags/#网络结构" title="网络结构" rel="2">
                                    网络结构
                                </a>
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
        			</div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">
                    
                        <li><a href="https://www.zhihu.com/people/luonango">Nango 明楠</a></li>
                    
                        <li><a href="http://www.jianshu.com/u/fe747d686af8">简书·BY</a></li>
                    
                        <li><a href="https://apple.com">Apple</a></li>
                    
                        <li><a href="https://developer.apple.com/">Apple Developer</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>






<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
        // BY Fix:去除标题前的‘#’ issues:<https://github.com/luonango/luonango.github.io/issues/137>
        // anchors.options = {
        //   visible: 'always',
        //   placement: 'right',
        //   icon: '#'
        // };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>


    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    
                    <!-- add jianshu add target = "_blank" to <a> by BY -->
                    
                            <li>
                                <a target="_blank" href="https://www.jianshu.com/u/fe747d686af8">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa  fa-stack-1x fa-inverse">简</i>
                                    </span>
                                </a>
                            </li>
                    
                    

                    <!-- add Weibo, Zhihu by Hux, add target = "_blank" to <a> by Hux -->
                    
                    <li>
                        <a target="_blank" href="https://www.zhihu.com/people/luonango">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa  fa-stack-1x fa-inverse">知</i>
                            </span>
                        </a>
                    </li>
                    
                    


                    
                    <li>
                        <a target="_blank" href="https://www.facebook.com/luonango">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li>
                        <a target="_blank" href="https://github.com/luonango">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; BY Blog 2018
                    <br>
                    Theme on <a href="https://github.com/luonango/luonango.github.io.git">GitHub</a> |
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=luonango&repo=luonango.github.io&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>

<!-- Service Worker -->

<script type="text/javascript">
    if(navigator.serviceWorker){
        // For security reasons, a service worker can only control the pages that are in the same directory level or below it. That's why we put sw.js at ROOT level.
        navigator.serviceWorker
            .register('/sw.js')
            .then((registration) => {console.log('Service Worker Registered. ', registration)})
            .catch((error) => {console.log('ServiceWorker registration failed: ', error)})
    }
</script>



<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/ 
     - https://github.com/jneen/rouge/wiki/list-of-supported-languages-and-lexers   
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async('/js/jquery.tagcloud.js',function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->

<script>
    // dynamic User by Hux
    var _gaId = 'UA-90855596-1';
    var _gaDomain = 'auto';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>



<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = 'b50bf2b12b5338a1845e33832976fd68';

    // Originial
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>




<!-- Side Catalog -->

<script type="text/javascript">
    function generateCatalog (selector) {
        var P = $('div.post-container'),a,n,t,l,i,c;
        a = P.find('h1,h2,h3,h4,h5,h6');
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#"+$(this).prop('id');
            t = $(this).text();
            c = $('<a href="'+i+'" rel="nofollow">'+t+'</a>');
            l = $('<li class="'+n+'_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;    
    }

    generateCatalog(".catalog-body");

    // toggle side catalog
    $(".catalog-toggle").click((function(e){
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    /*
     * Doc: https://github.com/davist11/jQuery-One-Page-Nav
     * Fork by Hux to support padding
     */
    async("/js/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>





<!-- Image to hack wechat -->
<img src="/img/apple-touch-icon.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->
<!-- MathJax Section -->
<!-- copied from http://docs.mathjax.org/en/latest/configuration.html -->
</body>

</html>
